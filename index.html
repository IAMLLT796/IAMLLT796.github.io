<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><link rel="icon" href="/images/icons/3.jpg?v=2.8.0" type="image/png" sizes="16x16"><link rel="icon" href="/images/icons/4.jpg?v=2.8.0" type="image/png" sizes="32x32"><meta property="og:type" content="website">
<meta property="og:title" content="葵海">
<meta property="og:url" content="https://iamllt796.github.io/index.html">
<meta property="og:site_name" content="葵海">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="LLT796">
<meta name="twitter:card" content="summary"><title>葵海</title><link ref="canonical" href="https://iamllt796.github.io/index.html"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.12.1/css/all.min.css" type="text/css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" type="text/css"><link rel="stylesheet" href="/css/index.css?v=2.8.0"><link rel="stylesheet" href="css/custom.css"><script>var Stun = window.Stun || {};
var CONFIG = {
  root: '/',
  algolia: undefined,
  assistSearch: undefined,
  fontIcon: {"prompt":{"success":"fas fa-check-circle","info":"fas fa-arrow-circle-right","warning":"fas fa-exclamation-circle","error":"fas fa-times-circle"},"copyBtn":"fas fa-copy"},
  sidebar: {"offsetTop":"20px","tocMaxDepth":6},
  header: {"enable":true,"showOnPost":true,"scrollDownIcon":false},
  postWidget: {"endText":true},
  nightMode: {"enable":true},
  back2top: {"enable":true},
  codeblock: {"style":"default","highlight":"light","wordWrap":true},
  reward: false,
  fancybox: true,
  zoomImage: {"gapAside":"20px"},
  galleryWaterfall: undefined,
  lazyload: false,
  pjax: {"avoidBanner":false},
  externalLink: {"icon":{"enable":true,"name":"fas fa-external-link-alt"}},
  shortcuts: undefined,
  prompt: {"copyButton":"复制","copySuccess":"复制成功","copyError":"复制失败"},
  sourcePath: {"js":"js","css":"css","images":"images"},
};

window.CONFIG = CONFIG;</script><meta name="generator" content="Hexo 6.3.0"></head><body><div class="container" id="container"><header class="header" id="header"><div class="header-inner"><nav class="header-nav header-nav--fixed"><div class="header-nav-inner"><div class="header-nav-menubtn"><i class="fas fa-bars"></i></div><div class="header-nav-menu"><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/"><span class="header-nav-menu-item__icon"><i class="fas fa-home"></i></span><span class="header-nav-menu-item__text">首页</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/archives/"><span class="header-nav-menu-item__icon"><i class="fas fa-folder-open"></i></span><span class="header-nav-menu-item__text">归档</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/categories/"><span class="header-nav-menu-item__icon"><i class="fas fa-layer-group"></i></span><span class="header-nav-menu-item__text">分类</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/tags/"><span class="header-nav-menu-item__icon"><i class="fas fa-tags"></i></span><span class="header-nav-menu-item__text">标签</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/about/"><span class="header-nav-menu-item__icon"><i class="fas fa-user"></i></span><span class="header-nav-menu-item__text">博主</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/read/"><span class="header-nav-menu-item__icon"><i class="fas fa-book"></i></span><span class="header-nav-menu-item__text">阅读</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/timeline/"><span class="header-nav-menu-item__icon"><i class="fas fa-history"></i></span><span class="header-nav-menu-item__text">menu.timeline</span></a></div></div><div class="header-nav-mode"><div class="mode"><div class="mode-track"><span class="mode-track-moon"></span><span class="mode-track-sun"></span></div><div class="mode-thumb"></div></div></div></div></nav><div class="header-banner"><div class="header-banner-info"><div class="header-banner-info__title">葵海</div><div class="header-banner-info__subtitle">欢迎来到我的博客空间</div></div></div></div></header><main class="main" id="main"><div class="main-inner"><div class="content-wrap" id="content-wrap"><div class="content content-home" id="content"><section class="postlist"><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2025/12/07/MCP-Function-Calling%E5%92%8CA2A/">MCP, Function Calling和A2A</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2025-12-07</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2025-12-27</span></span></div></header><div class="post-body"><div class="post-excerpt">
        <h1 id="MCP、Function-Calling和A2A">
          <a href="#MCP、Function-Calling和A2A" class="heading-link"><i class="fas fa-link"></i></a><a href="#MCP、Function-Calling和A2A" class="headerlink" title="MCP、Function Calling和A2A"></a>MCP、Function Calling和A2A</h1>
      <p><strong>MCP:</strong> Model Context Protocol，AI Agent 和 AI Tools 之间的工具发现、注册和调用协议。注意：MCP 和大模型并没有很大关系，不要被 Model 误导了。</p>
<p><strong>Function Calling:</strong> AI Agent 和 AI Model 之间的工具调用协议。</p>
<p><strong>A2A:</strong> Agent to Agent Protocol，AI Agent 和 AI Agent之间的工具调用协议。</p>

        <h2 id="MCP的工作流程">
          <a href="#MCP的工作流程" class="heading-link"><i class="fas fa-link"></i></a><a href="#MCP的工作流程" class="headerlink" title="MCP的工作流程"></a>MCP的工作流程</h2>
      <p><strong>MCP</strong> 是一种标准化协议，用来把大模型和外部工具&#x2F;数据&#x2F;能力解耦，通过上下文协商，能力发现，调用执行，结果回填，让 LLM 像调用函数一样安全、可控地使用外部世界。</p>

        <h3 id="参与角色">
          <a href="#参与角色" class="heading-link"><i class="fas fa-link"></i></a><a href="#参与角色" class="headerlink" title="参与角色"></a>参与角色</h3>
      <div class="table-container"><table>
<thead>
<tr>
<th>角色</th>
<th>职责</th>
</tr>
</thead>
<tbody><tr>
<td><strong>LLM &#x2F; Agent</strong></td>
<td>负责推理、决策、生成回答</td>
</tr>
<tr>
<td><strong>MCP Client</strong></td>
<td>模型一侧的“适配器”，负责和 MCP Server 通信</td>
</tr>
<tr>
<td><strong>MCP Server</strong></td>
<td>对外暴露资源、工具、Prompt 的服务</td>
</tr>
<tr>
<td><strong>External Systems</strong></td>
<td>文件系统、数据库、API、GitHub、Notion 等</td>
</tr>
</tbody></table></div>

        <h3 id="MCP-的标准流程">
          <a href="#MCP-的标准流程" class="heading-link"><i class="fas fa-link"></i></a><a href="#MCP-的标准流程" class="headerlink" title="MCP 的标准流程"></a>MCP 的标准流程</h3>
      <ul>
<li>用户提出任务<ul>
<li>LLM接收到自然语言请求。</li>
</ul>
</li>
<li>模型进行意图判断<ul>
<li>模型会在内部进行判断：<ul>
<li>是否需要<strong>外部信息</strong>；</li>
<li>是否有<strong>可用工具</strong>能更好完成任务；</li>
</ul>
</li>
<li>如果只是闲聊，流程到此结束。</li>
<li>如果需要工具，进入MCP流程。</li>
</ul>
</li>
<li>MCP Client 向 Server 查询能力<ul>
<li>MCP Client 会向 MCP Server询问是否能够提供 resources、tools、prompts；</li>
<li>Server 返回结构化描述（JSON Schema)，比如可读文件、可调用参数、参数说明等。</li>
</ul>
</li>
<li>模型选择合适的能力<ul>
<li>模型基于返回的元数据进行推理，模型只选择，真正的执行由MCP Client完成。<ul>
<li>代码：<code>read_file</code></li>
<li>数据：<code>query_database</code></li>
<li>操作：<code>run_command</code></li>
</ul>
</li>
</ul>
</li>
<li>MCP Client 调用 MCP Server<ul>
<li>MCP Server会执行以下操作：<ul>
<li>执行真实操作；</li>
<li>做权限与安全控制；</li>
<li>返回结果（文本&#x2F;JSON&#x2F;二进制）</li>
</ul>
</li>
</ul>
</li>
<li>结果作为上下文回注模型<ul>
<li>工具返回的结果不会直接给用户，而是作文新的上下文，注入模型输入中。</li>
</ul>
</li>
<li>模型生成最终回答<ul>
<li>LLM会综合用户原始问题、工具返回结果、自身推理能力，然后生成总结、建议、代码或者分析报告。</li>
</ul>
</li>
</ul>

        <h3 id="MCP-缺点">
          <a href="#MCP-缺点" class="heading-link"><i class="fas fa-link"></i></a><a href="#MCP-缺点" class="headerlink" title="MCP 缺点"></a>MCP 缺点</h3>
      <ul>
<li>架构复杂度高<ul>
<li>需要 Client + Server + Schema 三层；</li>
<li>不适合小项目或一次性脚本；</li>
</ul>
</li>
<li>对工程能力要求高<ul>
<li>要设计工具接口（JSON Schema）</li>
<li>要处理权限、沙箱、安全；</li>
</ul>
</li>
<li>调试体验差，错误可能来自模型决策、Client 调用或者 Server 实现</li>
<li>对模型能力有依赖，工具选择是由模型推理完成的</li>
<li>不适合纯文本任务</li>
</ul>

        <h2 id="幻觉">
          <a href="#幻觉" class="heading-link"><i class="fas fa-link"></i></a><a href="#幻觉" class="headerlink" title="幻觉"></a>幻觉</h2>
      <p><strong>幻觉就是 LLM 输出了看起来很合理，但实际上是错误的、虚构或未经验证的内容，并且语气很自信。</strong></p>
<p><strong>幻觉产生的原因：</strong></p>
<ul>
<li>语言模型≠事实模型，LLM的训练目标是预测下一个最可能的 token；</li>
<li>上下文信息不足，比如模型拿不到外部数据、上下文缺失被截断，或者问题本身不完整；</li>
<li>Prompt暗示了必须回答，即使LLM不知道，也会硬编一个详细实现；</li>
<li>训练数据的统计偏差，模型学到的是出现概率，不是真实值。</li>
</ul>
<p><strong>解决办法：</strong></p>
<ul>
<li><p>不要让模型猜，在系统中加入：</p>
<ul>
<li>如果信息不足，请明确说不知道；</li>
<li>不要编造，不确定就标注不确定性；</li>
</ul>
</li>
<li><p>把事实来源从模型里移出来（RAG），模型不负责记事实，只负责用事实</p>
<ul>
<li>用搜索&#x2F;数据库&#x2F;文档系统查到真实信息；</li>
<li>把结果作为上下文喂给模型；</li>
<li>要求只基于提供的材料回答；</li>
</ul>
</li>
<li><p>要求引用&#x2F;证据对齐</p>
<ul>
<li>每个结论必须对应一段输入材料；</li>
<li>没有证据就不能下结论；</li>
</ul>
</li>
<li><p>用结构化输出替代自由发挥，结构化会限制模型编故事的空间</p>
</li>
<li></li>
</ul>

        <h2 id="AI-Agent">
          <a href="#AI-Agent" class="heading-link"><i class="fas fa-link"></i></a><a href="#AI-Agent" class="headerlink" title="AI Agent"></a>AI Agent</h2>
      <p>AI Agent 是一种能自主感知环境、指定规划并执行动作，以完成特定任务的智能系统。AI Agent 区别于传统的 SOP（Standard Operation Procedure），workflow 在于传统的方式是基于规则的自动化、有明确的、预设的“如果-那么”规则。而 AI Agent 是基于目标的自主性，能自主规划、执行、调整。能感知环境变化、动态调整策略。能处理模糊、不确定的任务。其本质在于大模型本身所具有的强大语言理解、推理和泛化能力。</p>
<p>一个完整的 Agent 除了LLM（大脑）以外，还需要：</p>
<ul>
<li><strong>感知模块：</strong> 通过 API、搜索引擎、文件系统等获取外部信息；</li>
<li><strong>工具集：</strong> 可以调用各种函数、API、软件来影响现实。这就是 Function Calling 、MCP 等技术的用武之地；</li>
<li><strong>记忆模块：</strong> 通过向量数据库或记忆流记录过去的历史，用于长期规划和参考；</li>
</ul>
</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2025/12/06/transformer%E6%9E%B6%E6%9E%84/">transformer架构</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2025-12-06</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2025-12-07</span></span></div></header><div class="post-body"><div class="post-excerpt">
        <h1 id="transformer">
          <a href="#transformer" class="heading-link"><i class="fas fa-link"></i></a><a href="#transformer" class="headerlink" title="transformer"></a>transformer</h1>
      <p><strong>1. 随着seq_len的增加，推导 transformer layers 的计算复杂度</strong></p>
<ul>
<li>序列长度：L&#x3D;seq_len</li>
<li>模型维度：d（通常是 d_model）</li>
<li>注意力头数：h，每个头的维度 dk&#x3D;d&#x2F;h</li>
<li>前馈层隐层维度：dff（通常≈4d）</li>
</ul>
<img src="/2025/12/06/transformer%E6%9E%B6%E6%9E%84/image-20251206224631402.png" class title="image-20251206224631402">



<p><strong>2. 是否了解稀疏注意力机制，具体讲讲有哪些稀疏注意力</strong></p>
<p>标准 Transformer 的注意力复杂度是：<br>$$<br>O(n^2⋅d)<br>$$<br>其中 <strong>n 是序列长度，</strong> 随着 n 增大代价急剧上升。</p>
<p><strong>稀疏注意力的核心思想：并不是所有 token 都需要互相注意，将注意力矩阵变稀疏，</strong> 从而把复杂度降到线性或者次线性：<br>$$<br>O(n) 或 O(nlogn)<br>$$<br>稀疏方式一般分为 <strong>结构化稀疏</strong> 或者 <strong>基于内容的稀疏</strong></p>

        <h2 id="结构化稀疏注意力">
          <a href="#结构化稀疏注意力" class="heading-link"><i class="fas fa-link"></i></a><a href="#结构化稀疏注意力" class="headerlink" title="结构化稀疏注意力"></a>结构化稀疏注意力</h2>
      <ol>
<li>局部注意力（Local &#x2F; Sliding  Window Attention）</li>
<li>Stride Attention（跳跃注意力）</li>
<li>Dilated Attention（扩张卷积式注意力）</li>
<li>Block &#x2F; Chunk Attention（块状注意力）</li>
</ol>

        <h2 id="随机或低秩近似类稀疏注意力">
          <a href="#随机或低秩近似类稀疏注意力" class="heading-link"><i class="fas fa-link"></i></a><a href="#随机或低秩近似类稀疏注意力" class="headerlink" title="随机或低秩近似类稀疏注意力"></a>随机或低秩近似类稀疏注意力</h2>
      <ol>
<li>Reformer 的 LSH Attention （局部敏感哈希稀疏）</li>
<li>Linformer（低秩投影 Attention）</li>
<li>Performer（基于随机特征的线性 Attention）</li>
</ol>

        <h2 id="全局-token-局部-Token-混合注意力">
          <a href="#全局-token-局部-Token-混合注意力" class="heading-link"><i class="fas fa-link"></i></a><a href="#全局-token-局部-Token-混合注意力" class="headerlink" title="全局 token + 局部 Token 混合注意力"></a>全局 token + 局部 Token 混合注意力</h2>
      <ol>
<li>Longformer Hybrid attention</li>
<li>BigBird Block-Sparse Attention (Local + Random + Global)</li>
</ol>

        <h2 id="自适应-x2F-内容相关稀疏">
          <a href="#自适应-x2F-内容相关稀疏" class="heading-link"><i class="fas fa-link"></i></a><a href="#自适应-x2F-内容相关稀疏" class="headerlink" title="自适应 &#x2F; 内容相关稀疏"></a>自适应 &#x2F; 内容相关稀疏</h2>
      <ol>
<li>Sparsemax &#x2F; Entmax Attention（稀疏化的概率分布）</li>
<li>Routing Attention（路由稀疏注意力）</li>
</ol>
<p><strong>3. Padding 的 mask 操作具体如何实现</strong></p>
<p><strong>4. 自注意力机制的公式，为什么要除以sqrt(d_k)</strong></p>
<p>缩放点积注意力：<br>$$<br>Attention(Q, K, V) &#x3D; softmax(QK^T&#x2F;\sqrt d_k)V<br>$$</p>
<ul>
<li>Q, K, V: query, key, value</li>
<li>d_k: key&#x2F;query 的维度</li>
<li>关键操作：<strong>将点积除以根号d_k</strong></li>
</ul>
<p>**为什么要除以sqrt(d_k)**： <strong>防止点积随着维度增大而过大，让 softmax 进入”梯度消失区“</strong></p>
<ul>
<li>softmax 输入更稳定</li>
<li>不易饱和</li>
<li>梯度更健康</li>
<li>训练速度和效果明显提升</li>
</ul>
<p><strong>5. 为什么不使用维度更深的单一注意力机制，而是采用多头注意力，多头注意力（MHA）机制带来了什么优势</strong></p>
<p><strong>为什么不用单一注意力机制：</strong></p>
<ul>
<li>单一注意力会强行把所有模式混合在同一个空间里</li>
<li>难以捕捉多粒度、多类型的关系</li>
<li>学习复杂模式更困难、优化更不稳定</li>
<li>MHA是并行便宜的，而一个深维单头其实更贵</li>
</ul>
<p><strong>多头注意力带来了什么优点：</strong></p>
<ul>
<li>多视角表达能力</li>
<li>提高模型表达能力而不增加太多计算</li>
<li>提高梯度稳定性和训练可控性</li>
<li>天然的结构正则化</li>
<li>更多可解释性</li>
</ul>
</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2025/12/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%90%86%E8%AE%BA/">神经网络理论</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2025-12-06</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2025-12-06</span></span></div></header><div class="post-body"><div class="post-excerpt"></div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2025/12/06/%E5%9F%BA%E6%9C%AC%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/">基本机器学习理论</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2025-12-06</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2025-12-06</span></span></div></header><div class="post-body"><div class="post-excerpt"></div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2025/12/06/pytorch%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/">pytorch的基本操作</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2025-12-06</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2025-12-06</span></span></div></header><div class="post-body"><div class="post-excerpt">
        <h1 id="Pytorch">
          <a href="#Pytorch" class="heading-link"><i class="fas fa-link"></i></a><a href="#Pytorch" class="headerlink" title="Pytorch"></a>Pytorch</h1>
      
        <h2 id="Tensor-转置">
          <a href="#Tensor-转置" class="heading-link"><i class="fas fa-link"></i></a><a href="#Tensor-转置" class="headerlink" title="Tensor 转置"></a>Tensor 转置</h2>
      <p><strong>1. 交换张量的两个指定维度</strong></p>
<ul>
<li><strong>方法一</strong>：<code>transpose(dim0, dim1)</code></li>
</ul>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)	<span class="comment"># 形状[2, 3, 4]</span></span><br><span class="line">y = x.transpose(<span class="number">1</span>, <span class="number">2</span>)		<span class="comment"># 交换维度1和2</span></span><br><span class="line"><span class="built_in">print</span>(y.shape)					<span class="comment"># [2, 4, 3]</span></span><br></pre></td></tr></table></div></figure>

<ul>
<li><strong>方法二</strong>： <code>permute</code> （适合交换多个维度）</li>
</ul>
<p>可以通过重新排列所有维度来实现任意两个维度交换。</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 交换 dim = 0 和 dim = 2</span></span><br><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">y = x.permute(<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(y.shape)			<span class="comment"># [3, 4, 2]</span></span><br></pre></td></tr></table></div></figure>




        <h2 id="Tensor-升维-x2F-降维">
          <a href="#Tensor-升维-x2F-降维" class="heading-link"><i class="fas fa-link"></i></a><a href="#Tensor-升维-x2F-降维" class="headerlink" title="Tensor 升维&#x2F;降维"></a>Tensor 升维&#x2F;降维</h2>
      <p><strong>1. 在指定的 dim 位置插入一个大小为 1 的新维度</strong></p>
<p>使用 <code>unsqueeze</code> :</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># y = x.unsqueeze(dim)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x = torch.randn(<span class="number">3</span>, <span class="number">4</span>)		<span class="comment"># shape: (3, 4)</span></span><br><span class="line">y = x.unsqueeze(<span class="number">1</span>)			<span class="comment"># 在 dim = 1 插入新维度</span></span><br><span class="line"><span class="built_in">print</span>(y.shape)				<span class="comment"># torch.Size([3, 1, 4])</span></span><br></pre></td></tr></table></div></figure>




        <h2 id="维度合并">
          <a href="#维度合并" class="heading-link"><i class="fas fa-link"></i></a><a href="#维度合并" class="headerlink" title="维度合并"></a>维度合并</h2>
      <p><strong>1. 将多个张量沿着一个现有的维度 dim 连接起来</strong></p>
<p>使用 <code>torch.cat</code>:</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 语法</span></span><br><span class="line"><span class="comment"># tensor: 由张量组成的序列（list或tuple）</span></span><br><span class="line"><span class="comment"># dim: 指定要连接的维度（必须是已有维度）</span></span><br><span class="line"><span class="comment">#torch.cat(tensor, dim=0)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例</span></span><br><span class="line"><span class="comment"># 沿 dim=0 方向拼接（batch 方向）</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">a = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">b = torch.randn(<span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">out = torch.cat([a, b], dim=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(out.shape)		<span class="comment"># torch.Size([6, 3])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 沿 dim=1 拼接（特征维度）</span></span><br><span class="line">a = torch.randn(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">b = torch.randn(<span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">out = torch.cat([a, b], dim=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(out.shape)		$ torch.Size([<span class="number">3</span>, <span class="number">7</span>])</span><br></pre></td></tr></table></div></figure>


        <h2 id="Tensor-堆叠">
          <a href="#Tensor-堆叠" class="heading-link"><i class="fas fa-link"></i></a><a href="#Tensor-堆叠" class="headerlink" title="Tensor 堆叠"></a>Tensor 堆叠</h2>
      <p><strong>1. 将多个相同形状的张量沿着一个新的维度 dim 堆叠起来</strong></p>
<p><code>torch.stack(tensor, dim)</code> : 在指定的 dim 位置 创建一个新的维度，并把输入张量沿该新维度堆叠。</p>
<ul>
<li>所有张量必须 形状完全相同；</li>
<li>输出张量的形状为：(n, …original shape…) 如果 dim&#x3D;0</li>
</ul>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 三个形状相同的张量</span></span><br><span class="line">t1 = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">t2 = torch.tensor([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line">t3 = torch.tensor([<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在新维度 dim=0 堆叠</span></span><br><span class="line">out0 = torch.stack([t1, t2, t3], dim=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(out0.shape)  <span class="comment"># torch.Size([3, 3])</span></span><br><span class="line"><span class="built_in">print</span>(out0)</span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">        [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在新维度 dim=1 堆叠</span></span><br><span class="line">out1 = torch.stack([t1, t2, t3], dim=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(out1.shape)  <span class="comment"># torch.Size([3, 3])</span></span><br><span class="line"><span class="built_in">print</span>(out1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">4</span>, <span class="number">7</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">5</span>, <span class="number">8</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">6</span>, <span class="number">9</span>]])</span><br></pre></td></tr></table></div></figure>



<p><strong><code>torch.cat</code> 与 <code>torch.stack</code> 对比</strong></p>
<div class="table-container"><table>
<thead>
<tr>
<th>操作</th>
<th>是否创建新维度</th>
<th>形状要求</th>
</tr>
</thead>
<tbody><tr>
<td><code>torch.cat</code></td>
<td>不创建新维度</td>
<td>连接维度以外的形状必须一致</td>
</tr>
<tr>
<td><code>torch.stack</code></td>
<td>创建新维度</td>
<td>所有张量形状必须完全一致</td>
</tr>
</tbody></table></div>

        <h2 id="Tensor-形状改变">
          <a href="#Tensor-形状改变" class="heading-link"><i class="fas fa-link"></i></a><a href="#Tensor-形状改变" class="headerlink" title="Tensor 形状改变"></a>Tensor 形状改变</h2>
      <p><strong>1. 在保持元素总数不变的情况下改变张量的形状</strong></p>
<p>在 <strong>保持元素总数不变</strong> 的前提下改变张量形状，在 PyTorch 中通常使用 <strong><code>torch.reshape</code></strong> 或 **<code>tensor.view</code>**。</p>
<ul>
<li>使用 <code>reshape</code> 改变形状：<code>reshape</code> 会根据需要自动选择是否拷贝数据，更灵活。</li>
</ul>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x = torch.arange(<span class="number">12</span>)      <span class="comment"># 1D: [12]</span></span><br><span class="line">y = x.reshape(<span class="number">3</span>, <span class="number">4</span>)       <span class="comment"># 2D: [3, 4]</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(y.shape)  <span class="comment"># torch.Size([3, 4])</span></span><br></pre></td></tr></table></div></figure>

<ul>
<li>使用 <code>view</code> 改变形状（要求张量内存连续）：<code>view</code> 需要张量是 contiguous（连续内存），否则需要 <code>.contiguous().view(...)</code>。</li>
</ul>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.arange(<span class="number">12</span>)</span><br><span class="line">y = x.view(<span class="number">2</span>, <span class="number">6</span>)</span><br><span class="line"><span class="built_in">print</span>(y.shape)  <span class="comment"># torch.Size([2, 6])</span></span><br></pre></td></tr></table></div></figure>



<p><strong>Torch 中的乘法有哪些？分别有哪些区别</strong></p>
<ol>
<li><p>逐元素相乘</p>
<p><code>torch.mul(a, b)</code> 或 <code>a * b</code></p>
<ul>
<li>要求 a 与 b 的形状可广播</li>
<li>每个对应位置相乘</li>
</ul>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">b = torch.tensor([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line">a * b   <span class="comment"># tensor([ 4, 10, 18])</span></span><br></pre></td></tr></table></div></figure>
</li>
<li><p>矩阵乘法</p>
<p><code>torch.matmul(a, b)</code> 或者 a @ b</p>
<ul>
<li>2D * 2D（标准矩阵乘法）</li>
<li>1D * 2D &#x2F; 2D * 1D（向量-矩阵乘）</li>
<li>高维张量的批量矩阵乘法</li>
<li>自动处理广播</li>
<li>用于线性层、神经网络核心计算</li>
</ul>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">b = torch.randn(<span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line">a @ b     <span class="comment"># shape: (3,5)</span></span><br></pre></td></tr></table></div></figure>
</li>
<li><p>严格的矩阵乘法（不广播）</p>
<p><code>torch.mm(a, b)</code></p>
<ul>
<li>只能用于 2D 张量</li>
<li>不支持 batch, 不支持广播</li>
</ul>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">b = torch.randn(<span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line">torch.mm(a, b)  <span class="comment"># shape (3,5)</span></span><br></pre></td></tr></table></div></figure>
</li>
<li><p>内积 &#x2F; 点乘</p>
<p><code>torch.dot</code></p>
<ul>
<li>仅用于 1D 张量</li>
<li>返回一个标量</li>
<li>计算向量相似度、余弦相似度的中间步骤。</li>
</ul>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">b = torch.tensor([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line">torch.dot(a, b)   <span class="comment"># 1*4 + 2*5 + 3*6 = 32</span></span><br></pre></td></tr></table></div></figure>
</li>
<li><p>批量矩阵-向量乘法</p>
<p><code>torch.mv(a, b)</code></p>
<ul>
<li>a: 2D</li>
<li>b: 1D</li>
<li>结果是 1D 向量</li>
</ul>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">b = torch.randn(<span class="number">4</span>)</span><br><span class="line">torch.mv(a, b)  <span class="comment"># shape (3,)</span></span><br></pre></td></tr></table></div></figure>
</li>
<li><p>批量矩阵乘法</p>
<p><code>torch.bmm(a, b)</code></p>
<ul>
<li>只能用于 3D 张量  (batch, m, n) @ (batch, n, p)</li>
<li>batch 维必须匹配</li>
<li>RNN &#x2F; Transformer 内部常用。</li>
</ul>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">10</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">b = torch.randn(<span class="number">10</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line">torch.bmm(a, b)  <span class="comment"># shape (10, 3, 5)</span></span><br></pre></td></tr></table></div></figure>
</li>
<li><p>逐元素乘的加权求和</p>
<p><code>torch.einsum</code></p>
<ul>
<li>可表达多种乘法，如点积、矩阵乘、转置乘等</li>
</ul>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.einsum(<span class="string">&#x27;ij,jk-&gt;ik&#x27;</span>, a, b)</span><br><span class="line">torch.einsum(<span class="string">&#x27;i,i-&gt;&#x27;</span>, a, b)   <span class="comment"># 点积</span></span><br></pre></td></tr></table></div></figure></li>
</ol>
<div class="table-container"><table>
<thead>
<tr>
<th>操作</th>
<th>接受形状</th>
<th>广播</th>
<th>输出形状</th>
<th>用途</th>
</tr>
</thead>
<tbody><tr>
<td><code>a * b</code> &#x2F; <code>torch.mul</code></td>
<td>任意可广播</td>
<td>✔️</td>
<td>与广播后一致</td>
<td>逐元素乘</td>
</tr>
<tr>
<td><code>a @ b</code> &#x2F; <code>torch.matmul</code></td>
<td>灵活</td>
<td>✔️</td>
<td>矩阵乘逻辑</td>
<td>NN 线性计算</td>
</tr>
<tr>
<td><code>torch.mm</code></td>
<td>2D×2D</td>
<td>❌</td>
<td>(m, p)</td>
<td>严格矩阵乘</td>
</tr>
<tr>
<td><code>torch.dot</code></td>
<td>1D×1D</td>
<td>❌</td>
<td>scalar</td>
<td>向量内积</td>
</tr>
<tr>
<td><code>torch.mv</code></td>
<td>2D×1D</td>
<td>❌</td>
<td>1D</td>
<td>矩阵乘向量</td>
</tr>
<tr>
<td><code>torch.bmm</code></td>
<td>3D×3D</td>
<td>❌</td>
<td>batch 矩阵乘</td>
<td>RNN&#x2F;批处理</td>
</tr>
<tr>
<td><code>torch.einsum</code></td>
<td>任意</td>
<td>自定义</td>
<td>任意</td>
<td>高度灵活</td>
</tr>
</tbody></table></div>
</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2025/11/24/%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83/">生产环境</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2025-11-24</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2025-12-04</span></span></div></header><div class="post-body"><div class="post-excerpt">
        <h1 id="CPU-100-问题怎么排查">
          <a href="#CPU-100-问题怎么排查" class="heading-link"><i class="fas fa-link"></i></a><a href="#CPU-100-问题怎么排查" class="headerlink" title="CPU 100% 问题怎么排查"></a>CPU 100% 问题怎么排查</h1>
      <p><strong>如果你线上遇到CPU占用100%的情况，你会怎么一步步排查，说一下你的分析思路。</strong></p>
<blockquote>
<ol>
<li>是否有条理地从系统到进程再到线程定位</li>
<li>理解CPU高的可能原因</li>
</ol>
</blockquote>
<p>如果线上CPU占用100%，有可能是算得太多，也有可能是卡得太久，对比，我会按照系统 -&gt; 进程 -&gt; 线程 -&gt; 代码这四步进行排查。</p>
<p><strong>第一步，系统层面</strong></p>
<p>需要先使用<code>top</code> 或<code>htop</code> 查看整体CPU占用，确认是单核打满还是多核整体飙升；其次如果是整体高，可能是负载激增，如果是单核高，多半是逻辑问题。</p>
<p>对于微服务系统，首先看监控&#x2F;告警：</p>
<ul>
<li>是单台机器100%，还是某个pod&#x2F;容器100%;</li>
<li>查看CPU各项：user&#x2F;system&#x2F;iowait&#x2F;steal占比不一样，方向也不一样<ul>
<li>us高：大概率是业务代码算得多&#x2F;死循环&#x2F;复杂SQL；</li>
<li>sy高：系统调用频繁（比如大量IO、锁、上下文切换）</li>
<li>wa高：CPU不是瓶颈，是IO卡住了；</li>
<li>steal高：虚拟化环境里被邻居抢了CPU。</li>
</ul>
</li>
</ul>
<figure class="highlight bash"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl top node</span><br><span class="line">kubectl top pod -n &lt;namespace&gt;  <span class="comment"># 定位到哪个pod/容器的CPU拉满</span></span><br></pre></td></tr></table></div></figure>



<p><strong>第二步，定位具体进程</strong></p>
<p>通过第一步，已经知道是某个节点上问题最大。</p>
<p><strong>登录机器，查看整体情况：</strong></p>
<figure class="highlight bash"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">top -c		<span class="comment"># 实时查看每个进程的CPU, 用&#x27;P&#x27;按CPU排序</span></span><br><span class="line"><span class="built_in">uptime</span>		<span class="comment"># 看 load average</span></span><br><span class="line">mpstat -P ALL 1	<span class="comment"># 看每个核的占用情况</span></span><br></pre></td></tr></table></div></figure>

<p>需要关注的有：</p>
<ul>
<li>是所有核打满，还是某几个核；</li>
<li>Load average很高但是 CPU iowait也高，可能是IO问题不是CPU算力问题；</li>
<li>top里能直接看到哪个进程%CPU排第一；</li>
</ul>
<p>或者用<code>ps</code>：</p>
<figure class="highlight bash"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ps -eo pid,ppid,user,%cpu,%mem,cmd --<span class="built_in">sort</span>=%cpu | <span class="built_in">head</span></span><br></pre></td></tr></table></div></figure>

<p>进而确定：<strong>PID + 可执行命令，确定是哪个服务。</strong> 在 K8s可以反查这个PID属于哪个容器（kubectl exec进Pod后用Top）</p>
<p><strong>第三步，线程级分析</strong></p>
<p>CPU 100%不是进程整体每一行代码一起跑，而是 <strong>某几个线程</strong> 在疯狂干活。</p>
<p>通过命令：</p>
<figure class="highlight bash"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">top -H -p &lt;pid&gt; 	<span class="comment"># 查看该进程中各线程 CPU 占用</span></span><br></pre></td></tr></table></div></figure>

<p>将占用最高的线程ID转成十六进制，如果是Go，就用 <code>go tool pprof</code> 查调用栈。进而明确 <strong>是那段代码在吞CPU。</strong></p>
<p><strong>第四步，代码层面</strong></p>
<p>结合业务指标，判断问题，常见业务指标有：</p>
<ul>
<li>QPS &#x2F; 请求数</li>
<li>RT &#x2F; 响应时间</li>
<li>错误率 (5xx，超时)</li>
<li>GC次数 &#x2F; 耗时</li>
<li>下游依赖（DB&#x2F;Redis&#x2F;外部接口）的QPS&amp;RT</li>
</ul>
<p>常见场景：</p>
<ul>
<li>QPS高+RT变慢+CPU高<ul>
<li>典型容量&#x2F;性能问题（流量变多、代码&#x2F;O(N2)&#x2F;SQL慢</li>
</ul>
</li>
<li>QPS正常甚至偏低+RT正常+CPU高<ul>
<li>死循环、自旋锁、空轮询，或者是某些后台业务疯掉</li>
</ul>
</li>
<li>错误率飙升+CPU高<ul>
<li>可能是某种异常疯狂重试，降级逻辑写挂了等</li>
</ul>
</li>
<li>system CPU高+context switch多<ul>
<li>线程数太多、频繁加锁&#x2F;解锁、频繁syscalls</li>
</ul>
</li>
</ul>

        <h2 id="深挖">
          <a href="#深挖" class="heading-link"><i class="fas fa-link"></i></a><a href="#深挖" class="headerlink" title="深挖"></a>深挖</h2>
      <p><strong>场景A：业务量确实变大 -&gt; 性能&#x2F;容量问题</strong></p>
<p><strong>场景B：业务平稳但CPU飙高 -&gt; 疑似死循环&#x2F;自锁&#x2F;bug</strong></p>
<p><strong>场景C：system CPU很高 -&gt; 系统调用&#x2F;内核开销大</strong></p>

        <h1 id="找到-10-亿个数中的-Top-K">
          <a href="#找到-10-亿个数中的-Top-K" class="heading-link"><i class="fas fa-link"></i></a><a href="#找到-10-亿个数中的-Top-K" class="headerlink" title="找到 10 亿个数中的 Top K"></a>找到 10 亿个数中的 Top K</h1>
      <p>假设你现在有 10 亿个整数，请你找出其中最大的 K 个数。如果数据量太大内存放不下，又该如何查找。</p>
<ul>
<li>能否从算法的角度理清思路</li>
<li>能否从工程角度考虑落地实现</li>
<li>能否说清楚时间复杂度和空间复杂度</li>
</ul>

        <h2 id="数据能放到内存中去">
          <a href="#数据能放到内存中去" class="heading-link"><i class="fas fa-link"></i></a><a href="#数据能放到内存中去" class="headerlink" title="数据能放到内存中去"></a>数据能放到内存中去</h2>
      <p><strong>维护一个大小为 K 的最小堆（Min-Heap）</strong></p>
<p>只维护当前最大的 K 个数：</p>
<ul>
<li>堆顶永远是这 K 个数里最小的</li>
<li>新数比堆顶大，加进来，并弹出最小值</li>
<li>新数比堆顶小，直接跳过</li>
</ul>
<p>**时间复杂度 O(NlogK)**，空间复杂度为 O(K)</p>

        <h2 id="10-亿数据放不下内存（真正的海量数据场景）">
          <a href="#10-亿数据放不下内存（真正的海量数据场景）" class="heading-link"><i class="fas fa-link"></i></a><a href="#10-亿数据放不下内存（真正的海量数据场景）" class="headerlink" title="10 亿数据放不下内存（真正的海量数据场景）"></a>10 亿数据放不下内存（真正的海量数据场景）</h2>
      <p>你不能一次读进内存，所以需要使用 <strong>外部排序</strong> 思路。</p>
<p><strong>先分片 + 部分排序 + 再归并（外部排序）</strong>,时间复杂度为 O(N logK)</p>
<p>chunksize &#x3D; UsableMemory * 0.7</p>
<ul>
<li>1.分片：把文件大小切成若干块，每块能放内存</li>
<li>对每个块做：<ul>
<li>排序；</li>
<li>去除每个块中的最大的K个；</li>
</ul>
</li>
<li>把所有的块（K*N个数）再用最小堆取一次 Top K，最终得到全局最大的 K 个。</li>
</ul>

        <h1 id="发生内存泄漏时如何定位">
          <a href="#发生内存泄漏时如何定位" class="heading-link"><i class="fas fa-link"></i></a><a href="#发生内存泄漏时如何定位" class="headerlink" title="发生内存泄漏时如何定位"></a>发生内存泄漏时如何定位</h1>
      <p>针对Go的项目，如果线上发生了内存泄漏，你会如何排查。</p>
<div class="table-container"><table>
<thead>
<tr>
<th>泄漏原因</th>
<th>pprof 特征</th>
<th>代码特征</th>
</tr>
</thead>
<tbody><tr>
<td><strong>goroutine 泄漏</strong></td>
<td>goroutine 数量持续上升</td>
<td>业务忘记退出、阻塞</td>
</tr>
<tr>
<td><strong>slice &#x2F; map 无限增长</strong></td>
<td>inuse_space 指向具体包和行号</td>
<td>append 不断插入；map cache 无淘汰</td>
</tr>
<tr>
<td><strong>未关闭 chan</strong></td>
<td>goroutine 堆积在 send&#x2F;recv</td>
<td>通常在管道关闭&#x2F;退出逻辑缺失</td>
</tr>
<tr>
<td><strong>Ticker 未 Stop()</strong></td>
<td>goroutine 堆积</td>
<td>新建 ticker 忘记 stop</td>
</tr>
<tr>
<td><strong>HTTP client 未关闭 Body</strong></td>
<td>pprof 显示大量 net&#x2F;http 内存</td>
<td>resp.Body 未 Close 导致泄漏连接</td>
</tr>
<tr>
<td><strong>缓存未清理（LFU&#x2F;LRU 不完善）</strong></td>
<td>map 占用内存大</td>
<td>自定义 cache 没有 eviction</td>
</tr>
<tr>
<td><strong>Cgo 导致内存泄漏</strong></td>
<td>Go 侧正常但 RSS 持续上涨</td>
<td>C 分配的内存未 free</td>
</tr>
</tbody></table></div>
</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2025/11/17/%E4%BA%91%E5%8E%9F%E7%94%9F%E5%9C%BA%E6%99%AF/">云原生场景</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2025-11-17</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2025-11-24</span></span></div></header><div class="post-body"><div class="post-excerpt">
        <h1 id="Kubernetes">
          <a href="#Kubernetes" class="heading-link"><i class="fas fa-link"></i></a><a href="#Kubernetes" class="headerlink" title="Kubernetes"></a>Kubernetes</h1>
      
        <h2 id="Pod启动后一直CrashLoopBackOff，该怎么排查">
          <a href="#Pod启动后一直CrashLoopBackOff，该怎么排查" class="heading-link"><i class="fas fa-link"></i></a><a href="#Pod启动后一直CrashLoopBackOff，该怎么排查" class="headerlink" title="Pod启动后一直CrashLoopBackOff，该怎么排查"></a>Pod启动后一直CrashLoopBackOff，该怎么排查</h2>
      <p><strong>1.查看 Pod 事件和状态</strong></p>
<p>先确认 Pod 为什么在重启。</p>
<figure class="highlight bash"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl describe pod &lt;pod-name&gt; -n &lt;namespace&gt;</span><br></pre></td></tr></table></div></figure>

<p>主要看以下几个参数：</p>
<ul>
<li><strong>Last State：Terminated</strong> （退出码、原因)</li>
<li><strong>Events</strong> （如无法挂载卷、拉取镜像失败、探针失败等）</li>
<li><strong>OOMKilled</strong> （内存不够）</li>
<li><strong>Error 或 Completed</strong> （非0退出）</li>
</ul>
<p><strong>2.查看 Pod 内部日志</strong></p>
<p>如果容器启动后马上退出，有可能日志里已经告诉你原因：</p>
<figure class="highlight bash"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl logs &lt;pod-name&gt; -n &lt;namespace&gt;</span><br></pre></td></tr></table></div></figure>

<p>Pod 有多个容器时：</p>
<figure class="highlight bash"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl logs &lt;pod-name&gt; -c &lt;container-name&gt; -n &lt;namespace&gt;</span><br></pre></td></tr></table></div></figure>

<p>如果是不断重启的容器，查看前一个容器的日志：</p>
<figure class="highlight bash"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl logs &lt;pod-name&gt; --previous</span><br></pre></td></tr></table></div></figure>

<p><strong>常见 CrashLoopBackOff 场景与排查方式</strong></p>
<ul>
<li><strong>应用本身异常退出</strong></li>
</ul>
<p>​	症状：<code>ExitCode: 1</code> 或 <code>2</code><br>​    解决：根据日志修复应用错误。</p>
<p>​    比如：配置文件错误、数据库连接失败、程序抛出异常、找不到启动命令。</p>
<ul>
<li><strong>健康检查探针失败（Liveness&#x2F;Readiness&#x2F;Startup）</strong></li>
</ul>
<p>​	查看 describe 中是否有：</p>
<figure class="highlight nginx"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">Liveness</span> probe failed</span><br><span class="line">Readiness probe failed</span><br></pre></td></tr></table></div></figure>

<p>排查方向：探针是否正确、探针请求路径是否存在、应用启动时间太长(initialDelaySeconds、timeoutSeconds)等</p>
<ul>
<li><strong>OOMKilled（内存不足）</strong></li>
</ul>
<p>查看 describe 是否有：</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">State: Terminated</span><br><span class="line">Reason: OOMKilled</span><br></pre></td></tr></table></div></figure>

<p>解决办法：增大 Pod 内存Limits；如果设置了limit而没设置request，可能造成调度问题也会 OOM.</p>
<ul>
<li><p><strong>权限或文件系统问题</strong></p>
<p>事件中可能看到：无法挂载卷、权限不足。可以检查：</p>
<figure class="highlight bash"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl describe pod &lt;pod&gt;</span><br><span class="line"><span class="comment"># 或者查看挂载情况</span></span><br><span class="line">kubectl <span class="built_in">exec</span> -it &lt;pod&gt; --<span class="built_in">ls</span> -l &lt;path&gt;</span><br></pre></td></tr></table></div></figure>


</li>
<li><p><strong>容器 Entrypointt &#x2F; CMD 配置错误</strong></p>
</li>
</ul>
<p>常见错误：入口脚本找不到、镜像打包不完整、Dockerfile没有正确CMD&#x2F;ENTTRYPOINT。</p>
<p>查看：</p>
<figure class="highlight bash"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl describe pod &lt;pod&gt; | grep Command -A 2</span><br></pre></td></tr></table></div></figure>

<ul>
<li>环境变量缺失或配置错误</li>
<li>镜像异常（例如程序 Crash 或基础镜像损坏）</li>
</ul>
<p><strong>进一步调试</strong> ： 让 Pod 启动时先不执行程序，只执行 sleep：</p>
<p>给Deployment 加一个临时 command:</p>
<figure class="highlight yaml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">command:</span> [<span class="string">&quot;sh&quot;</span>, <span class="string">&quot;-c&quot;</span>, <span class="string">&quot;sleep 3000&quot;</span>]</span><br></pre></td></tr></table></div></figure>




        <h1 id="如何排查-Service-无法访问的问题">
          <a href="#如何排查-Service-无法访问的问题" class="heading-link"><i class="fas fa-link"></i></a><a href="#如何排查-Service-无法访问的问题" class="headerlink" title="如何排查 Service 无法访问的问题"></a>如何排查 Service 无法访问的问题</h1>
      <p><strong>1.Service 是否存在、端口是否正确</strong></p>
<figure class="highlight bash"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl get svc -n &lt;ns&gt;</span><br><span class="line">kubectl describe svc &lt;svc-name&gt; -n &lt;ns&gt;</span><br></pre></td></tr></table></div></figure>

<p>检查：</p>
<ul>
<li>Service 类型（ClusterIP &#x2F; Nodeport &#x2F; LoadBalancer）</li>
<li>Port &#x2F; targetPort 是否对应 Pod 监听端口</li>
<li>Selector 是否正确匹配 Pod Label</li>
<li>ClusterIP 是否正常分配</li>
</ul>
<p><strong>2.检查Endpoints &#x2F; EndpoinSlice（是否成功绑定 Pod）</strong></p>
<figure class="highlight bash"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl get endpoints &lt;svc-name&gt; -n &lt;ns&gt;</span><br><span class="line">kubectl describe endpoints &lt;svc-name&gt; -n &lt;ns&gt;</span><br><span class="line">kubectl get endpoinsslice -n &lt;ns&gt; | grep &lt;svc-name&gt;</span><br></pre></td></tr></table></div></figure>

<p>如果看到：</p>
<figure class="highlight makefile"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">Endpoints: &lt;none&gt;</span></span><br></pre></td></tr></table></div></figure>

<p>说明Service找不到Pod，原因可能是：</p>
<ul>
<li>Pod的label与Service的selector不一致；</li>
<li>Pod NotReady: Readiness probe failed</li>
<li>Pod不属于同一namespace</li>
</ul>
<p><strong>3.确认Pod是否实际监听 targetPort</strong></p>
<p>进入Pod内部检查：</p>
<figure class="highlight bash"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl <span class="built_in">exec</span> -it &lt;pod&gt; -n &lt;ns&gt; -- netstat -ntlp</span><br><span class="line"><span class="comment"># 或者</span></span><br><span class="line">ss -lntp</span><br></pre></td></tr></table></div></figure>




        <h1 id="Pod-OOMKilled-的原因？如何定位？">
          <a href="#Pod-OOMKilled-的原因？如何定位？" class="heading-link"><i class="fas fa-link"></i></a><a href="#Pod-OOMKilled-的原因？如何定位？" class="headerlink" title="Pod OOMKilled 的原因？如何定位？"></a>Pod OOMKilled 的原因？如何定位？</h1>
      <p>当容器内存占用超过Pod的memory limi时，Kubernetes会触发OOM，直接杀死容器进程，状态显示：</p>
<figure class="highlight makefile"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">State: Terminated</span></span><br><span class="line"><span class="section">Reason: OOMKilled</span></span><br></pre></td></tr></table></div></figure>

<p><strong>1.应用内存使用超过了limit</strong></p>
<ul>
<li>堆太大</li>
<li>go程序内存泄漏</li>
<li>redis&#x2F;mongodb缓存占满</li>
<li>请求量突然增大，内存峰值高</li>
</ul>
<p><strong>2. memory request 与 limit 配置不合理</strong></p>
<p>比如：</p>
<figure class="highlight yaml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">resources:</span></span><br><span class="line">  <span class="attr">requests:</span></span><br><span class="line">    <span class="attr">memory:</span> <span class="string">512Mi</span></span><br><span class="line">  <span class="attr">limits:</span></span><br><span class="line">    <span class="attr">memory:</span> <span class="string">512Mi</span></span><br></pre></td></tr></table></div></figure>

<p>应用稍微抖动就超过 limit → 立刻 OOM。</p>
<p><strong>3.节点本身内存不足（Node OOM）</strong></p>
<p>即使容器没超过 limit，但节点内存不足，可能触发：</p>
<ol>
<li>cgroup OOM</li>
<li>kubelet EvictPod</li>
</ol>

        <h2 id="定位思路">
          <a href="#定位思路" class="heading-link"><i class="fas fa-link"></i></a><a href="#定位思路" class="headerlink" title="定位思路"></a>定位思路</h2>
      <p><strong>1.查看Pod状态（确定是否是OOMKilled）</strong></p>
<figure class="highlight bash"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl describe pod &lt;pod&gt; -n &lt;ns&gt;</span><br></pre></td></tr></table></div></figure>

<p>检查：</p>
<figure class="highlight yaml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">Last State:</span></span><br><span class="line">  <span class="attr">Reason:</span>       <span class="string">OOMKilled</span></span><br><span class="line">  <span class="attr">Exit Code:</span>    <span class="number">137</span></span><br></pre></td></tr></table></div></figure>

<p>Exit Code 137 &#x3D; 128 + 9，容器被系统杀死。</p>
<p><strong>2.确认容器的memory limit</strong></p>
<figure class="highlight bash"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pod &lt;pod&gt; -o jsonpath=<span class="string">&#x27;&#123;.spec.container[0].resources&#125;&#x27;</span></span><br></pre></td></tr></table></div></figure>

<p>重点看,如果limits低，一定会OOM：</p>
<ul>
<li>limits.memory</li>
<li>requests.memory</li>
</ul>
<p><strong>3.查看容器真实内存使用量</strong></p>
<p>使用 metric-server 或 Prometheus:</p>
<figure class="highlight bash"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl top pod &lt;pod&gt; -n &lt;ns&gt;</span><br></pre></td></tr></table></div></figure>

<p>如果看到：</p>
<figure class="highlight scss"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">MEMORY</span>(%) <span class="number">150%</span> 或 MEM &gt; limit</span><br></pre></td></tr></table></div></figure>

<p>则OOM。</p>
<p><strong>4.查看内存历史曲线</strong></p>
<p>通过k8s dashboard查看：</p>
<ul>
<li>内存是否慢慢上涨→内存泄漏</li>
<li>峰值是否在某个时刻暴涨→大流量&#x2F;大对象</li>
</ul>
<p><strong>5.检查节点内存是否不足（Node OOM）</strong></p>
<figure class="highlight bash"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl describe node &lt;node-name&gt;</span><br></pre></td></tr></table></div></figure>

<p>查看是否有：</p>
<figure class="highlight sql"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">System</span> OOM encounterer</span><br><span class="line">Evition Threshould crossed</span><br></pre></td></tr></table></div></figure>

<p>或者</p>
<figure class="highlight nginx"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">The</span> node was low <span class="literal">on</span> resource: memory.</span><br></pre></td></tr></table></div></figure>

<p>进而确定是否是node OOM，而不是应用的 OOM。</p>
<p><strong>6.查看语言运行时参数</strong></p>
<p>对于Go语言：</p>
<ul>
<li>检查 goroutine 是否泄漏；</li>
<li>map 无限制增长；</li>
<li>slice 扩容次数多等；</li>
</ul>

        <h1 id="Ingress-和-Service-的区别？什么时候用-Ingress？">
          <a href="#Ingress-和-Service-的区别？什么时候用-Ingress？" class="heading-link"><i class="fas fa-link"></i></a><a href="#Ingress-和-Service-的区别？什么时候用-Ingress？" class="headerlink" title="Ingress 和 Service 的区别？什么时候用 Ingress？"></a>Ingress 和 Service 的区别？什么时候用 Ingress？</h1>
      <p><strong>Service &#x3D; 暴露服务的网络抽象，Service 负责把流量送到 Pod。</strong> </p>
<p><strong>Ingress &#x3D; 对外 HTTP 入口 + 反向代理 + 路由， Ingress 负责把互联网的 HTTP 流量路由进来。</strong></p>
<p>两者通常 <strong>一起使用</strong>。Ingress 后端必须是 Service。</p>
<div class="table-container"><table>
<thead>
<tr>
<th align="center">项目</th>
<th align="center">Service</th>
<th align="center">Ingress</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><strong>作用</strong></td>
<td align="center">让集群内或外部访问 Pod</td>
<td align="center">提供 HTTP&#x2F;HTTPS 七层路由，管理多个服务的外部访问</td>
</tr>
<tr>
<td align="center"><strong>工作层级</strong></td>
<td align="center">四层（L4，TCP&#x2F;UDP）</td>
<td align="center">七层（L7，HTTP&#x2F;HTTPS）</td>
</tr>
<tr>
<td align="center"><strong>暴露方式</strong></td>
<td align="center">ClusterIP &#x2F; NodePort &#x2F; LoadBalancer</td>
<td align="center">通过域名和路径路由入站流量</td>
</tr>
<tr>
<td align="center"><strong>是否提供域名&#x2F;路径路由</strong></td>
<td align="center">❌ 不支持</td>
<td align="center">✔ 支持</td>
</tr>
<tr>
<td align="center"><strong>是否需要 Ingress Controller</strong></td>
<td align="center">❌ 不需要</td>
<td align="center">✔ 必须有（如 nginx &#x2F; traefik &#x2F; istio）</td>
</tr>
<tr>
<td align="center"><strong>典型用途</strong></td>
<td align="center">服务发现、Pod 访问入口</td>
<td align="center">对外统一入口、域名管理、SSL、反向代理</td>
</tr>
</tbody></table></div>
<p><strong>Ingress 的意义（解决了什么问题）</strong></p>
<p>你有 10 个服务，希望用域名访问，比如：</p>
<ul>
<li>api.example.com</li>
<li>web.example.com</li>
<li>admin.example.com</li>
</ul>
<p>如果不用 Ingress，你得为每个 Service 创建一个 <strong>LoadBalancer</strong>：</p>
<ul>
<li>10 个 LB → 费用高</li>
<li>每个 LB 有一个公网 IP → 不方便管理</li>
<li>无法统一做 HTTPS、WAF、路由</li>
</ul>
<p>使用 Ingress 后：</p>
<p><strong>只需要 1 个 LoadBalancer</strong><br>通过 Ingress Controller 做路由：</p>
<figure class="highlight text"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">api.example.com   -&gt; service api-svc</span><br><span class="line">web.example.com   -&gt; service web-svc</span><br><span class="line">/admin            -&gt; service admin-svc</span><br></pre></td></tr></table></div></figure>

<p>甚至可以基于路径：</p>
<ul>
<li>&#x2F;api → api 服务</li>
<li>&#x2F;web → 前端服务</li>
</ul>
<p>还能加：</p>
<ul>
<li>HTTPS 证书</li>
<li>限流</li>
<li>重试</li>
<li>反向代理</li>
<li>统一入口日志</li>
</ul>
<p><strong>什么时候用Ingress</strong></p>
<p>当你需要对外 HTTP &#x2F; HTTPS 访问时→用 Ingress</p>
<ul>
<li>需要域名访问（xxx.com）</li>
<li>需要路径路由（&#x2F;api、&#x2F;app）</li>
<li>需要 HTTPS（自带 TLS 管理）</li>
<li>需要统一入口负载均衡</li>
<li>不想创建多个外部 LoadBalancer</li>
<li>需要 nginx 级别的功能：重写、限流、请求头修改等</li>
</ul>
<p><strong>什么时候不用Ingress</strong></p>
<ul>
<li>服务仅集群内部访问 → 用 ClusterIP 即可</li>
<li>用 gRPC&#x2F;TCP&#x2F;UDP 等非 HTTP 协议 → 用 Service（L4）</li>
<li>你用的是 Service Mesh（如 istio Gateway） → Gateway 替代 Ingress</li>
</ul>
</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2025/11/09/Transformer%E5%85%A5%E9%97%A8/">Transformer入门</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2025-11-09</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2025-11-10</span></span></div></header><div class="post-body"><div class="post-excerpt">
        <h1 id="Transformer入门">
          <a href="#Transformer入门" class="heading-link"><i class="fas fa-link"></i></a><a href="#Transformer入门" class="headerlink" title="Transformer入门"></a>Transformer入门</h1>
      
        <h2 id="神经网络">
          <a href="#神经网络" class="heading-link"><i class="fas fa-link"></i></a><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2>
      
        <h3 id="循环神经网络（RNN）">
          <a href="#循环神经网络（RNN）" class="heading-link"><i class="fas fa-link"></i></a><a href="#循环神经网络（RNN）" class="headerlink" title="循环神经网络（RNN）"></a>循环神经网络（RNN）</h3>
      <p><strong>缺点：</strong></p>
<ul>
<li>无法进行并行计算，只能串行就算，长语句输入，计算效率低；</li>
<li>梯度消失，梯度爆炸，链式信息损失，难以实现长期记忆；</li>
</ul>

        <h3 id="卷积神经网络（CNN）">
          <a href="#卷积神经网络（CNN）" class="heading-link"><i class="fas fa-link"></i></a><a href="#卷积神经网络（CNN）" class="headerlink" title="卷积神经网络（CNN）"></a>卷积神经网络（CNN）</h3>
      <p><strong>优点：</strong></p>
<ul>
<li>可以进行并行计算，计算效率高；</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>一个filter感受长度有限，如果输入长度很长，需要叠很多层来感受到完整句子，如果不满足这些层数，就无法感受完整的句子；</li>
</ul>

        <h2 id="Transformer特点">
          <a href="#Transformer特点" class="heading-link"><i class="fas fa-link"></i></a><a href="#Transformer特点" class="headerlink" title="Transformer特点"></a>Transformer特点</h2>
      
        <h3 id="Inputs">
          <a href="#Inputs" class="heading-link"><i class="fas fa-link"></i></a><a href="#Inputs" class="headerlink" title="Inputs"></a>Inputs</h3>
      <p><strong>为什么会出现Transformer，诉求：</strong></p>
<ul>
<li>高计算效率，可以做并行计算；</li>
<li>需要长期记忆；</li>
</ul>
<p><strong>相对于RNN，Transformer特点：</strong></p>
<ul>
<li>高效率计算，可以做并行计算；</li>
<li>固有的全局视野，能够捕获长距离依赖；</li>
</ul>
<p><strong>相对于CNN，Transformer特点：</strong></p>
<ul>
<li>无局限的感受长度；</li>
<li>能够更加灵活地处理位置信息；</li>
</ul>

        <h3 id="Tokenize-分词">
          <a href="#Tokenize-分词" class="heading-link"><i class="fas fa-link"></i></a><a href="#Tokenize-分词" class="headerlink" title="Tokenize(分词)"></a>Tokenize(分词)</h3>
      <p>单词级别的分词；</p>
<p>字符级别的分词；</p>
<p>字节级的分词；</p>
<p>中文分词；</p>

        <h3 id="词嵌入（Embedding）">
          <a href="#词嵌入（Embedding）" class="heading-link"><i class="fas fa-link"></i></a><a href="#词嵌入（Embedding）" class="headerlink" title="词嵌入（Embedding）"></a>词嵌入（Embedding）</h3>
      <p>将词的向量转为特征向量；</p>
<p>词嵌入矩阵维度：需要嵌入的一句句子中词汇的总数（<strong>sequence</strong>）*嵌入向量的维度（<strong>d_model</strong>）；</p>
<p>Embedding是缺乏词的位置信息的；</p>

        <h3 id="位置编码（Positional-Encoding）">
          <a href="#位置编码（Positional-Encoding）" class="heading-link"><i class="fas fa-link"></i></a><a href="#位置编码（Positional-Encoding）" class="headerlink" title="位置编码（Positional Encoding）"></a>位置编码（Positional Encoding）</h3>
      <p>偶数位置</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// pos是元素在序列中的位置</span><br><span class="line">// i是编码向量中的维度索引</span><br><span class="line">// d_model是嵌入式向量的维度，及模型的维度</span><br><span class="line">PE(pos, 2i) = sin(pos/10000^(2i/d_model))  // 表示位置pos在编码向量中的第2i个维度的值；</span><br></pre></td></tr></table></div></figure>

<p>奇数位置</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">PE(pos, 2i+1) = cos(pos/10000^(2i/d_model))	// 表示位置pos在编码向量中的第2i+1个维度的值；</span><br></pre></td></tr></table></div></figure>

<p><strong>这样设计编码的好处：</strong></p>
<ul>
<li><strong>唯一性：</strong> 每个位置的编码是唯一的，这确保了模型能够区分序列中的不同位置；</li>
<li><strong>周期性：</strong> 能够根据相位捕捉位置关系；</li>
<li><strong>正交性：</strong> 偶数位置和奇数位置的编码是正交的，增加了编码的区分度和信息丰富度；</li>
</ul>

        <h3 id="编码器模块（Encoder-Block）">
          <a href="#编码器模块（Encoder-Block）" class="heading-link"><i class="fas fa-link"></i></a><a href="#编码器模块（Encoder-Block）" class="headerlink" title="编码器模块（Encoder Block）"></a>编码器模块（Encoder Block）</h3>
      <img src="/2025/11/09/Transformer%E5%85%A5%E9%97%A8/transformer.png" class title="transformer">


        <h3 id="自注意力-Self-attention">
          <a href="#自注意力-Self-attention" class="heading-link"><i class="fas fa-link"></i></a><a href="#自注意力-Self-attention" class="headerlink" title="自注意力 (Self-attention)"></a>自注意力 (Self-attention)</h3>
      
        <h3 id="多头注意力层-Multi-Head-Attention">
          <a href="#多头注意力层-Multi-Head-Attention" class="heading-link"><i class="fas fa-link"></i></a><a href="#多头注意力层-Multi-Head-Attention" class="headerlink" title="多头注意力层(Multi-Head Attention)"></a>多头注意力层(Multi-Head Attention)</h3>
      <p>每个头独立地关注输入的不同表示子空间，从而能够捕捉不同方面的信息，并综合这些信息来更全面地理解文本。</p>

        <h3 id="残差-amp-层归一-Add-amp-Layer-Normalization">
          <a href="#残差-amp-层归一-Add-amp-Layer-Normalization" class="heading-link"><i class="fas fa-link"></i></a><a href="#残差-amp-层归一-Add-amp-Layer-Normalization" class="headerlink" title="残差&amp;层归一(Add &amp; Layer Normalization)"></a>残差&amp;层归一(Add &amp; Layer Normalization)</h3>
      <p><strong>残差的目的</strong></p>
<p>虽然每次循环都在充分地挖掘特征，但是希望能把以前循环地挖掘结果一并记住。</p>
<p><strong>归一的目的</strong></p>
<p>有助于稳定神经网络的学习过程，减少训练时间，并有时还可以提高模型的最终性能。</p>

        <h3 id="前馈神经网络（Feed-Forward）">
          <a href="#前馈神经网络（Feed-Forward）" class="heading-link"><i class="fas fa-link"></i></a><a href="#前馈神经网络（Feed-Forward）" class="headerlink" title="前馈神经网络（Feed Forward）"></a>前馈神经网络（Feed Forward）</h3>
      <p><strong>目的</strong></p>
<ul>
<li>增加网络结构的非线性，原来只是个加权求和的过程；</li>
<li>增加参数量，神经元带来大量可训练的参数，提升模型复杂度和深度，使模型可以处理更复杂的任务。</li>
</ul>

        <h3 id="解码器模块-Decoder-Block">
          <a href="#解码器模块-Decoder-Block" class="heading-link"><i class="fas fa-link"></i></a><a href="#解码器模块-Decoder-Block" class="headerlink" title="解码器模块(Decoder Block)"></a>解码器模块(Decoder Block)</h3>
      <p><strong>Decoder block整体用于：</strong></p>
<ul>
<li>集成上下文信息；</li>
<li>自回归生成输出；</li>
<li>保证输出时序性；</li>
</ul>

        <h3 id="遮蔽多头注意力-Masked-Multi-Head-Attention">
          <a href="#遮蔽多头注意力-Masked-Multi-Head-Attention" class="heading-link"><i class="fas fa-link"></i></a><a href="#遮蔽多头注意力-Masked-Multi-Head-Attention" class="headerlink" title="遮蔽多头注意力 Masked Multi-Head Attention"></a>遮蔽多头注意力 Masked Multi-Head Attention</h3>
      <p><strong>原理：</strong> 通过对注意力分数矩阵应用一个遮蔽（一个上三角矩阵，其中未来位置的元素被设置为非常大的负数），在计算softmax前有效地将这些位置地注意力分数降低到接近0。这意味着生成序列的每一步，模型只能注意到当前位置的词或标记。</p>
<p><strong>目的：</strong> 遮蔽多头注意力机制确保了在解码器生成当前输出时，不会受到未来输出的影响。</p>

        <h3 id="交叉注意力-Cross-Attention">
          <a href="#交叉注意力-Cross-Attention" class="heading-link"><i class="fas fa-link"></i></a><a href="#交叉注意力-Cross-Attention" class="headerlink" title="交叉注意力 Cross Attention"></a>交叉注意力 Cross Attention</h3>
      <p><strong>原理：</strong> 解码器使用当前的状态作为查询，与编码器的输出（键和值）进行交互，通过注意力机制确定编码器输出中的哪些部分是重要的，并据此生成下一个输出元素。</p>
<p><strong>目的：</strong> 融合两个不同序列或信息源的特征。</p>

        <h3 id="线性层-amp-Softmax层-Linear-amp-Softmax">
          <a href="#线性层-amp-Softmax层-Linear-amp-Softmax" class="heading-link"><i class="fas fa-link"></i></a><a href="#线性层-amp-Softmax层-Linear-amp-Softmax" class="headerlink" title="线性层&amp;Softmax层 (Linear &amp; Softmax)"></a>线性层&amp;Softmax层 (Linear &amp; Softmax)</h3>
      <p><strong>原理：</strong> decoder输出为[100, 512]，这个512是抽象的特征，无法指导我具体下面该生成哪个概率最大的词，所以我要向办法把[100, 512]的矩阵映射到[100, 1000]，其中每个维度的值代表了相应单词作为序列下一单词的未归一化分数，再利用Softmax做下归一，就能得到10000个词的概率。</p>

        <h3 id="Transformer推理过程">
          <a href="#Transformer推理过程" class="heading-link"><i class="fas fa-link"></i></a><a href="#Transformer推理过程" class="headerlink" title="Transformer推理过程"></a>Transformer推理过程</h3>
      <p><strong>贪心搜索(Greedy)：</strong> 每次去概率最大的词作为推理输出；</p>
<p><strong>宽度有限搜索(Beam Search)：</strong> 保留了若干个最有可能的候选序列，最后输出概率最大的序列作为推理输出；</p>
</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2025/10/19/k8s%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/">k8s集群环境搭建.md</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2025-10-19</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2025-10-19</span></span></div></header><div class="post-body"><div class="post-excerpt">
        <h1 id="Kubernetes集群">
          <a href="#Kubernetes集群" class="heading-link"><i class="fas fa-link"></i></a><a href="#Kubernetes集群" class="headerlink" title="Kubernetes集群"></a>Kubernetes集群</h1>
      
        <h2 id="环境准备">
          <a href="#环境准备" class="heading-link"><i class="fas fa-link"></i></a><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h2>
      <div class="table-container"><table>
<thead>
<tr>
<th align="center">节点</th>
<th align="center">ip</th>
</tr>
</thead>
<tbody><tr>
<td align="center">k8s-node1</td>
<td align="center">192.168.157.21</td>
</tr>
<tr>
<td align="center">k8s-node2</td>
<td align="center">192.168.157.22</td>
</tr>
<tr>
<td align="center">k8s-node3</td>
<td align="center">192.168.157.23</td>
</tr>
</tbody></table></div>
</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2025/07/28/%E9%A1%B6%E7%BA%A7%E5%B9%B2%E8%B4%A7/">顶级干货.md</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2025-07-28</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2025-08-26</span></span></div></header><div class="post-body"><div class="post-excerpt">
        <h1 id="顶级龙头战法实操">
          <a href="#顶级龙头战法实操" class="heading-link"><i class="fas fa-link"></i></a><a href="#顶级龙头战法实操" class="headerlink" title="顶级龙头战法实操"></a>顶级龙头战法实操</h1>
      <ul>
<li>批量大单一字，定题材的方向和强度</li>
<li>大题材启动，要第一时间上车，溢价买入都要跟着上车，指数的分歧，反而是上车题材前排、核心的最好机会</li>
<li>一轮大题材即将启动，需要一个来打高度，若高度票给机会，一定要先上车</li>
</ul>
<img src="/2025/07/28/%E9%A1%B6%E7%BA%A7%E5%B9%B2%E8%B4%A7/%E9%BE%99%E5%A4%B4%E6%89%93%E6%9D%BF.png" class title="龙头打板">

<p><strong>上车</strong></p>
<ul>
<li><p>大题材启动，没上到短线票，找能容纳大资金的中军上车，安全稳定；</p>
</li>
<li><p>第一买点的核心票是PK出来的，只能临盘判断，需要综合比较辨识，等走成核心票了再入场就是第二买点；</p>
<img src="/2025/07/28/%E9%A1%B6%E7%BA%A7%E5%B9%B2%E8%B4%A7/%E9%BE%99%E5%A4%B4%E6%88%98%E6%B3%95%E5%AE%9E%E9%99%85%E6%93%8D%E4%BD%9C%E4%B8%8A%E8%BD%A6.png" class title="龙头战法实际操作上车"></li>
</ul>

        <h1 id="题材运行规律">
          <a href="#题材运行规律" class="heading-link"><i class="fas fa-link"></i></a><a href="#题材运行规律" class="headerlink" title="题材运行规律"></a>题材运行规律</h1>
      <p>启动</p>
<p>高潮</p>
<p>分化、分歧</p>
<p>退潮</p>
<p>二波</p>

        <h2 id="一、题材大小">
          <a href="#一、题材大小" class="heading-link"><i class="fas fa-link"></i></a><a href="#一、题材大小" class="headerlink" title="一、题材大小"></a>一、题材大小</h2>
      <ul>
<li>政策驱动题材</li>
<li>事件驱动题材</li>
<li>革命性产品驱动的题材</li>
</ul>

        <h2 id="二、题材炒作节奏">
          <a href="#二、题材炒作节奏" class="heading-link"><i class="fas fa-link"></i></a><a href="#二、题材炒作节奏" class="headerlink" title="二、题材炒作节奏"></a>二、题材炒作节奏</h2>
      <p>一个大的题材，常有三个大的波段</p>
<ul>
<li>炒概念炒预期</li>
<li>炒项目落地</li>
<li>炒业绩</li>
</ul>

        <h2 id="启动">
          <a href="#启动" class="heading-link"><i class="fas fa-link"></i></a><a href="#启动" class="headerlink" title="启动"></a>启动</h2>
      <p><strong>冷启动</strong></p>
<p>首板，半路前排；</p>
<p>1进2用卡位晋级、低吸；</p>
<p>2B以上卡位晋级，竞价三一，爆量转一致；</p>
<p><strong>热启动</strong> ：启动即高潮</p>
<ul>
<li>7-8个一字，能买到的就已经排第9了，后排竞价过热，盘中有分歧，后排开始炸板，等待前排开板，被带开的前排票，就是炸板低接机会；</li>
<li>竞价三一，分歧在竞价阶段就解决了，开盘就秒板，竞价换手、竞价金额，两市前三有地位、有身位、有逻辑；</li>
<li>中军、趋势容量票、300、688、ETF</li>
</ul>
<p><strong>三一原则的定义</strong></p>
<p>“竞价三一票”是短线交易中的一种策略模式，核心是通过‌<strong>竞价阶段的强度筛选</strong>‌锁定最具潜力的标的，其操作逻辑围绕 ‌**”第一、唯一、专一”**‌ 三大原则展开‌。以下是关键要点解析：</p>
<div class="table-container"><table>
<thead>
<tr>
<th><strong>原则</strong>‌</th>
<th>‌<strong>内涵解析</strong>‌</th>
<th>‌<strong>操作示例</strong>‌</th>
</tr>
</thead>
<tbody><tr>
<td>‌<strong>第一</strong>‌</td>
<td>• ‌<strong>竞价综合强度第一</strong>‌：全市场&#x2F;板块内竞价金额、换手率、涨幅等维度领先者 • ‌<strong>身位第一</strong>‌：同梯队（如连板、首板、反包）中最早异动或涨停者‌</td>
<td>若某股竞价成交额全市场第1，且为板块内唯一3连板股，即符合”第一”属性‌</td>
</tr>
<tr>
<td>‌<strong>唯一</strong>‌</td>
<td>‌<strong>不可替代性</strong>‌：题材炒作中资金绕不开的标杆标的（如行业龙头、稀缺逻辑承载者）‌</td>
<td>科技行情中的核心EDA企业、基建周期的绝对龙头等‌</td>
</tr>
<tr>
<td>‌<strong>专一</strong>‌</td>
<td>‌<strong>聚焦策略</strong>‌：小资金只参与符合前两项的标的，避免分散操作杂毛股‌</td>
<td>早盘仅盯住1-2只三一票，封板失败则放弃当日交易‌</td>
</tr>
</tbody></table></div>

        <h2 id="高潮">
          <a href="#高潮" class="heading-link"><i class="fas fa-link"></i></a><a href="#高潮" class="headerlink" title="高潮"></a>高潮</h2>
      <p>二次高潮</p>
<p><strong>汰弱留强</strong> ，竞价三一题材、梯队、两市最强；</p>
<p>越强越容易竞价阶段解决完分歧，开盘直接转一致上板；</p>
<p>别的票都是大单没有换手，竞价三一有换手，<strong>换手票&gt;一字票</strong>；</p>
<p><strong>逻辑&gt;情绪&gt;成本&gt;技术</strong></p>
<p><strong>注意：</strong> 能买的不敢买，买到的都是坑</p>

        <h2 id="分化、分歧">
          <a href="#分化、分歧" class="heading-link"><i class="fas fa-link"></i></a><a href="#分化、分歧" class="headerlink" title="分化、分歧"></a>分化、分歧</h2>
      <p><strong>分化：</strong> 龙头不断板，首次分化必有回流，水下等题材回流拉升减仓出局为主</p>
<p>核心票：水下还是低吸机会</p>
<p><strong>分歧：</strong> 龙头断板，“高开是罪”，竞价越“强”（分歧大的表现），容易被砸死，龙头断板，风险在次日</p>
<p><strong>分歧后的三种走法：</strong></p>
<ul>
<li>龙头断板直接反包，爆量后直接打高度</li>
<li>温和断板，高位横盘整理，锁定空间</li>
<li>A杀，压制空间</li>
<li>龙头断版次日，直接核按，退潮，冲高出局为主</li>
</ul>
<p>新周期，新高度；</p>
<p>补涨龙超预期打破前高龙头身位，就打开短线新周期，接着奏乐接着舞</p>
<p><strong>分化：</strong> 卡位晋级，汰弱留强，<strong>分化必有回流</strong>去低吸核心票</p>

        <h2 id="退潮">
          <a href="#退潮" class="heading-link"><i class="fas fa-link"></i></a><a href="#退潮" class="headerlink" title="退潮"></a>退潮</h2>
      <p><strong>特点：</strong> 竞价不是冰点，盘中可能还有更冰的点；</p>
<p><strong>出货方法</strong></p>
<p>撤退龙：用利润垫在跑，处理持仓为主，弱的砍掉，强的继续留着；</p>
<p>退潮期如何逃顶，顶部信号线</p>

        <h2 id="二波">
          <a href="#二波" class="heading-link"><i class="fas fa-link"></i></a><a href="#二波" class="headerlink" title="二波"></a>二波</h2>
      <ul>
<li>反核、翘板 5原则</li>
<li>只做第一，不做跟风，不做后排</li>
<li>回光返照龙</li>
<li>龙头反包，或新的分支龙头打破老龙高度</li>
</ul>

        <h1 id="趋势票，如何买在起涨点">
          <a href="#趋势票，如何买在起涨点" class="heading-link"><i class="fas fa-link"></i></a><a href="#趋势票，如何买在起涨点" class="headerlink" title="趋势票，如何买在起涨点"></a>趋势票，如何买在起涨点</h1>
      
        <h2 id="1-选股">
          <a href="#1-选股" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-选股" class="headerlink" title="1.选股"></a>1.选股</h2>
      <ul>
<li>三日选股法（60%-70%）：三日内涨停、断板、炸板的票；</li>
<li>DDE：涨幅排名（30%-40%）；</li>
</ul>

        <h2 id="2-上车">
          <a href="#2-上车" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-上车" class="headerlink" title="2. 上车"></a>2. 上车</h2>
      <ul>
<li>底部放量，平移整理，不要跌破一些关键支撑</li>
</ul>
<img src="/2025/07/28/%E9%A1%B6%E7%BA%A7%E5%B9%B2%E8%B4%A7/image-20250818223640629.png" class title="image-20250818223640629">

<ul>
<li>量窒息：筹码相对稳定，且多头，空头趋于平衡了</li>
<li>主力建仓动作+量窒息+次日收红信号（反转信号）</li>
<li><img src="/2025/07/28/%E9%A1%B6%E7%BA%A7%E5%B9%B2%E8%B4%A7/image-20250818230710817.png" class title="image-20250818230710817"></li>
<li>底部放量，N形调整</li>
</ul>
<img src="/2025/07/28/%E9%A1%B6%E7%BA%A7%E5%B9%B2%E8%B4%A7/image-20250818232908182.png" class title="image-20250818232908182">

<ul>
<li><p><strong>放量上涨后，次日水下缩量整理+不破关键支撑+净额大额流入</strong></p>
</li>
<li><p>底部放量，向上调整</p>
<img src="/2025/07/28/%E9%A1%B6%E7%BA%A7%E5%B9%B2%E8%B4%A7/image-20250818234615700.png" class title="image-20250818234615700"></li>
</ul>

        <h2 id="3-持股、做T">
          <a href="#3-持股、做T" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-持股、做T" class="headerlink" title="3.持股、做T"></a>3.持股、做T</h2>
      
        <h2 id="4-出局">
          <a href="#4-出局" class="heading-link"><i class="fas fa-link"></i></a><a href="#4-出局" class="headerlink" title="4. 出局"></a>4. 出局</h2>
      
        <h1 id="趋势票三个买点">
          <a href="#趋势票三个买点" class="heading-link"><i class="fas fa-link"></i></a><a href="#趋势票三个买点" class="headerlink" title="趋势票三个买点"></a>趋势票三个买点</h1>
      <img src="/2025/07/28/%E9%A1%B6%E7%BA%A7%E5%B9%B2%E8%B4%A7/%E8%B6%8B%E5%8A%BF%E7%A5%A8%E4%B8%89%E4%B8%AA%E4%B9%B0%E7%82%B9.png" class title="趋势票三个买点">




        <h1 id="股票如何卖在最高">
          <a href="#股票如何卖在最高" class="heading-link"><i class="fas fa-link"></i></a><a href="#股票如何卖在最高" class="headerlink" title="股票如何卖在最高"></a>股票如何卖在最高</h1>
      
        <h2 id="减仓的必要性">
          <a href="#减仓的必要性" class="heading-link"><i class="fas fa-link"></i></a><a href="#减仓的必要性" class="headerlink" title="减仓的必要性"></a>减仓的必要性</h2>
      <p>涨停为何卖票？利润垫是持股的唯一信仰，非顶战法，是一种心态管理的手段，永远立于不败之地。</p>

        <h2 id="几种卖出逻辑">
          <a href="#几种卖出逻辑" class="heading-link"><i class="fas fa-link"></i></a><a href="#几种卖出逻辑" class="headerlink" title="几种卖出逻辑"></a>几种卖出逻辑</h2>
      
        <h3 id="非绝对龙头，涨停板减仓">
          <a href="#非绝对龙头，涨停板减仓" class="heading-link"><i class="fas fa-link"></i></a><a href="#非绝对龙头，涨停板减仓" class="headerlink" title="非绝对龙头，涨停板减仓"></a>非绝对龙头，涨停板减仓</h3>
      <p>市场环境弱势的时候一定要减仓做出利润垫，防范未发生的风险；</p>
<p>非市场绝对龙头，次日涨停减仓做厚利润垫；</p>

        <h3 id="分时背离-x2F-乖离-x2F-反抽减仓">
          <a href="#分时背离-x2F-乖离-x2F-反抽减仓" class="heading-link"><i class="fas fa-link"></i></a><a href="#分时背离-x2F-乖离-x2F-反抽减仓" class="headerlink" title="分时背离&#x2F;乖离&#x2F;反抽减仓"></a>分时背离&#x2F;乖离&#x2F;反抽减仓</h3>
      <ul>
<li>分时背离（量价背离）：法本信息，巨轮智能；</li>
</ul>
<img src="/2025/07/28/%E9%A1%B6%E7%BA%A7%E5%B9%B2%E8%B4%A7/%E5%88%86%E6%97%B6%E8%83%8C%E7%A6%BB2.png" class title="分时背离2">

<ul>
<li>分时乖离：常山北明，长盛轴承；</li>
</ul>
<img src="/2025/07/28/%E9%A1%B6%E7%BA%A7%E5%B9%B2%E8%B4%A7/%E5%88%86%E6%97%B6%E4%B9%96%E7%A6%BB.png" class title="分时乖离">

<ul>
<li><p>分时反抽：</p>
<img src="/2025/07/28/%E9%A1%B6%E7%BA%A7%E5%B9%B2%E8%B4%A7/%E7%A6%BB%E5%88%AB%E9%92%A9.png" class title="离别钩"></li>
</ul>

        <h3 id="789减仓法">
          <a href="#789减仓法" class="heading-link"><i class="fas fa-link"></i></a><a href="#789减仓法" class="headerlink" title="789减仓法"></a>789减仓法</h3>
      <p>为什么要789减仓，可能是日内高点，不差这两个点，做厚利润垫，保护自己心态。</p>

        <h3 id="压力位减仓">
          <a href="#压力位减仓" class="heading-link"><i class="fas fa-link"></i></a><a href="#压力位减仓" class="headerlink" title="压力位减仓"></a>压力位减仓</h3>
      <ul>
<li>近期爆量高点，吉宏股份；</li>
<li>前高压力，常山北明；</li>
<li>历史高点：赛里斯周K；</li>
</ul>

        <h3 id="缩量板减仓">
          <a href="#缩量板减仓" class="heading-link"><i class="fas fa-link"></i></a><a href="#缩量板减仓" class="headerlink" title="缩量板减仓"></a>缩量板减仓</h3>
      <ul>
<li>有开板预期，直接减，如果不回封？</li>
</ul>
<img src="/2025/07/28/%E9%A1%B6%E7%BA%A7%E5%B9%B2%E8%B4%A7/%E7%BC%A9%E9%87%8F%E6%9D%BF%E5%87%8F%E4%BB%93.png" class title="缩量板减仓">




        <h1 id="情绪运行规律">
          <a href="#情绪运行规律" class="heading-link"><i class="fas fa-link"></i></a><a href="#情绪运行规律" class="headerlink" title="情绪运行规律"></a>情绪运行规律</h1>
      
        <h2 id="情绪">
          <a href="#情绪" class="heading-link"><i class="fas fa-link"></i></a><a href="#情绪" class="headerlink" title="情绪"></a>情绪</h2>
      <p><strong>情绪特征</strong></p>
<ul>
<li>客观存在<ul>
<li>短线情绪，虽然看不见摸不着，但在A股市场是真实存在的，情绪好的时候，各种模式出手成功率高、容错率高。各路股神都会冒出来。</li>
</ul>
</li>
<li>难以刻画<ul>
<li>情绪很多时候是难以刻画的，因为不讲道理。讲道理的就不是情绪了，是理性。</li>
</ul>
</li>
<li>快速变化<ul>
<li>情绪并非一成不变的，有时甚至一日多变，这就给情绪的的判定，增加了更多不确定性。</li>
</ul>
</li>
</ul>
<p>在A股市场，当前还是一个散户居多的市场生态，甚至很多机构也是散户思维。追涨杀跌、跟风抱团、一窝蜂这样的现象尤为严重，所以“短线情绪”就会格外重要。</p>
<p><strong>核心干货：</strong> 情绪风标定每日市场情绪</p>
<p>竞价情绪，根据多空组合，可分成：</p>
<ul>
<li>强平衡：关键个股的走势以及指数所决定胜负手，转势</li>
<li>弱平衡：指数带动情绪</li>
<li>多方胜：开盘杀</li>
<li>空方胜：开盘修复</li>
</ul>
<p>D：多头风标，D在涨停价之上表示一字涨停</p>
<p>K：空头风标，K在跌停价之下表示一字跌停</p>
<img src="/2025/07/28/%E9%A1%B6%E7%BA%A7%E5%B9%B2%E8%B4%A7/%E6%83%85%E7%BB%AA%E9%A3%8E%E6%A0%87.png" class title="情绪风标">

<p>有时会失效，跟实际情况对不上。。。</p>
<p>更高阶：题材运行阶段，情绪运行节奏和节点。</p>
<p><strong>情绪风标的选择</strong></p>
<ul>
<li>D头：市场最高空间板，市场最大封单，人气妖股、各题材分支老大，地天板；</li>
<li>K头：最高断板，前一日跌停票，天地板，最大跌停封单的票；</li>
</ul>
<p>高阶：指数与情绪的组合、情绪节点、情绪与题材运行阶段配合；</p>

        <h1 id="板块选股法，上车最强前排">
          <a href="#板块选股法，上车最强前排" class="heading-link"><i class="fas fa-link"></i></a><a href="#板块选股法，上车最强前排" class="headerlink" title="板块选股法，上车最强前排"></a>板块选股法，上车最强前排</h1>
      <p><strong>操作</strong></p>
<p>先选中<strong>94板块</strong>，竞价<strong>涨幅</strong>靠前的，开盘拉升最快的板块通过<strong>竞价金额</strong>、<strong>竞价涨幅</strong>去挑选合适的个股，配合开盘拉升开盘杀的放弃。</p>
<p><strong>操盘12字诀：盘前预案，竞价信号，开盘跟随</strong> —&gt; 大局观</p>
<p><strong>应用场景</strong></p>
<ul>
<li>题材涨幅靠前：题材无方向的时候，题材的涨幅靠前，关注他开盘是否走强承接资金（结合消息刺激，结合对流预案，通过大局观保证它不会冲高回落，那就可以半路上车，可以通过板块选股法上车前排）</li>
<li>开盘涨速靠前：题材无方向，题材涨幅也没有亮眼的，看涨速靠前的方向；</li>
<li>主流兑现，资金切对流题材：主流题材有方向，开盘兑现，资金切换对流盘，对流题材拉升（涨速）—–94板块三一票</li>
</ul>
<p><strong>注意事项：</strong> 一定要通过大局观保证，它的拉升有持续性。</p>

        <h1 id="地天板的核心秘密">
          <a href="#地天板的核心秘密" class="heading-link"><i class="fas fa-link"></i></a><a href="#地天板的核心秘密" class="headerlink" title="地天板的核心秘密"></a>地天板的核心秘密</h1>
      <ul>
<li><p>人气龙头短期超跌&#x2F;短期超跌，连续一字后跌停，股票的空间感（双成药业）</p>
<ul>
<li>短期超跌</li>
<li>人气度高，市场资金关注</li>
<li>筹码断层，往上没有抛压</li>
<li>最好是竞价反核，两市三一（市场资金都能关注到）</li>
</ul>
<img src="/2025/07/28/%E9%A1%B6%E7%BA%A7%E5%B9%B2%E8%B4%A7/%E7%9F%AD%E6%9C%9F%E8%B6%85%E8%B7%8C.png" class title="短期超跌">
</li>
<li><p>绝对龙头，题材、情绪助攻（日出东方）</p>
<ul>
<li>东方系玄学炒作，题材</li>
<li>东方精工，不会打板，继续打高度</li>
<li>情绪特别好</li>
</ul>
</li>
</ul>
<img src="/2025/07/28/%E9%A1%B6%E7%BA%A7%E5%B9%B2%E8%B4%A7/%E6%97%A5%E5%87%BA%E4%B8%9C%E6%96%B9.png" class title="日出东方">

<ul>
<li><p>龙头继续打高度，龙二&#x2F;核心补涨被核按（大东方）</p>
<ul>
<li>补涨，按名字概念</li>
</ul>
<img src="/2025/07/28/%E9%A1%B6%E7%BA%A7%E5%B9%B2%E8%B4%A7/%E5%A4%A7%E4%B8%9C%E6%96%B9.png" class title="大东方"></li>
</ul>

        <h1 id="三一模式">
          <a href="#三一模式" class="heading-link"><i class="fas fa-link"></i></a><a href="#三一模式" class="headerlink" title="三一模式"></a>三一模式</h1>
      <p>何为三一，即第一、唯一、专一；</p>
<p>竞价三一不是指标的模式，不是有了指标就能无脑干，<strong>需要大局观，题材运行阶段、情绪节点都需要考虑进去</strong>。</p>
<ul>
<li>第一：比较范围<ul>
<li>两市第一：主板第一、创业板第一、科创板第一</li>
<li>题材第一、板块第一</li>
<li>梯队第一，如首板第一，二板第一</li>
<li>自建票池第一，炸板票池的第一，断板票池的第一</li>
</ul>
</li>
<li>第一：比较指标<ul>
<li>竞价金额</li>
<li>竞价换手</li>
<li>竞价涨幅</li>
</ul>
</li>
<li>唯一：票的地位<ul>
<li>是市场前期炒过之后，后面绕不开的标的</li>
<li>龙头，趋势容量</li>
</ul>
</li>
<li>专一：持仓核仓位管理<ul>
<li>小资金要控制标的的数量一般不要超过三支票</li>
<li>小资金一般指100万-300以下的量</li>
<li>没掌握好这个模式之前要小仓位验证，适合你的情况下再加大仓位，要做好持仓管理</li>
</ul>
</li>
</ul>

        <h2 id="如何实操寻找三一票">
          <a href="#如何实操寻找三一票" class="heading-link"><i class="fas fa-link"></i></a><a href="#如何实操寻找三一票" class="headerlink" title="如何实操寻找三一票"></a>如何实操寻找三一票</h2>
      <p><strong>竞价阶段</strong>，各板块按竞价涨幅、竞价金额、主力净额排序；</p>
<p>两市三一则按竞价金额、竞价换手排序；</p>
<p>合锻智能：两市三一，第一天不去，第二天去（爆量后的修正）</p>
<p>真视通：两市三一，补涨逻辑，短线环境好</p>
<p>双城药业：竞价反核的三一票，绿三一</p>
<img src="/2025/07/28/%E9%A1%B6%E7%BA%A7%E5%B9%B2%E8%B4%A7/%E5%8F%8C%E6%88%90%E8%8D%AF%E4%B8%9A.png" class title="双成药业">

<p>海能达：断板三一，指数、情绪爆棚，市场量能充沛</p>
<p>大众交通：环境弱势，第一个走出持续性的题材，里面表态最明显的票</p>
<p><strong>适用环境：</strong></p>
<ul>
<li>指数情绪配合，题材方向明确，个股PK还要胜出</li>
<li>不是强，是分歧大的表现</li>
<li>行情好，最暴利的挣钱战法</li>
<li>环境不好，慢一步，空仓，不干</li>
</ul>

        <h1 id="如何做T">
          <a href="#如何做T" class="heading-link"><i class="fas fa-link"></i></a><a href="#如何做T" class="headerlink" title="如何做T"></a>如何做T</h1>
      <p><strong>做T五要素</strong></p>
<ul>
<li><p>市场环境</p>
<p>大局观一直是我们强调的重点，做股票没有大局观犹如出门不看天气预报。</p>
<ul>
<li><p>题材上升期：低吸买入</p>
<p>若指数强势，题材也是日内主流，题材属于<strong>分化有回流预期</strong>，这时候水下缩量的个股就是不错的低吸机会，上冲压力位或量价背离T出。</p>
</li>
<li><p>题材退潮期：逃顶卖出</p>
<p>若指数强势，题材退潮，个股高开低走，低开低走，开盘就要卖，跟随做倒T或者直接出局。</p>
</li>
</ul>
</li>
<li><p>支撑压力</p>
<ul>
<li>个股的支撑压力位是股票运行中很关键的价位；</li>
<li>日K、5分K的大量点，爆量点，是关键的支撑位；</li>
<li>多日分时看支撑压力</li>
</ul>
<img src="/2025/07/28/%E9%A1%B6%E7%BA%A7%E5%B9%B2%E8%B4%A7/%E5%A4%9A%E6%97%A5%E5%88%86%E6%97%B6%E7%9C%8B%E6%94%AF%E6%92%91%E5%8E%8B%E5%8A%9B.png" class title="多日分时看支撑压力">

<ul>
<li>5分K看支撑压力</li>
</ul>
<img src="/2025/07/28/%E9%A1%B6%E7%BA%A7%E5%B9%B2%E8%B4%A7/5%E5%88%86K%E7%9C%8B%E6%94%AF%E6%92%91%E7%82%B9%E4%BD%8D.png" class title="5分K看支撑点位">

<ul>
<li>突破压力</li>
</ul>
<img src="/2025/07/28/%E9%A1%B6%E7%BA%A7%E5%B9%B2%E8%B4%A7/%E7%AA%81%E7%A0%B4%E5%8E%8B%E5%8A%9B%E4%BD%8D.png" class title="突破压力位">

<ul>
<li>一破颈线成空方，破位就要第一时间出局</li>
</ul>
<img src="/2025/07/28/%E9%A1%B6%E7%BA%A7%E5%B9%B2%E8%B4%A7/%E4%B8%80%E7%A0%B4%E9%A2%88%E7%BA%BF%E6%88%90%E7%A9%BA%E6%96%B9.png" class title="一破颈线成空方">
</li>
<li><p>盘口</p>
</li>
<li><p>分时量价</p>
<ul>
<li>顶背离</li>
</ul>
<p>顶背离是股价上涨但是第二次股价高点的量能没上去，突破关键价位时是需要量能配合的，若量能上不去是很难突破的，所以量背离的二次股价高点一般都是日内高点，是卖出机会，这是大多数机会。</p>
<p>少数情况下股价背离之后还能二次背离，这个时候<strong>一定要减仓</strong>，这是给你改错的机会。</p>
<p>还有一种极少数情况，二次背离之后股价还能涨，这种时候卖盘是惜售的，之所以未涨停只是因为没有人点火拉升，只要买盘充裕这种票也能背离上板，这个需要对盘面整体把控能力。</p>
<img src="/2025/07/28/%E9%A1%B6%E7%BA%A7%E5%B9%B2%E8%B4%A7/%E9%A1%B6%E8%83%8C%E7%A6%BB1.png" class title="顶背离1">

<ul>
<li>底背离</li>
</ul>
<p>底背离是股价下跌过程中越跌量能越大，说明越跌越有人买，承接很强。你可能会辩证的看，越跌越有人卖大家都想便宜出局难道不是一种弱的表现吗？</p>
<p>股价下跌过程中的量能是越跌量越小的，是完全没有人承接，如果越跌量能越大，说明这里是有资金承接的，关键价位有支撑，是一种强的表现。</p>
<p>关键价位的支撑不一定马上止跌，盘中可能跌破一下后收回，然后还有一个缩量回踩的过程，缩量回踩如果不跌破，基本就是非常确定的入场机会。</p>
</li>
</ul>
<img src="/2025/07/28/%E9%A1%B6%E7%BA%A7%E5%B9%B2%E8%B4%A7/%E5%BA%95%E8%83%8C%E7%A6%BB.png" class title="底背离">

<ul>
<li>仓位管理<ul>
<li>做T必须<strong>等仓位</strong>做T才有效果，如果昨天买了100手，今天买入10手去做T，就没有意义；</li>
<li>一般是等仓位的做T会比较好一点，或者最起码最少是半仓去做，比如昨天买了100手，今天去做T，最少是50手；</li>
<li>也不能把仓位增加到200手去做T，除非是特别确定性的机会。</li>
</ul>
</li>
</ul>

        <h1 id="缩量上涨的本质，第一时间接龙头">
          <a href="#缩量上涨的本质，第一时间接龙头" class="heading-link"><i class="fas fa-link"></i></a><a href="#缩量上涨的本质，第一时间接龙头" class="headerlink" title="缩量上涨的本质，第一时间接龙头"></a>缩量上涨的本质，第一时间接龙头</h1>
      <p><strong>爆量的本质</strong></p>
<ul>
<li>筹码的充分交换，想进来的人都进来了，想走的人都走了</li>
<li>主力成本重置，爆量附近就是主力成本附近</li>
</ul>
<p>解决了3个问题，爆量的票后续是涨是跌？爆量的票这么高了还能不能买？为什么爆量不买</p>
<p><strong>战法：</strong> 爆量转一致（弱转强）</p>
<p><strong>缩量上涨的本质</strong></p>
<ul>
<li><p>筹码稳定，资金无论多空，都是一致看好</p>
</li>
<li><p>短期浮筹很大，很快脱离了主力的成本区，容易被砸</p>
</li>
<li><p>为什么缩量不买？岩山科技（市场环境，第一买点），缩量票一旦开板，就容易爆量，爆量 不买，你无法确认当天是主力吃货还是小散接盘</p>
</li>
</ul>
<img src="/2025/07/28/%E9%A1%B6%E7%BA%A7%E5%B9%B2%E8%B4%A7/%E7%BC%A9%E9%87%8F%E5%90%8E%E7%88%86%E9%87%8F%E4%B8%8D%E4%B9%B0.png" class title="缩量后爆量不买">

<ul>
<li>什么票可以缩量买，缩量票你一面就吃面，别人买就是大肉（地位，买点，模式），龙头缩量可以买，市场环境（题材继续走，情绪上升期）</li>
</ul>

        <h1 id="题材各阶段运行实操">
          <a href="#题材各阶段运行实操" class="heading-link"><i class="fas fa-link"></i></a><a href="#题材各阶段运行实操" class="headerlink" title="题材各阶段运行实操"></a>题材各阶段运行实操</h1>
      
        <h1 id="市场三只核心股">
          <a href="#市场三只核心股" class="heading-link"><i class="fas fa-link"></i></a><a href="#市场三只核心股" class="headerlink" title="市场三只核心股"></a>市场三只核心股</h1>
      <p><strong>1. 龙头股、核心股、中军（容量票）</strong></p>
<ul>
<li>龙头股：一字龙头、天天一字板，一波到顶</li>
<li>换手龙：天天正常换手，人人都有参与的机会</li>
<li>日内龙头：日内板块最先涨停，最有带动性的那只</li>
<li>高度龙头：1-2板，最高板</li>
<li>趋势龙头：走趋势，人气度最高的，不涨停，涨不停</li>
</ul>
<p>核心股：沪深整个市场上，人气极高的，关注度很高的的核心标的，市场地位都很高</p>
<p><strong>龙头股跟核心股的区别</strong></p>
<ul>
<li>龙头有大龙头、小龙头，大龙头是核心，小龙头不一定是核心股</li>
<li>大题材，由于涨停个股极多，持续性非常好，板块的龙二、龙三都能成为核心股</li>
<li>独立行情的个股，也有可能是核心股，比如中毅达</li>
<li>中军：板块内人气较高，流通盘较大或成交量较大（15-35亿，100亿），看整体市场环境，是大资金对轰的战场，容易受大盘指数的强弱影响，更容易收到接力气氛强弱的影响。指数弱势、接力气氛不行的时候，不能追中军</li>
<li>买中军的最佳时刻：龙二龙三一字涨停</li>
</ul>
<p><strong>2. 个股的顺位</strong></p>
<p>市场上强势股强弱的顺位排名是：</p>
<ul>
<li>人气妖股</li>
<li>人气龙头</li>
<li>人气中军（容量票）</li>
<li>核心股（前三个也是核心股，大题材的龙二也是核心股）</li>
<li>人气趋势股</li>
<li>前排跟风（龙二龙三龙四）<ul>
<li>有板块效应的强于走独立的</li>
<li>股性好的要强于股性差的</li>
<li>第一买点强于第二买点，强于第三买点（模式内买点）</li>
<li>走势稳定健康换手的，优于缩量跳空乱涨的，一字龙头定高度，伴生换手，走到最后的是换手龙，主力成本不断被拉高</li>
<li>新题材概念强于老弱的题材</li>
<li>一天之中先涨停的，强于后涨停的，一天之中最后炸板的，强于先炸板的（被动开板）</li>
</ul>
</li>
</ul>
<p>机构票：防守性能很好，上涨的动力不足</p>
<p>游资票：进攻能力很好，但是防守能力很差，通过仓位管理去防守</p>

        <h1 id="便宜老大战法">
          <a href="#便宜老大战法" class="heading-link"><i class="fas fa-link"></i></a><a href="#便宜老大战法" class="headerlink" title="便宜老大战法"></a>便宜老大战法</h1>
      <p><strong>便宜老大的逻辑：</strong> 就是在题材集体走强的时候，具有身位优势的票，它容易被反推上板；</p>
<p>借势，本质是被动上板，属于套利（只有老大继续连扳才可以拿）；</p>
<p><strong>身为板的2种玩法</strong></p>
<ul>
<li>便宜老大，被题材反推，被动上板</li>
<li>主动封板，引导资金进攻题材方向</li>
</ul>
<p><strong>第一买点与模式失效的情况</strong></p>
<p>第一买点的概念：就是第一次出现模式内的B点</p>
<p>便宜老大它只能使用一次；</p>
<p>环境，指数差，题材差，做票失败率变高</p>

        <h1 id="三种主流战法">
          <a href="#三种主流战法" class="heading-link"><i class="fas fa-link"></i></a><a href="#三种主流战法" class="headerlink" title="三种主流战法"></a>三种主流战法</h1>
      <p><strong>1. 跟主力战法</strong></p>
<p>主力净额大幅流入的就是主力，主力净额大幅流入的股票只能说明它的防守性很好，不容易被砸成大面，但不代表它上涨的确定性很高。</p>
<p>比如：</p>
<p>某个票20亿净买入，明天一定会涨吗？—&gt;不一定，多头没有规划好进攻的节奏，一天全干进去了，后面由谁来拉呢？</p>
<p><strong>上涨确定性：</strong> 市场的地位、是否热点题材，股性好不好，是不是核心股、龙头股、板块前排，也就是顺位高低。</p>
<p>占大资金便宜，最暴利的方法论。</p>
<p>日成交几十亿上百亿的个股，包括两市成交量前50大票，很容易受大盘指数强弱影响，要学会看大盘，看指数，大盘指数，就能提高命中率。</p>
<p>大单金额占日成交量20%以上的，涨停概率极高，都是强主力介入标的。</p>
<p>大单净额&#x2F;流通盘&#x3D;大单净量</p>
<p>大单净流入&#x3D;大单净买入-大单净卖出，同花顺对大单的统计口径是30万。</p>

        <h1 id="何为前后排">
          <a href="#何为前后排" class="heading-link"><i class="fas fa-link"></i></a><a href="#何为前后排" class="headerlink" title="何为前后排"></a>何为前后排</h1>
      
        <h1 id="量价关系">
          <a href="#量价关系" class="heading-link"><i class="fas fa-link"></i></a><a href="#量价关系" class="headerlink" title="量价关系"></a>量价关系</h1>
      <p><strong>缩量上涨还会上涨</strong></p>
<img src="/2025/07/28/%E9%A1%B6%E7%BA%A7%E5%B9%B2%E8%B4%A7/image-20250810181606379.png" class title="image-20250810181606379">

<p><strong>缩量下跌还会下跌</strong></p>
<img src="/2025/07/28/%E9%A1%B6%E7%BA%A7%E5%B9%B2%E8%B4%A7/image-20250810181640510.png" class title="image-20250810181640510">

<p><strong>高位放巨量上涨必会下跌（短期震荡）</strong></p>
<img src="/2025/07/28/%E9%A1%B6%E7%BA%A7%E5%B9%B2%E8%B4%A7/image-20250810181747927.png" class title="image-20250810181747927">

<p><strong>低位放巨量上涨必会回调</strong></p>
<img src="/2025/07/28/%E9%A1%B6%E7%BA%A7%E5%B9%B2%E8%B4%A7/image-20250810182107554.png" class title="image-20250810182107554">

<p><strong>低位放巨量下跌必会反弹</strong></p>
<img src="/2025/07/28/%E9%A1%B6%E7%BA%A7%E5%B9%B2%E8%B4%A7/%E8%81%94%E6%83%B3%E6%88%AA%E5%9B%BE_20250810182248.png" class title="img">

<p><strong>放量滞涨，顶部信号</strong></p>
<img src="/2025/07/28/%E9%A1%B6%E7%BA%A7%E5%B9%B2%E8%B4%A7/image-20250810182516173.png" class title="image-20250810182516173">

<p><strong>缩量不跌，底部信号</strong></p>
<img src="/2025/07/28/%E9%A1%B6%E7%BA%A7%E5%B9%B2%E8%B4%A7/image-20250810182614112.png" class title="image-20250810182614112">

<p><strong>量大成头，量小成底</strong></p>
<img src="/2025/07/28/%E9%A1%B6%E7%BA%A7%E5%B9%B2%E8%B4%A7/image-20250810182736511.png" class title="image-20250810182736511">

<p><strong>顶部无量下跌，后市大概率创新高</strong></p>
<img src="/2025/07/28/%E9%A1%B6%E7%BA%A7%E5%B9%B2%E8%B4%A7/image-20250810182813805.png" class title="image-20250810182813805">

<p><strong>顶部放量下跌，后市难创新高</strong></p>
<img src="/2025/07/28/%E9%A1%B6%E7%BA%A7%E5%B9%B2%E8%B4%A7/image-20250810182830367.png" class title="image-20250810182830367">




        <h1 id="复盘">
          <a href="#复盘" class="heading-link"><i class="fas fa-link"></i></a><a href="#复盘" class="headerlink" title="复盘"></a>复盘</h1>
      
        <h1 id="短线OR趋势">
          <a href="#短线OR趋势" class="heading-link"><i class="fas fa-link"></i></a><a href="#短线OR趋势" class="headerlink" title="短线OR趋势"></a>短线OR趋势</h1>
      
        <h3 id="短线">
          <a href="#短线" class="heading-link"><i class="fas fa-link"></i></a><a href="#短线" class="headerlink" title="短线"></a>短线</h3>
      
        <h3 id="趋势">
          <a href="#趋势" class="heading-link"><i class="fas fa-link"></i></a><a href="#趋势" class="headerlink" title="趋势"></a>趋势</h3>
      
        <h1 id="名词解释">
          <a href="#名词解释" class="heading-link"><i class="fas fa-link"></i></a><a href="#名词解释" class="headerlink" title="名词解释"></a>名词解释</h1>
      <p><strong>九转</strong></p>
<ul>
<li>上涨九转（买入信号）：连续九天收盘价均高于4天前的收盘价，且第九天收盘价位于5日均线之上；</li>
<li>下跌九转（卖出信号）：连续九天收盘价均低于4天前的收盘价，且第九天收盘价位于5日均线之下；</li>
</ul>
<p><strong>竞价反核</strong></p>
<p><strong>定义：</strong> 是A股短线交易中的一种逆向博弈策略，指在集合竞价阶段（9:15~9:25）针对被恐慌性抛售至跌停或深水区的个股进行反向买入操作，核心逻辑是利用市场情绪极端悲观后的修复动能获取日内超额收益。</p>
<p><strong>标的</strong></p>
<p>标的指金融交易中双方权利和义务共同指向的具体对象，是投资决策和交易行为的核心载体。即投资者买卖的具体对象，如股票、债券、基金等证券品种，代表对特定资产的所有权或收益权。</p>
</div></div></article></section><nav class="paginator"><div class="paginator-inner"><span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/7/">7</a><a class="extend next" rel="next" href="/page/2/"><i class="fas fa-angle-right"></i></a></div></nav></div></div><div class="sidebar-wrap" id="sidebar-wrap"><aside class="sidebar" id="sidebar"><section class="sidebar-toc hide"></section><!-- ov = overview--><section class="sidebar-ov"><div class="sidebar-ov-author"><div class="sidebar-ov-author__avatar"><img class="sidebar-ov-author__avatar_img" src="/images/icons/2.jpg" alt="avatar"></div><p class="sidebar-ov-author__text">今天要比昨天进步一点点</p></div><div class="sidebar-ov-social"><a class="sidebar-ov-social-item" href="https://github.com/IAMLLT796" target="_blank" rel="noopener" data-popover="Github" data-popover-pos="up"><span class="sidebar-ov-social-item__icon"><i class="fab fa-github"></i></span></a><a class="sidebar-ov-social-item" href="lt.liu796@gmail.com" target="_blank" rel="noopener" data-popover="Gmail" data-popover-pos="up"><span class="sidebar-ov-social-item__icon">邮</span></a><a class="sidebar-ov-social-item" href="https://space.bilibili.com/274733853?spm_id_from=333.1007.0.0" target="_blank" rel="noopener" data-popover="bilibili" data-popover-pos="up"><span class="sidebar-ov-social-item__icon">Bili</span></a><a class="sidebar-ov-social-item" href="https://www.zhihu.com/people/kui-hua-cheng-hai-xiang-yang-er-kai-76" target="_blank" rel="noopener" data-popover="知乎" data-popover-pos="up"><span class="sidebar-ov-social-item__icon">知</span></a></div><div class="sidebar-ov-state"><a class="sidebar-ov-state-item sidebar-ov-state-item--posts" href="/archives/"><div class="sidebar-ov-state-item__count">65</div><div class="sidebar-ov-state-item__name">归档</div></a><a class="sidebar-ov-state-item sidebar-ov-state-item--categories" href="/categories/"><div class="sidebar-ov-state-item__count">14</div><div class="sidebar-ov-state-item__name">分类</div></a><a class="sidebar-ov-state-item sidebar-ov-state-item--tags" href="/tags/"><div class="sidebar-ov-state-item__count">71</div><div class="sidebar-ov-state-item__name">标签</div></a></div><div class="sidebar-ov-cc"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" target="_blank" rel="noopener" data-popover="知识共享许可协议" data-popover-pos="up"><img src="/images/cc-by-nc-sa.svg"></a></div></section></aside></div><div class="clearfix"></div></div></main><footer class="footer" id="footer"><div class="footer-inner"><div><span>冀ICP备19023374</span></div><div><span>由 <a href="http://hexo.io/" title="Hexo" target="_blank" rel="noopener">Hexo</a> 大力支持</span><span> v6.3.0</span><span class="footer__devider">|</span><span>Theme - <a href="https://github.com/liuyib/hexo-theme-stun/" title="Stun" target="_blank" rel="noopener">Stun</a></span><span> v2.8.0</span></div><div class="busuanzi"><span class="busuanzi-siteuv"><span class="busuanzi-siteuv__icon"><i class="fas fa-user"></i></span><span class="busuanzi-siteuv__info">Visitors</span><span class="busuanzi-siteuv__value" id="busuanzi_value_site_uv"></span></span><span class="busuanzi-sitepv"><span class="busuanzi-siteuv__icon"><i class="fas fa-eye"></i></span><span class="busuanzi-siteuv__info">Views</span><span class="busuanzi-siteuv__value" id="busuanzi_value_site_pv"></span></span></div><div>葵海-做一个有趣的人</div></div></footer><div class="loading-bar" id="loading-bar"><div class="loading-bar__progress"></div></div><div class="back2top" id="back2top"><span class="back2top__icon"><i class="fas fa-rocket"></i></span></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@v3.4.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.ui.min.js"></script><script src="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax@latest/pjax.min.js"></script><script>window.addEventListener('DOMContentLoaded', function () {
  var pjax = new Pjax({"selectors":["head title","#main",".pjax-reload",".header-banner"],"history":true,"scrollTo":false,"scrollRestoration":false,"cacheBust":false,"debug":false,"currentUrlFullReload":false,"timeout":0});
  // 加载进度条的计时器
  var loadingTimer = null;

  // 重置页面 Y 方向上的滚动偏移量
  document.addEventListener('pjax:send', function () {
    $('.header-nav-menu').removeClass('show');
    if (CONFIG.pjax && CONFIG.pjax.avoidBanner) {
      $('html').velocity('scroll', {
        duration: 500,
        offset: $('#header').height(),
        easing: 'easeInOutCubic'
      });
    }

    var loadingBarWidth = 20;
    var MAX_LOADING_WIDTH = 95;

    $('.loading-bar').addClass('loading');
    $('.loading-bar__progress').css('width', loadingBarWidth + '%');
    clearInterval(loadingTimer);
    loadingTimer = setInterval(function () {
      loadingBarWidth += 3;
      if (loadingBarWidth > MAX_LOADING_WIDTH) {
        loadingBarWidth = MAX_LOADING_WIDTH;
      }
      $('.loading-bar__progress').css('width', loadingBarWidth + '%');
    }, 500);
  }, false);

  window.addEventListener('pjax:complete', function () {
    clearInterval(loadingTimer);
    $('.loading-bar__progress').css('width', '100%');
    $('.loading-bar').removeClass('loading');
    setTimeout(function () {
      $('.loading-bar__progress').css('width', '0');
    }, 400);
    $('link[rel=prefetch], script[data-pjax-rm]').each(function () {
      $(this).remove();
    });
    $('script[data-pjax], #pjax-reload script').each(function () {
      $(this).parent().append($(this).remove());
    });

    if (Stun.utils.pjaxReloadBoot) {
      Stun.utils.pjaxReloadBoot();
    }
    if (Stun.utils.pjaxReloadScroll) {
      Stun.utils.pjaxReloadScroll();
    }
    if (Stun.utils.pjaxReloadSidebar) {
      Stun.utils.pjaxReloadSidebar();
    }
    if (false) {
      if (Stun.utils.pjaxReloadHeader) {
        Stun.utils.pjaxReloadHeader();
      }
      if (Stun.utils.pjaxReloadScrollIcon) {
        Stun.utils.pjaxReloadScrollIcon();
      }
      if (Stun.utils.pjaxReloadLocalSearch) {
        Stun.utils.pjaxReloadLocalSearch();
      }
    }
  }, false);
}, false);</script><div id="pjax-reload"><script src="https://cdn.jsdelivr.net/npm/quicklink@1.0.1/dist/quicklink.umd.js"></script><script>function initQuicklink() {
  quicklink({
    timeout: '10000',
    priority: true,
    ignores: [uri => uri.includes('#'), uri => uri === 'https://iamllt796.github.io/', /\/api\/?/,uri => uri.includes('.xml'),uri => uri.includes('.zip'),(uri, el) => el.hasAttribute('nofollow'),(uri, el) => el.hasAttribute('noprefetch')]
  });
}

if (true || false) {
  initQuicklink();
} else {
  window.addEventListener('DOMContentLoaded', initQuicklink, false);
}</script><script src="https://cdn.jsdelivr.net/gh/sukkaw/busuanzi@latest/bsz.pure.mini.js" async></script></div><script src="/js/utils.js?v=2.8.0"></script><script src="/js/stun-boot.js?v=2.8.0"></script><script src="/js/scroll.js?v=2.8.0"></script><script src="/js/header.js?v=2.8.0"></script><script src="/js/sidebar.js?v=2.8.0"></script></body></html>