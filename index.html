<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><link rel="icon" href="/images/icons/3.jpg?v=2.8.0" type="image/png" sizes="16x16"><link rel="icon" href="/images/icons/4.jpg?v=2.8.0" type="image/png" sizes="32x32"><meta property="og:type" content="website">
<meta property="og:title" content="葵海">
<meta property="og:url" content="https://iamllt796.github.io/index.html">
<meta property="og:site_name" content="葵海">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="LLT796">
<meta name="twitter:card" content="summary"><title>葵海</title><link ref="canonical" href="https://iamllt796.github.io/index.html"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.12.1/css/all.min.css" type="text/css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" type="text/css"><link rel="stylesheet" href="/css/index.css?v=2.8.0"><link rel="stylesheet" href="css/custom.css"><script>var Stun = window.Stun || {};
var CONFIG = {
  root: '/',
  algolia: undefined,
  assistSearch: undefined,
  fontIcon: {"prompt":{"success":"fas fa-check-circle","info":"fas fa-arrow-circle-right","warning":"fas fa-exclamation-circle","error":"fas fa-times-circle"},"copyBtn":"fas fa-copy"},
  sidebar: {"offsetTop":"20px","tocMaxDepth":6},
  header: {"enable":true,"showOnPost":true,"scrollDownIcon":false},
  postWidget: {"endText":true},
  nightMode: {"enable":true},
  back2top: {"enable":true},
  codeblock: {"style":"default","highlight":"light","wordWrap":true},
  reward: false,
  fancybox: true,
  zoomImage: {"gapAside":"20px"},
  galleryWaterfall: undefined,
  lazyload: false,
  pjax: {"avoidBanner":false},
  externalLink: {"icon":{"enable":true,"name":"fas fa-external-link-alt"}},
  shortcuts: undefined,
  prompt: {"copyButton":"复制","copySuccess":"复制成功","copyError":"复制失败"},
  sourcePath: {"js":"js","css":"css","images":"images"},
};

window.CONFIG = CONFIG;</script><meta name="generator" content="Hexo 6.3.0"></head><body><div class="container" id="container"><header class="header" id="header"><div class="header-inner"><nav class="header-nav header-nav--fixed"><div class="header-nav-inner"><div class="header-nav-menubtn"><i class="fas fa-bars"></i></div><div class="header-nav-menu"><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/"><span class="header-nav-menu-item__icon"><i class="fas fa-home"></i></span><span class="header-nav-menu-item__text">首页</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/archives/"><span class="header-nav-menu-item__icon"><i class="fas fa-folder-open"></i></span><span class="header-nav-menu-item__text">归档</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/categories/"><span class="header-nav-menu-item__icon"><i class="fas fa-layer-group"></i></span><span class="header-nav-menu-item__text">分类</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/tags/"><span class="header-nav-menu-item__icon"><i class="fas fa-tags"></i></span><span class="header-nav-menu-item__text">标签</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/about/"><span class="header-nav-menu-item__icon"><i class="fas fa-user"></i></span><span class="header-nav-menu-item__text">博主</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/read/"><span class="header-nav-menu-item__icon"><i class="fas fa-book"></i></span><span class="header-nav-menu-item__text">阅读</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/timeline/"><span class="header-nav-menu-item__icon"><i class="fas fa-history"></i></span><span class="header-nav-menu-item__text">menu.timeline</span></a></div></div><div class="header-nav-mode"><div class="mode"><div class="mode-track"><span class="mode-track-moon"></span><span class="mode-track-sun"></span></div><div class="mode-thumb"></div></div></div></div></nav><div class="header-banner"><div class="header-banner-info"><div class="header-banner-info__title">葵海</div><div class="header-banner-info__subtitle">欢迎来到我的博客空间</div></div></div></div></header><main class="main" id="main"><div class="main-inner"><div class="content-wrap" id="content-wrap"><div class="content content-home" id="content"><section class="postlist"><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2026/01/01/%E8%BD%BB%E6%9D%BE%E5%85%A5%E9%97%A8AI-Agent/">轻松入门AI Agent</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2026-01-01</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2026-01-01</span></span></div></header><div class="post-body"><div class="post-excerpt">
        <h1 id="轻松入门-AI-Agent">
          <a href="#轻松入门-AI-Agent" class="heading-link"><i class="fas fa-link"></i></a><a href="#轻松入门-AI-Agent" class="headerlink" title="轻松入门 AI Agent"></a>轻松入门 AI Agent</h1>
      
        <h2 id="Agent-是什么">
          <a href="#Agent-是什么" class="heading-link"><i class="fas fa-link"></i></a><a href="#Agent-是什么" class="headerlink" title="Agent 是什么"></a>Agent 是什么</h2>
      
        <h3 id="简介">
          <a href="#简介" class="heading-link"><i class="fas fa-link"></i></a><a href="#简介" class="headerlink" title="简介"></a>简介</h3>
      <p>AI Agent ，即人工智能代理，也称为<strong>智能体</strong>。它是一种能够感知环境、做出决策并且采取行动的系统。这些系统能够执行被动的任务，也能够主动寻找解决问题的方法，适应环境的变化，并在没有人直接干预的情况下做出决策。</p>
<p>LLMs 的强大推理能力让 AI Agent 的表现跨越式提升。让 AI Agent 向决策性转型。AI Agent 的技术框架涉及多个层面，包括规划、记忆、工具和行动，其中 <strong>规划、记忆、工具</strong> 是AI Agent 的3个核心组件。</p>

        <h3 id="Agent-的3个核心组件">
          <a href="#Agent-的3个核心组件" class="heading-link"><i class="fas fa-link"></i></a><a href="#Agent-的3个核心组件" class="headerlink" title="Agent 的3个核心组件"></a>Agent 的3个核心组件</h3>
      <ul>
<li><p><strong>Planning 规划</strong></p>
<p>Agent 需要具备规划（同时也包含决策）能力，能有效执行复杂任务。这涉及子目标的分解（Subgoal Decomposition）、连续的思考（思维链）、自我反思和批评（Self-Critics），以及对过去行动的反思（Reflection）。</p>
</li>
<li><p><strong>Memory 记忆</strong></p>
<p>记忆包含短期记忆和长期记忆。短期记忆和上下文学习有关，属于提示工程的一部分；而长期记忆涉及信息的长时间保留和检索，通常利用外部向量存储和快速检索（RAG）。</p>
</li>
<li><p><strong>Tool 工具</strong></p>
<p>包含 Agent 可能调用的各种工具，如日历、计算器、代码解释器和搜索功能等。由于大模型一旦完成预训练，其内部能力和知识边界就基本固定下来，而且难以拓展，因此这些工具显得尤其重要。这些工具可以扩展 Agent 的能力，使其能够执行更复杂的任务，Agent 基于规划和记忆来执行具体的行动。这可能包括与外部世界互动，或者通过调用工具来完成一个动作。</p>
</li>
</ul>
<img src="/2026/01/01/%E8%BD%BB%E6%9D%BE%E5%85%A5%E9%97%A8AI-Agent/%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6.png" class title="核心组件">




        <h2 id="AI-Agent-工作流程">
          <a href="#AI-Agent-工作流程" class="heading-link"><i class="fas fa-link"></i></a><a href="#AI-Agent-工作流程" class="headerlink" title="AI Agent 工作流程"></a>AI Agent 工作流程</h2>
      <p>AI Agent 通过一个完整的流程互相关联各个组件来处理和解决任务。</p>
<ul>
<li><p><strong>接收任务 Task Receiving</strong></p>
<p>Agent 首先通过读入提示（即图中的查询+附加知+人设指示）来接收需要处理的任务。</p>
</li>
<li><p><strong>记忆更新 Memory Update</strong></p>
<p>Agent 根据具体任务更新系统的记忆，确保所有相关信息都是最新的，以便在处理任务时使用。</p>
</li>
<li><p><strong>记忆检索 Memory Retrieval</strong></p>
<p>由于记忆可能非常庞大，因此需要从记忆中检索相关信息，或者在必要时进行截断，以便高效处理信息。</p>
</li>
<li><p><strong>任务规划 Task Plan</strong></p>
<p>基于提供的结构化工具、记忆和查询提示，大模型生成一个包含任务名称的计划，计划包含后续步骤和动作，其中说明了需要调用哪些工具及参数。</p>
</li>
<li><p><strong>工具执行 Tool Execution</strong></p>
<p>如果在任务规划模块产生的是任务完成的信号那么循环将终止，并提示 Agent 任务完成，可以生成结论，否则，系统将调用并执行指定的工具。大模型在观察工具生成的指定格式的结果后，将其整合到任务记忆中。</p>
</li>
<li><p><strong>总结 Concluding</strong></p>
<p>系统会总结出最终的答案，已完成整个任务处理过程。</p>
</li>
</ul>

        <h2 id="AI-Agent-的规划能力">
          <a href="#AI-Agent-的规划能力" class="heading-link"><i class="fas fa-link"></i></a><a href="#AI-Agent-的规划能力" class="headerlink" title="AI Agent 的规划能力"></a>AI Agent 的规划能力</h2>
      <p>一个复杂的任务通常涉及许多步骤。AI Agent 需要知道他们是什么，并且提前规划。</p>

        <h3 id="任务分解-Task-Decomposition">
          <a href="#任务分解-Task-Decomposition" class="heading-link"><i class="fas fa-link"></i></a><a href="#任务分解-Task-Decomposition" class="headerlink" title="任务分解 Task Decomposition"></a>任务分解 Task Decomposition</h3>
      <ul>
<li><p><strong>思维链 Chain of thought, CoT</strong></p>
<p>模型被指示逐步思考，以利用更多的测试时间计算将困难任务分解成更小、更简单的步骤。 CoT 将大任务转化为多个可管理的任务，并揭示了模型思考过程。</p>
</li>
<li><p><strong>思维树 Tree of Thoughts, ToT</strong></p>
<p>通过在每个步骤探索多种推理可能性，进而形成一种树状结构。思维树可以用不同的搜索方法，例如 BFS 或 DFS，并且通过提示或投票来评估每个步骤。</p>
</li>
</ul>

        <h3 id="AI-Agent-自我反思自我决策-ReAct">
          <a href="#AI-Agent-自我反思自我决策-ReAct" class="heading-link"><i class="fas fa-link"></i></a><a href="#AI-Agent-自我反思自我决策-ReAct" class="headerlink" title="AI Agent 自我反思自我决策 ReAct"></a>AI Agent 自我反思自我决策 ReAct</h3>
      <p>自我反思是自主代理通过细化过去的行动决策和纠正之前的错误来迭代改进的重要方面。它在试错不可避免的真实世界任务中扮演着关键角色。</p>
<p><strong>大模型 ReAct</strong> 是一种新兴的技术框架，旨在通过逻辑推理和行动序列的构建，使大语言模型能够达成特定的目标。这一框架的核心思想是赋予机器模型类似人类的推理和行动能力，从而在各种任务中实现更高效、更智能的决策和操作。</p>
<p>ReAct 框架主要由三个关键概念组成：<strong>Thought（思考）、Action（行动）、Observation（观察）</strong> 。</p>
<ul>
<li><p><strong>Thought</strong></p>
<p>由 LLM 模型生成，是 LLM 产生行为和依据的基础。它代表了模型在面对特定任务时的逻辑推理过程，是决策的前提。</p>
</li>
<li><p><strong>Action</strong></p>
<p>指 LLM 判断本次需要执行的具体行为。这通常涉及选择合适的工具或 API，并生成所需的参数，以实现目标行动。</p>
</li>
<li><p><strong>Observation</strong></p>
<p>LLM 框架对于外界输入的获取，类似于 LLM 的五官，将外界的反馈信息同步给 LLM 模型，协助进一步的分析和决策。</p>
</li>
</ul>

        <h2 id="AI-Agent-的记忆机制">
          <a href="#AI-Agent-的记忆机制" class="heading-link"><i class="fas fa-link"></i></a><a href="#AI-Agent-的记忆机制" class="headerlink" title="AI Agent 的记忆机制"></a>AI Agent 的记忆机制</h2>
      <p>大模型形成记忆的机制可以总结为以下几种：</p>
<ul>
<li><p><strong>通过预训练形成记忆</strong></p>
<p>大模型在大量包含世界知识的数据集上进行预训练。在预训练中，大模型通过调整神经网络的权重，学习理解和生成人类语言，这可以被视为其记忆的形成过程，通过使用深度学习神经网络和梯度下降等技术，大模型可以不断提高基于输入预测或生成文本的能力，进而形成世界知识和长期记忆。</p>
</li>
<li><p><strong>上下文互动</strong></p>
<p>大模型在执行任务时，会将长期记忆和提供的上下文结合起来使用。理想情况下，如果上下文包含与大模型的记忆知识冲突的任务相关信息，那么大模型应优先考虑上下文，以生成更准确和具有上下文特定性的回应。通过诸如知识意识型微调等方法，可以增强大模型在使用上下文和记忆知识方面的可控性和鲁棒性。</p>
</li>
<li><p><strong>针对特定任务的微调进行增强</strong></p>
<p>大模型可以在更具体的数据上进一步微调，以适应特定行为或提高特定任务的性能。例如，针对 SAT（可满足性）问题数据集进行微调的大模型在回答此类问题时会更加熟练。</p>
</li>
<li><p><strong>大模型与外部记忆系统整合</strong></p>
<p>通过提供长期记忆来增强大模型性能，使大模型能够记住和回忆过去的互动、理解用户的个性并提供更个性化的互动。这涉及动态个性理解、使用双塔密集检索模型的记忆检索，以及受艾宾浩斯遗忘曲线理论启发的记忆更新机制等。RAG也可视为和外部知识系统整合的过程，这相当于给大模型提供了一个外挂第二大脑。</p>
</li>
</ul>
<img src="/2026/01/01/%E8%BD%BB%E6%9D%BE%E5%85%A5%E9%97%A8AI-Agent/Agent%E7%9A%84%E8%AE%B0%E5%BF%86%E6%9C%BA%E5%88%B6.drawio.png" class title="Agent的记忆机制.drawio">


        <h2 id="AI-Agent-的工具调用能力">
          <a href="#AI-Agent-的工具调用能力" class="heading-link"><i class="fas fa-link"></i></a><a href="#AI-Agent-的工具调用能力" class="headerlink" title="AI Agent 的工具调用能力"></a>AI Agent 的工具调用能力</h2>
      <p>工具使用是人类的一个显著和独特的特征。我们创造、修改和利用外部物体来做超出我们身体和认知极限的事情。调用工具的能力被视为 Agent 的核心功能之一。这些工具可以提供额外的数据、处理能力、专业知识或其他资源，使 Agent 能够执行更加复杂的任务。</p>
</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2025/12/07/MCP-Function-Calling%E5%92%8CA2A/">MCP, Function Calling和A2A</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2025-12-07</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2025-12-27</span></span></div></header><div class="post-body"><div class="post-excerpt">
        <h1 id="MCP、Function-Calling和A2A">
          <a href="#MCP、Function-Calling和A2A" class="heading-link"><i class="fas fa-link"></i></a><a href="#MCP、Function-Calling和A2A" class="headerlink" title="MCP、Function Calling和A2A"></a>MCP、Function Calling和A2A</h1>
      <p><strong>MCP:</strong> Model Context Protocol，AI Agent 和 AI Tools 之间的工具发现、注册和调用协议。注意：MCP 和大模型并没有很大关系，不要被 Model 误导了。</p>
<p><strong>Function Calling:</strong> AI Agent 和 AI Model 之间的工具调用协议。</p>
<p><strong>A2A:</strong> Agent to Agent Protocol，AI Agent 和 AI Agent之间的工具调用协议。</p>

        <h2 id="MCP的工作流程">
          <a href="#MCP的工作流程" class="heading-link"><i class="fas fa-link"></i></a><a href="#MCP的工作流程" class="headerlink" title="MCP的工作流程"></a>MCP的工作流程</h2>
      <p><strong>MCP</strong> 是一种标准化协议，用来把大模型和外部工具&#x2F;数据&#x2F;能力解耦，通过上下文协商，能力发现，调用执行，结果回填，让 LLM 像调用函数一样安全、可控地使用外部世界。</p>

        <h3 id="参与角色">
          <a href="#参与角色" class="heading-link"><i class="fas fa-link"></i></a><a href="#参与角色" class="headerlink" title="参与角色"></a>参与角色</h3>
      <div class="table-container"><table>
<thead>
<tr>
<th>角色</th>
<th>职责</th>
</tr>
</thead>
<tbody><tr>
<td><strong>LLM &#x2F; Agent</strong></td>
<td>负责推理、决策、生成回答</td>
</tr>
<tr>
<td><strong>MCP Client</strong></td>
<td>模型一侧的“适配器”，负责和 MCP Server 通信</td>
</tr>
<tr>
<td><strong>MCP Server</strong></td>
<td>对外暴露资源、工具、Prompt 的服务</td>
</tr>
<tr>
<td><strong>External Systems</strong></td>
<td>文件系统、数据库、API、GitHub、Notion 等</td>
</tr>
</tbody></table></div>

        <h3 id="MCP-的标准流程">
          <a href="#MCP-的标准流程" class="heading-link"><i class="fas fa-link"></i></a><a href="#MCP-的标准流程" class="headerlink" title="MCP 的标准流程"></a>MCP 的标准流程</h3>
      <ul>
<li>用户提出任务<ul>
<li>LLM接收到自然语言请求。</li>
</ul>
</li>
<li>模型进行意图判断<ul>
<li>模型会在内部进行判断：<ul>
<li>是否需要<strong>外部信息</strong>；</li>
<li>是否有<strong>可用工具</strong>能更好完成任务；</li>
</ul>
</li>
<li>如果只是闲聊，流程到此结束。</li>
<li>如果需要工具，进入MCP流程。</li>
</ul>
</li>
<li>MCP Client 向 Server 查询能力<ul>
<li>MCP Client 会向 MCP Server询问是否能够提供 resources、tools、prompts；</li>
<li>Server 返回结构化描述（JSON Schema)，比如可读文件、可调用参数、参数说明等。</li>
</ul>
</li>
<li>模型选择合适的能力<ul>
<li>模型基于返回的元数据进行推理，模型只选择，真正的执行由MCP Client完成。<ul>
<li>代码：<code>read_file</code></li>
<li>数据：<code>query_database</code></li>
<li>操作：<code>run_command</code></li>
</ul>
</li>
</ul>
</li>
<li>MCP Client 调用 MCP Server<ul>
<li>MCP Server会执行以下操作：<ul>
<li>执行真实操作；</li>
<li>做权限与安全控制；</li>
<li>返回结果（文本&#x2F;JSON&#x2F;二进制）</li>
</ul>
</li>
</ul>
</li>
<li>结果作为上下文回注模型<ul>
<li>工具返回的结果不会直接给用户，而是作文新的上下文，注入模型输入中。</li>
</ul>
</li>
<li>模型生成最终回答<ul>
<li>LLM会综合用户原始问题、工具返回结果、自身推理能力，然后生成总结、建议、代码或者分析报告。</li>
</ul>
</li>
</ul>

        <h3 id="MCP-缺点">
          <a href="#MCP-缺点" class="heading-link"><i class="fas fa-link"></i></a><a href="#MCP-缺点" class="headerlink" title="MCP 缺点"></a>MCP 缺点</h3>
      <ul>
<li>架构复杂度高<ul>
<li>需要 Client + Server + Schema 三层；</li>
<li>不适合小项目或一次性脚本；</li>
</ul>
</li>
<li>对工程能力要求高<ul>
<li>要设计工具接口（JSON Schema）</li>
<li>要处理权限、沙箱、安全；</li>
</ul>
</li>
<li>调试体验差，错误可能来自模型决策、Client 调用或者 Server 实现</li>
<li>对模型能力有依赖，工具选择是由模型推理完成的</li>
<li>不适合纯文本任务</li>
</ul>

        <h2 id="幻觉">
          <a href="#幻觉" class="heading-link"><i class="fas fa-link"></i></a><a href="#幻觉" class="headerlink" title="幻觉"></a>幻觉</h2>
      <p><strong>幻觉就是 LLM 输出了看起来很合理，但实际上是错误的、虚构或未经验证的内容，并且语气很自信。</strong></p>
<p><strong>幻觉产生的原因：</strong></p>
<ul>
<li>语言模型≠事实模型，LLM的训练目标是预测下一个最可能的 token；</li>
<li>上下文信息不足，比如模型拿不到外部数据、上下文缺失被截断，或者问题本身不完整；</li>
<li>Prompt暗示了必须回答，即使LLM不知道，也会硬编一个详细实现；</li>
<li>训练数据的统计偏差，模型学到的是出现概率，不是真实值。</li>
</ul>
<p><strong>解决办法：</strong></p>
<ul>
<li><p>不要让模型猜，在系统中加入：</p>
<ul>
<li>如果信息不足，请明确说不知道；</li>
<li>不要编造，不确定就标注不确定性；</li>
</ul>
</li>
<li><p>把事实来源从模型里移出来（RAG），模型不负责记事实，只负责用事实</p>
<ul>
<li>用搜索&#x2F;数据库&#x2F;文档系统查到真实信息；</li>
<li>把结果作为上下文喂给模型；</li>
<li>要求只基于提供的材料回答；</li>
</ul>
</li>
<li><p>要求引用&#x2F;证据对齐</p>
<ul>
<li>每个结论必须对应一段输入材料；</li>
<li>没有证据就不能下结论；</li>
</ul>
</li>
<li><p>用结构化输出替代自由发挥，结构化会限制模型编故事的空间</p>
</li>
<li></li>
</ul>

        <h2 id="AI-Agent">
          <a href="#AI-Agent" class="heading-link"><i class="fas fa-link"></i></a><a href="#AI-Agent" class="headerlink" title="AI Agent"></a>AI Agent</h2>
      <p>AI Agent 是一种能自主感知环境、指定规划并执行动作，以完成特定任务的智能系统。AI Agent 区别于传统的 SOP（Standard Operation Procedure），workflow 在于传统的方式是基于规则的自动化、有明确的、预设的“如果-那么”规则。而 AI Agent 是基于目标的自主性，能自主规划、执行、调整。能感知环境变化、动态调整策略。能处理模糊、不确定的任务。其本质在于大模型本身所具有的强大语言理解、推理和泛化能力。</p>
<p>一个完整的 Agent 除了LLM（大脑）以外，还需要：</p>
<ul>
<li><strong>感知模块：</strong> 通过 API、搜索引擎、文件系统等获取外部信息；</li>
<li><strong>工具集：</strong> 可以调用各种函数、API、软件来影响现实。这就是 Function Calling 、MCP 等技术的用武之地；</li>
<li><strong>记忆模块：</strong> 通过向量数据库或记忆流记录过去的历史，用于长期规划和参考；</li>
</ul>
</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2025/12/06/transformer%E6%9E%B6%E6%9E%84/">transformer架构</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2025-12-06</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2025-12-07</span></span></div></header><div class="post-body"><div class="post-excerpt">
        <h1 id="transformer">
          <a href="#transformer" class="heading-link"><i class="fas fa-link"></i></a><a href="#transformer" class="headerlink" title="transformer"></a>transformer</h1>
      <p><strong>1. 随着seq_len的增加，推导 transformer layers 的计算复杂度</strong></p>
<ul>
<li>序列长度：L&#x3D;seq_len</li>
<li>模型维度：d（通常是 d_model）</li>
<li>注意力头数：h，每个头的维度 dk&#x3D;d&#x2F;h</li>
<li>前馈层隐层维度：dff（通常≈4d）</li>
</ul>
<img src="/2025/12/06/transformer%E6%9E%B6%E6%9E%84/image-20251206224631402.png" class title="image-20251206224631402">



<p><strong>2. 是否了解稀疏注意力机制，具体讲讲有哪些稀疏注意力</strong></p>
<p>标准 Transformer 的注意力复杂度是：<br>$$<br>O(n^2⋅d)<br>$$<br>其中 <strong>n 是序列长度，</strong> 随着 n 增大代价急剧上升。</p>
<p><strong>稀疏注意力的核心思想：并不是所有 token 都需要互相注意，将注意力矩阵变稀疏，</strong> 从而把复杂度降到线性或者次线性：<br>$$<br>O(n) 或 O(nlogn)<br>$$<br>稀疏方式一般分为 <strong>结构化稀疏</strong> 或者 <strong>基于内容的稀疏</strong></p>

        <h2 id="结构化稀疏注意力">
          <a href="#结构化稀疏注意力" class="heading-link"><i class="fas fa-link"></i></a><a href="#结构化稀疏注意力" class="headerlink" title="结构化稀疏注意力"></a>结构化稀疏注意力</h2>
      <ol>
<li>局部注意力（Local &#x2F; Sliding  Window Attention）</li>
<li>Stride Attention（跳跃注意力）</li>
<li>Dilated Attention（扩张卷积式注意力）</li>
<li>Block &#x2F; Chunk Attention（块状注意力）</li>
</ol>

        <h2 id="随机或低秩近似类稀疏注意力">
          <a href="#随机或低秩近似类稀疏注意力" class="heading-link"><i class="fas fa-link"></i></a><a href="#随机或低秩近似类稀疏注意力" class="headerlink" title="随机或低秩近似类稀疏注意力"></a>随机或低秩近似类稀疏注意力</h2>
      <ol>
<li>Reformer 的 LSH Attention （局部敏感哈希稀疏）</li>
<li>Linformer（低秩投影 Attention）</li>
<li>Performer（基于随机特征的线性 Attention）</li>
</ol>

        <h2 id="全局-token-局部-Token-混合注意力">
          <a href="#全局-token-局部-Token-混合注意力" class="heading-link"><i class="fas fa-link"></i></a><a href="#全局-token-局部-Token-混合注意力" class="headerlink" title="全局 token + 局部 Token 混合注意力"></a>全局 token + 局部 Token 混合注意力</h2>
      <ol>
<li>Longformer Hybrid attention</li>
<li>BigBird Block-Sparse Attention (Local + Random + Global)</li>
</ol>

        <h2 id="自适应-x2F-内容相关稀疏">
          <a href="#自适应-x2F-内容相关稀疏" class="heading-link"><i class="fas fa-link"></i></a><a href="#自适应-x2F-内容相关稀疏" class="headerlink" title="自适应 &#x2F; 内容相关稀疏"></a>自适应 &#x2F; 内容相关稀疏</h2>
      <ol>
<li>Sparsemax &#x2F; Entmax Attention（稀疏化的概率分布）</li>
<li>Routing Attention（路由稀疏注意力）</li>
</ol>
<p><strong>3. Padding 的 mask 操作具体如何实现</strong></p>
<p><strong>4. 自注意力机制的公式，为什么要除以sqrt(d_k)</strong></p>
<p>缩放点积注意力：<br>$$<br>Attention(Q, K, V) &#x3D; softmax(QK^T&#x2F;\sqrt d_k)V<br>$$</p>
<ul>
<li>Q, K, V: query, key, value</li>
<li>d_k: key&#x2F;query 的维度</li>
<li>关键操作：<strong>将点积除以根号d_k</strong></li>
</ul>
<p>**为什么要除以sqrt(d_k)**： <strong>防止点积随着维度增大而过大，让 softmax 进入”梯度消失区“</strong></p>
<ul>
<li>softmax 输入更稳定</li>
<li>不易饱和</li>
<li>梯度更健康</li>
<li>训练速度和效果明显提升</li>
</ul>
<p><strong>5. 为什么不使用维度更深的单一注意力机制，而是采用多头注意力，多头注意力（MHA）机制带来了什么优势</strong></p>
<p><strong>为什么不用单一注意力机制：</strong></p>
<ul>
<li>单一注意力会强行把所有模式混合在同一个空间里</li>
<li>难以捕捉多粒度、多类型的关系</li>
<li>学习复杂模式更困难、优化更不稳定</li>
<li>MHA是并行便宜的，而一个深维单头其实更贵</li>
</ul>
<p><strong>多头注意力带来了什么优点：</strong></p>
<ul>
<li>多视角表达能力</li>
<li>提高模型表达能力而不增加太多计算</li>
<li>提高梯度稳定性和训练可控性</li>
<li>天然的结构正则化</li>
<li>更多可解释性</li>
</ul>
</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2025/12/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%90%86%E8%AE%BA/">神经网络理论</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2025-12-06</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2026-01-04</span></span></div></header><div class="post-body"><div class="post-excerpt">
        <h2 id="1-在深度神经网络中，为什么会出现梯度消失和梯度爆炸">
          <a href="#1-在深度神经网络中，为什么会出现梯度消失和梯度爆炸" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-在深度神经网络中，为什么会出现梯度消失和梯度爆炸" class="headerlink" title="1. 在深度神经网络中，为什么会出现梯度消失和梯度爆炸"></a><strong>1. 在深度神经网络中，为什么会出现梯度消失和梯度爆炸</strong></h2>
      <blockquote>
<p>梯度消失和梯度爆炸的根本原因是反向传播中梯度的连乘效应。<br>通过<strong>合适的激活函数、权重初始化、归一化方法、残差结构和梯度裁剪</strong>，可以有效缓解甚至解决这些问题。</p>
</blockquote>
<p>神经网络训练依赖反向传播（Backpropagation），梯度通过<strong>链式法则</strong>从输出层逐层向前传播：</p>
<img src="/2025/12/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%90%86%E8%AE%BA/image-20260103112735378.png" class title="image-20260103112735378">

<ul>
<li>如果每一层的梯度 <strong>&lt; 1</strong> → 多次相乘 → <strong>指数级变小</strong> → 梯度消失</li>
<li>如果每一层的梯度 <strong>&gt; 1</strong> → 多次相乘 → <strong>指数级变大</strong> → 梯度爆炸</li>
</ul>
<p>网络越深，这个问题越严重。</p>

        <h3 id="常见原因">
          <a href="#常见原因" class="heading-link"><i class="fas fa-link"></i></a><a href="#常见原因" class="headerlink" title="常见原因"></a><strong>常见原因</strong></h3>
      <p><strong>激活函数的影响</strong></p>

        <h4 id="（1）Sigmoid-x2F-Tanh">
          <a href="#（1）Sigmoid-x2F-Tanh" class="heading-link"><i class="fas fa-link"></i></a><a href="#（1）Sigmoid-x2F-Tanh" class="headerlink" title="（1）Sigmoid &#x2F; Tanh"></a>（1）Sigmoid &#x2F; Tanh</h4>
      <ul>
<li>导数最大值 &lt; 1</li>
<li>在饱和区（输入很大或很小）导数 ≈ 0</li>
<li>深层网络中极易导致 <strong>梯度消失</strong></li>
</ul>

        <h4 id="（2）ReLU">
          <a href="#（2）ReLU" class="heading-link"><i class="fas fa-link"></i></a><a href="#（2）ReLU" class="headerlink" title="（2）ReLU"></a>（2）ReLU</h4>
      <ul>
<li>正区间导数 &#x3D; 1（缓解梯度消失）</li>
<li>负区间导数 &#x3D; 0（可能导致“神经元死亡”）</li>
</ul>
<p><strong>权重初始化不当</strong></p>
<ul>
<li>权重过小 → 梯度逐层缩小 → 梯度消失</li>
<li>权重过大 → 梯度逐层放大 → 梯度爆炸</li>
</ul>
<p><strong>特殊结构中的问题（如RNN）</strong></p>
<ul>
<li>RNN 在时间维度上反复使用同一权重矩阵</li>
<li>本质也是<strong>深层网络的连乘问题</strong></li>
<li>梯度消失 &#x2F; 爆炸在 RNN 中尤为严重</li>
</ul>

        <h3 id="针对梯度消失的解决办法">
          <a href="#针对梯度消失的解决办法" class="heading-link"><i class="fas fa-link"></i></a><a href="#针对梯度消失的解决办法" class="headerlink" title="针对梯度消失的解决办法"></a>针对梯度消失的解决办法</h3>
      <ul>
<li>合理选择激活函数<ul>
<li>使用 ReLU &#x2F; Leaky ReLU &#x2F; ELU &#x2F; GELU</li>
<li>避免在深层网络中使用 Sigmoid</li>
</ul>
</li>
<li>合理的权重初始化：保证前向传播和反向传播中，方差不会迅速缩小或放大<ul>
<li>Xavier 初始化 (适合 Tanh &#x2F; Sigmoid)</li>
<li>He 初始化（适合 ReLU）</li>
</ul>
</li>
<li>使用 Batch Normalization<ul>
<li>规范化每一层输入分布</li>
<li>让激活值不易进入饱和区</li>
<li>使梯度更稳定</li>
</ul>
</li>
<li>残差连接<ul>
<li>梯度可以走捷径</li>
<li>极大缓解深层网络的梯度消失问题</li>
</ul>
</li>
</ul>

        <h3 id="针对梯度爆炸的解决办法">
          <a href="#针对梯度爆炸的解决办法" class="heading-link"><i class="fas fa-link"></i></a><a href="#针对梯度爆炸的解决办法" class="headerlink" title="针对梯度爆炸的解决办法"></a>针对梯度爆炸的解决办法</h3>
      <ul>
<li>梯度裁剪：常用于 RNN &#x2F; LSTM<ul>
<li>防止梯度突然变得极大</li>
<li>不影响梯度方向，只限制大小</li>
</ul>
</li>
<li>合理设置学习率<ul>
<li>学习率过大，梯度更新剧烈，容易导致梯度爆炸</li>
<li>可以用学习率衰减、自适应优化器（Adam）</li>
</ul>
</li>
<li>合理设置权重初始化<ul>
<li>权重过大是梯度爆炸的直接原因之一</li>
<li>Xavier &#x2F; He 初始化同样有助于防止爆炸</li>
</ul>
</li>
</ul>

        <h2 id="2-BatchNorm，如果输入为-b-c-h-w-，那么可训练参数是多少">
          <a href="#2-BatchNorm，如果输入为-b-c-h-w-，那么可训练参数是多少" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-BatchNorm，如果输入为-b-c-h-w-，那么可训练参数是多少" class="headerlink" title="2. BatchNorm，如果输入为(b, c, h, w)，那么可训练参数是多少"></a><strong>2. BatchNorm，如果输入为(b, c, h, w)，那么可训练参数是多少</strong></h2>
      <blockquote>
<p>对于输入为 (b,c,h,w)(b, c, h, w)(b,c,h,w) 的 BatchNorm2d，可训练参数只有每个通道的缩放 γ 和偏置 β，因此一共是 <strong>2c 个</strong>。</p>
</blockquote>
<p>假设输入张量形状为：<br>$$<br>(b,c,h,w)<br>$$<br><strong>b</strong>：batch size</p>
<p><strong>c</strong>：通道数（channels）</p>
<p><strong>h, w</strong>：空间维度</p>
<img src="/2025/12/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%90%86%E8%AE%BA/image-20260103114617781.png" class title="image-20260103114617781">

<p><strong>可训练参数是 2c</strong></p>

        <h2 id="3-BatchNorm-和-LayerNorm-的区别，分别的适用场景">
          <a href="#3-BatchNorm-和-LayerNorm-的区别，分别的适用场景" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-BatchNorm-和-LayerNorm-的区别，分别的适用场景" class="headerlink" title="3. BatchNorm 和 LayerNorm 的区别，分别的适用场景"></a><strong>3. BatchNorm 和 LayerNorm 的区别，分别的适用场景</strong></h2>
      <blockquote>
<p>BN 在 batch 维度上做归一化，依赖 batch size;</p>
<p>LN 在特征维度上做归一化，与 batch size 无关。</p>
</blockquote>
<p>设输入张量为：</p>
<ul>
<li>CNN 常见：(b,c, h,w)</li>
<li>NLP &#x2F; Transformer 常见：(b, seq, d)</li>
</ul>

        <h3 id="BN">
          <a href="#BN" class="heading-link"><i class="fas fa-link"></i></a><a href="#BN" class="headerlink" title="BN"></a>BN</h3>
      <p><strong>BN 的可训练参数为 2c；</strong></p>
<p>BN 的特点：</p>
<p><strong>优点：</strong></p>
<ul>
<li>加速收敛；</li>
<li>缓解梯度消失 &#x2F; 爆炸</li>
<li>有正则化效果（类似 Dropout）</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>强依赖 batch size;</li>
<li>小 batch &#x2F; batch size &#x3D; 1 时效果很差；</li>
<li>训练和推理行为不一致（running mean &#x2F; var）</li>
</ul>
<p><strong>BN 典型使用场景</strong></p>
<ul>
<li>CNN &#x2F;  视觉任务<ul>
<li>图像分类；</li>
<li>目标检测（batch 够大）</li>
<li>语义分割（大 batch）</li>
</ul>
</li>
<li>不太适合<ul>
<li>RNN</li>
<li>Transformer</li>
<li>在线学习 &#x2F; 小 batch &#x2F; 单样本推理</li>
</ul>
</li>
</ul>

        <h3 id="LN">
          <a href="#LN" class="heading-link"><i class="fas fa-link"></i></a><a href="#LN" class="headerlink" title="LN"></a>LN</h3>
      <p><strong>LN 的可训练参数为 2d，d 为特征维度。</strong></p>
<p><strong>优点：</strong></p>
<ul>
<li>与 batch size 无关；</li>
<li>训练和推理完全一致；</li>
<li>非常稳定；</li>
<li>适合序列建模；</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>在 CNN 中效果通常不如 BN；</li>
<li>不利用 batch 统计信息；</li>
</ul>
<p><strong>BN 典型使用场景</strong></p>
<ul>
<li>NLP &#x2F; Transformer<ul>
<li>BERT；</li>
<li>GPT；</li>
<li>ViT（Transformer 部分）</li>
</ul>
</li>
<li>不太适合<ul>
<li>小 batch;</li>
<li>变长序列；</li>
<li>自回归模型；</li>
</ul>
</li>
</ul>

        <h2 id="4-交叉熵公式">
          <a href="#4-交叉熵公式" class="heading-link"><i class="fas fa-link"></i></a><a href="#4-交叉熵公式" class="headerlink" title="4.交叉熵公式"></a><strong>4.交叉熵公式</strong></h2>
      <blockquote>
<p><strong>交叉熵本质是负对数似然，二分类用 BCE，多分类用 Softmax + CE；最小化交叉熵等价于最小化 KL 散度。</strong></p>
<p>KL 散度是衡量两个概率分布差异的非对称量，训练模型最小化交叉熵，本质上是在最小化 KL 散度。</p>
</blockquote>
<p>对于真实分布 P 和 预测分布 Q，交叉熵定义为：</p>
<img src="/2025/12/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%90%86%E8%AE%BA/image-20260104222812593.png" class title="image-20260104222812593">

<p>含义：</p>
<ul>
<li>用真实分布 P 来衡量</li>
<li>使用编码方案 Q 时的期望信息量</li>
<li><strong>Q 越接近 P，交叉熵越小</strong></li>
</ul>

        <h3 id="分类任务中的交叉熵">
          <a href="#分类任务中的交叉熵" class="heading-link"><i class="fas fa-link"></i></a><a href="#分类任务中的交叉熵" class="headerlink" title="分类任务中的交叉熵"></a>分类任务中的交叉熵</h3>
      
        <h4 id="二分类交叉熵（BCE）">
          <a href="#二分类交叉熵（BCE）" class="heading-link"><i class="fas fa-link"></i></a><a href="#二分类交叉熵（BCE）" class="headerlink" title="二分类交叉熵（BCE）"></a>二分类交叉熵（BCE）</h4>
      <img src="/2025/12/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%90%86%E8%AE%BA/image-20260104223220451.png" class title="image-20260104223220451">




        <h4 id="多分类交叉熵（CE）">
          <a href="#多分类交叉熵（CE）" class="heading-link"><i class="fas fa-link"></i></a><a href="#多分类交叉熵（CE）" class="headerlink" title="多分类交叉熵（CE）"></a>多分类交叉熵（CE）</h4>
      <img src="/2025/12/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%90%86%E8%AE%BA/image-20260104223249129.png" class title="image-20260104223249129">


        <h4 id="Softmax-Cross-Entropy">
          <a href="#Softmax-Cross-Entropy" class="heading-link"><i class="fas fa-link"></i></a><a href="#Softmax-Cross-Entropy" class="headerlink" title="Softmax + Cross Entropy"></a>Softmax + Cross Entropy</h4>
      <img src="/2025/12/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%90%86%E8%AE%BA/image-20260104223421008.png" class title="image-20260104223421008">






        <h2 id="5-对比学习">
          <a href="#5-对比学习" class="heading-link"><i class="fas fa-link"></i></a><a href="#5-对比学习" class="headerlink" title="5.对比学习"></a><strong>5.对比学习</strong></h2>
      <blockquote>
<p><strong>对比学习的目标是：把相似的样本表示拉近，把不相似的样本表示推远。</strong></p>
<p>它不依赖人工标签，而是通过构造正样本对 &#x2F; 负样本对 来学习判别性的表示。</p>
</blockquote>
<ul>
<li><strong>正样本对：</strong> 语义相同的两个视角，比如同一张图片的两种数据增强；</li>
<li><strong>负样本对：</strong> 语义不同，比如不同照片。</li>
</ul>
<p>学习到的 embedding 空间应满足：</p>
<ul>
<li>正对：距离小，相似度高；</li>
<li>负对：距离大，相似度低；</li>
</ul>

        <h2 id="6-InfoNCE，温度系数的作用">
          <a href="#6-InfoNCE，温度系数的作用" class="heading-link"><i class="fas fa-link"></i></a><a href="#6-InfoNCE，温度系数的作用" class="headerlink" title="6.InfoNCE，温度系数的作用"></a><strong>6.InfoNCE，温度系数的作用</strong></h2>
      <blockquote>
<p><strong>InfoNCE 把找到正样本建模成一个 softmax 分类问题，通过拉高正样本相似度，压低负样本相似度来学习表示。</strong></p>
</blockquote>
<p>温度系数控制相似度分布的尖锐程度，决定了对比的强弱与训练稳定性。</p>

        <h2 id="7-SoftMax">
          <a href="#7-SoftMax" class="heading-link"><i class="fas fa-link"></i></a><a href="#7-SoftMax" class="headerlink" title="7.SoftMax"></a><strong>7.SoftMax</strong></h2>
      <p><strong>SoftMax 将任意实数向量映射为概率分布，实现多分类建模：</strong></p>
<img src="/2025/12/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%90%86%E8%AE%BA/image-20260104225252298.png" class title="image-20260104225252298">




        <h2 id="8-adam优化器">
          <a href="#8-adam优化器" class="heading-link"><i class="fas fa-link"></i></a><a href="#8-adam优化器" class="headerlink" title="8.adam优化器"></a><strong>8.adam优化器</strong></h2>
      <p>Adam &#x3D; <strong>Momentum + RMSProp 的结合</strong>，<strong>Adam 通过一阶和二阶矩的自适应估计，为每个参数动态调整学习率，兼具快速收敛和训练稳定性，是深度学习中最常用的优化器之一。</strong></p>
<p>它希望同时解决：</p>
<ul>
<li><strong>SGD 收敛慢、震荡大</strong></li>
<li><strong>不同参数梯度尺度差异大</strong></li>
<li><strong>学习率难以手动调节</strong></li>
</ul>
<p>核心思想：<strong>对每个参数自适应调整学习率，同时利用梯度的一阶和二阶矩信息。</strong></p>
<ul>
<li><strong>一阶矩：</strong> 梯度的指数加权平均；</li>
<li><strong>二阶矩：</strong> 梯度平方的指数加权平均；</li>
</ul>

        <h2 id="9-Kimi-K2-模型使用的-Muon优化器">
          <a href="#9-Kimi-K2-模型使用的-Muon优化器" class="heading-link"><i class="fas fa-link"></i></a><a href="#9-Kimi-K2-模型使用的-Muon优化器" class="headerlink" title="9. Kimi K2 模型使用的 Muon优化器"></a><strong>9. Kimi K2 模型使用的 Muon优化器</strong></h2>
      <p><strong>Muon 是一种方向归一化的动量优化器，它在参数更新时，显示消除梯度的尺度信息，只保留方向信息。</strong></p>
<div class="table-container"><table>
<thead>
<tr>
<th>维度</th>
<th>Adam &#x2F; AdamW</th>
<th>Muon</th>
</tr>
</thead>
<tbody><tr>
<td>是否使用梯度幅值</td>
<td>✅</td>
<td>❌</td>
</tr>
<tr>
<td>是否自适应学习率</td>
<td>✅（逐参数）</td>
<td>❌（全局）</td>
</tr>
<tr>
<td>关注重点</td>
<td>scale + direction</td>
<td><strong>direction only</strong></td>
</tr>
<tr>
<td>对初始化敏感</td>
<td>较敏感</td>
<td><strong>不敏感</strong></td>
</tr>
<tr>
<td>大模型稳定性</td>
<td>一般</td>
<td><strong>很强</strong></td>
</tr>
<tr>
<td>调参难度</td>
<td>中</td>
<td><strong>低</strong></td>
</tr>
</tbody></table></div>
</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2025/12/06/%E5%9F%BA%E6%9C%AC%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/">基本机器学习理论</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2025-12-06</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2025-12-06</span></span></div></header><div class="post-body"><div class="post-excerpt"></div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2025/12/06/pytorch%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/">pytorch的基本操作</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2025-12-06</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2025-12-06</span></span></div></header><div class="post-body"><div class="post-excerpt">
        <h1 id="Pytorch">
          <a href="#Pytorch" class="heading-link"><i class="fas fa-link"></i></a><a href="#Pytorch" class="headerlink" title="Pytorch"></a>Pytorch</h1>
      
        <h2 id="Tensor-转置">
          <a href="#Tensor-转置" class="heading-link"><i class="fas fa-link"></i></a><a href="#Tensor-转置" class="headerlink" title="Tensor 转置"></a>Tensor 转置</h2>
      <p><strong>1. 交换张量的两个指定维度</strong></p>
<ul>
<li><strong>方法一</strong>：<code>transpose(dim0, dim1)</code></li>
</ul>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)	<span class="comment"># 形状[2, 3, 4]</span></span><br><span class="line">y = x.transpose(<span class="number">1</span>, <span class="number">2</span>)		<span class="comment"># 交换维度1和2</span></span><br><span class="line"><span class="built_in">print</span>(y.shape)					<span class="comment"># [2, 4, 3]</span></span><br></pre></td></tr></table></div></figure>

<ul>
<li><strong>方法二</strong>： <code>permute</code> （适合交换多个维度）</li>
</ul>
<p>可以通过重新排列所有维度来实现任意两个维度交换。</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 交换 dim = 0 和 dim = 2</span></span><br><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">y = x.permute(<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(y.shape)			<span class="comment"># [3, 4, 2]</span></span><br></pre></td></tr></table></div></figure>




        <h2 id="Tensor-升维-x2F-降维">
          <a href="#Tensor-升维-x2F-降维" class="heading-link"><i class="fas fa-link"></i></a><a href="#Tensor-升维-x2F-降维" class="headerlink" title="Tensor 升维&#x2F;降维"></a>Tensor 升维&#x2F;降维</h2>
      <p><strong>1. 在指定的 dim 位置插入一个大小为 1 的新维度</strong></p>
<p>使用 <code>unsqueeze</code> :</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># y = x.unsqueeze(dim)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x = torch.randn(<span class="number">3</span>, <span class="number">4</span>)		<span class="comment"># shape: (3, 4)</span></span><br><span class="line">y = x.unsqueeze(<span class="number">1</span>)			<span class="comment"># 在 dim = 1 插入新维度</span></span><br><span class="line"><span class="built_in">print</span>(y.shape)				<span class="comment"># torch.Size([3, 1, 4])</span></span><br></pre></td></tr></table></div></figure>




        <h2 id="维度合并">
          <a href="#维度合并" class="heading-link"><i class="fas fa-link"></i></a><a href="#维度合并" class="headerlink" title="维度合并"></a>维度合并</h2>
      <p><strong>1. 将多个张量沿着一个现有的维度 dim 连接起来</strong></p>
<p>使用 <code>torch.cat</code>:</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 语法</span></span><br><span class="line"><span class="comment"># tensor: 由张量组成的序列（list或tuple）</span></span><br><span class="line"><span class="comment"># dim: 指定要连接的维度（必须是已有维度）</span></span><br><span class="line"><span class="comment">#torch.cat(tensor, dim=0)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例</span></span><br><span class="line"><span class="comment"># 沿 dim=0 方向拼接（batch 方向）</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">a = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">b = torch.randn(<span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">out = torch.cat([a, b], dim=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(out.shape)		<span class="comment"># torch.Size([6, 3])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 沿 dim=1 拼接（特征维度）</span></span><br><span class="line">a = torch.randn(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">b = torch.randn(<span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">out = torch.cat([a, b], dim=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(out.shape)		$ torch.Size([<span class="number">3</span>, <span class="number">7</span>])</span><br></pre></td></tr></table></div></figure>


        <h2 id="Tensor-堆叠">
          <a href="#Tensor-堆叠" class="heading-link"><i class="fas fa-link"></i></a><a href="#Tensor-堆叠" class="headerlink" title="Tensor 堆叠"></a>Tensor 堆叠</h2>
      <p><strong>1. 将多个相同形状的张量沿着一个新的维度 dim 堆叠起来</strong></p>
<p><code>torch.stack(tensor, dim)</code> : 在指定的 dim 位置 创建一个新的维度，并把输入张量沿该新维度堆叠。</p>
<ul>
<li>所有张量必须 形状完全相同；</li>
<li>输出张量的形状为：(n, …original shape…) 如果 dim&#x3D;0</li>
</ul>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 三个形状相同的张量</span></span><br><span class="line">t1 = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">t2 = torch.tensor([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line">t3 = torch.tensor([<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在新维度 dim=0 堆叠</span></span><br><span class="line">out0 = torch.stack([t1, t2, t3], dim=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(out0.shape)  <span class="comment"># torch.Size([3, 3])</span></span><br><span class="line"><span class="built_in">print</span>(out0)</span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">        [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在新维度 dim=1 堆叠</span></span><br><span class="line">out1 = torch.stack([t1, t2, t3], dim=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(out1.shape)  <span class="comment"># torch.Size([3, 3])</span></span><br><span class="line"><span class="built_in">print</span>(out1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">4</span>, <span class="number">7</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">5</span>, <span class="number">8</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">6</span>, <span class="number">9</span>]])</span><br></pre></td></tr></table></div></figure>



<p><strong><code>torch.cat</code> 与 <code>torch.stack</code> 对比</strong></p>
<div class="table-container"><table>
<thead>
<tr>
<th>操作</th>
<th>是否创建新维度</th>
<th>形状要求</th>
</tr>
</thead>
<tbody><tr>
<td><code>torch.cat</code></td>
<td>不创建新维度</td>
<td>连接维度以外的形状必须一致</td>
</tr>
<tr>
<td><code>torch.stack</code></td>
<td>创建新维度</td>
<td>所有张量形状必须完全一致</td>
</tr>
</tbody></table></div>

        <h2 id="Tensor-形状改变">
          <a href="#Tensor-形状改变" class="heading-link"><i class="fas fa-link"></i></a><a href="#Tensor-形状改变" class="headerlink" title="Tensor 形状改变"></a>Tensor 形状改变</h2>
      <p><strong>1. 在保持元素总数不变的情况下改变张量的形状</strong></p>
<p>在 <strong>保持元素总数不变</strong> 的前提下改变张量形状，在 PyTorch 中通常使用 <strong><code>torch.reshape</code></strong> 或 **<code>tensor.view</code>**。</p>
<ul>
<li>使用 <code>reshape</code> 改变形状：<code>reshape</code> 会根据需要自动选择是否拷贝数据，更灵活。</li>
</ul>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x = torch.arange(<span class="number">12</span>)      <span class="comment"># 1D: [12]</span></span><br><span class="line">y = x.reshape(<span class="number">3</span>, <span class="number">4</span>)       <span class="comment"># 2D: [3, 4]</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(y.shape)  <span class="comment"># torch.Size([3, 4])</span></span><br></pre></td></tr></table></div></figure>

<ul>
<li>使用 <code>view</code> 改变形状（要求张量内存连续）：<code>view</code> 需要张量是 contiguous（连续内存），否则需要 <code>.contiguous().view(...)</code>。</li>
</ul>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.arange(<span class="number">12</span>)</span><br><span class="line">y = x.view(<span class="number">2</span>, <span class="number">6</span>)</span><br><span class="line"><span class="built_in">print</span>(y.shape)  <span class="comment"># torch.Size([2, 6])</span></span><br></pre></td></tr></table></div></figure>



<p><strong>Torch 中的乘法有哪些？分别有哪些区别</strong></p>
<ol>
<li><p>逐元素相乘</p>
<p><code>torch.mul(a, b)</code> 或 <code>a * b</code></p>
<ul>
<li>要求 a 与 b 的形状可广播</li>
<li>每个对应位置相乘</li>
</ul>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">b = torch.tensor([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line">a * b   <span class="comment"># tensor([ 4, 10, 18])</span></span><br></pre></td></tr></table></div></figure>
</li>
<li><p>矩阵乘法</p>
<p><code>torch.matmul(a, b)</code> 或者 a @ b</p>
<ul>
<li>2D * 2D（标准矩阵乘法）</li>
<li>1D * 2D &#x2F; 2D * 1D（向量-矩阵乘）</li>
<li>高维张量的批量矩阵乘法</li>
<li>自动处理广播</li>
<li>用于线性层、神经网络核心计算</li>
</ul>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">b = torch.randn(<span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line">a @ b     <span class="comment"># shape: (3,5)</span></span><br></pre></td></tr></table></div></figure>
</li>
<li><p>严格的矩阵乘法（不广播）</p>
<p><code>torch.mm(a, b)</code></p>
<ul>
<li>只能用于 2D 张量</li>
<li>不支持 batch, 不支持广播</li>
</ul>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">b = torch.randn(<span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line">torch.mm(a, b)  <span class="comment"># shape (3,5)</span></span><br></pre></td></tr></table></div></figure>
</li>
<li><p>内积 &#x2F; 点乘</p>
<p><code>torch.dot</code></p>
<ul>
<li>仅用于 1D 张量</li>
<li>返回一个标量</li>
<li>计算向量相似度、余弦相似度的中间步骤。</li>
</ul>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">b = torch.tensor([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line">torch.dot(a, b)   <span class="comment"># 1*4 + 2*5 + 3*6 = 32</span></span><br></pre></td></tr></table></div></figure>
</li>
<li><p>批量矩阵-向量乘法</p>
<p><code>torch.mv(a, b)</code></p>
<ul>
<li>a: 2D</li>
<li>b: 1D</li>
<li>结果是 1D 向量</li>
</ul>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">b = torch.randn(<span class="number">4</span>)</span><br><span class="line">torch.mv(a, b)  <span class="comment"># shape (3,)</span></span><br></pre></td></tr></table></div></figure>
</li>
<li><p>批量矩阵乘法</p>
<p><code>torch.bmm(a, b)</code></p>
<ul>
<li>只能用于 3D 张量  (batch, m, n) @ (batch, n, p)</li>
<li>batch 维必须匹配</li>
<li>RNN &#x2F; Transformer 内部常用。</li>
</ul>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">10</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">b = torch.randn(<span class="number">10</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line">torch.bmm(a, b)  <span class="comment"># shape (10, 3, 5)</span></span><br></pre></td></tr></table></div></figure>
</li>
<li><p>逐元素乘的加权求和</p>
<p><code>torch.einsum</code></p>
<ul>
<li>可表达多种乘法，如点积、矩阵乘、转置乘等</li>
</ul>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.einsum(<span class="string">&#x27;ij,jk-&gt;ik&#x27;</span>, a, b)</span><br><span class="line">torch.einsum(<span class="string">&#x27;i,i-&gt;&#x27;</span>, a, b)   <span class="comment"># 点积</span></span><br></pre></td></tr></table></div></figure></li>
</ol>
<div class="table-container"><table>
<thead>
<tr>
<th>操作</th>
<th>接受形状</th>
<th>广播</th>
<th>输出形状</th>
<th>用途</th>
</tr>
</thead>
<tbody><tr>
<td><code>a * b</code> &#x2F; <code>torch.mul</code></td>
<td>任意可广播</td>
<td>✔️</td>
<td>与广播后一致</td>
<td>逐元素乘</td>
</tr>
<tr>
<td><code>a @ b</code> &#x2F; <code>torch.matmul</code></td>
<td>灵活</td>
<td>✔️</td>
<td>矩阵乘逻辑</td>
<td>NN 线性计算</td>
</tr>
<tr>
<td><code>torch.mm</code></td>
<td>2D×2D</td>
<td>❌</td>
<td>(m, p)</td>
<td>严格矩阵乘</td>
</tr>
<tr>
<td><code>torch.dot</code></td>
<td>1D×1D</td>
<td>❌</td>
<td>scalar</td>
<td>向量内积</td>
</tr>
<tr>
<td><code>torch.mv</code></td>
<td>2D×1D</td>
<td>❌</td>
<td>1D</td>
<td>矩阵乘向量</td>
</tr>
<tr>
<td><code>torch.bmm</code></td>
<td>3D×3D</td>
<td>❌</td>
<td>batch 矩阵乘</td>
<td>RNN&#x2F;批处理</td>
</tr>
<tr>
<td><code>torch.einsum</code></td>
<td>任意</td>
<td>自定义</td>
<td>任意</td>
<td>高度灵活</td>
</tr>
</tbody></table></div>
</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2025/11/24/%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83/">生产环境</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2025-11-24</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2025-12-04</span></span></div></header><div class="post-body"><div class="post-excerpt">
        <h1 id="CPU-100-问题怎么排查">
          <a href="#CPU-100-问题怎么排查" class="heading-link"><i class="fas fa-link"></i></a><a href="#CPU-100-问题怎么排查" class="headerlink" title="CPU 100% 问题怎么排查"></a>CPU 100% 问题怎么排查</h1>
      <p><strong>如果你线上遇到CPU占用100%的情况，你会怎么一步步排查，说一下你的分析思路。</strong></p>
<blockquote>
<ol>
<li>是否有条理地从系统到进程再到线程定位</li>
<li>理解CPU高的可能原因</li>
</ol>
</blockquote>
<p>如果线上CPU占用100%，有可能是算得太多，也有可能是卡得太久，对比，我会按照系统 -&gt; 进程 -&gt; 线程 -&gt; 代码这四步进行排查。</p>
<p><strong>第一步，系统层面</strong></p>
<p>需要先使用<code>top</code> 或<code>htop</code> 查看整体CPU占用，确认是单核打满还是多核整体飙升；其次如果是整体高，可能是负载激增，如果是单核高，多半是逻辑问题。</p>
<p>对于微服务系统，首先看监控&#x2F;告警：</p>
<ul>
<li>是单台机器100%，还是某个pod&#x2F;容器100%;</li>
<li>查看CPU各项：user&#x2F;system&#x2F;iowait&#x2F;steal占比不一样，方向也不一样<ul>
<li>us高：大概率是业务代码算得多&#x2F;死循环&#x2F;复杂SQL；</li>
<li>sy高：系统调用频繁（比如大量IO、锁、上下文切换）</li>
<li>wa高：CPU不是瓶颈，是IO卡住了；</li>
<li>steal高：虚拟化环境里被邻居抢了CPU。</li>
</ul>
</li>
</ul>
<figure class="highlight bash"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl top node</span><br><span class="line">kubectl top pod -n &lt;namespace&gt;  <span class="comment"># 定位到哪个pod/容器的CPU拉满</span></span><br></pre></td></tr></table></div></figure>



<p><strong>第二步，定位具体进程</strong></p>
<p>通过第一步，已经知道是某个节点上问题最大。</p>
<p><strong>登录机器，查看整体情况：</strong></p>
<figure class="highlight bash"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">top -c		<span class="comment"># 实时查看每个进程的CPU, 用&#x27;P&#x27;按CPU排序</span></span><br><span class="line"><span class="built_in">uptime</span>		<span class="comment"># 看 load average</span></span><br><span class="line">mpstat -P ALL 1	<span class="comment"># 看每个核的占用情况</span></span><br></pre></td></tr></table></div></figure>

<p>需要关注的有：</p>
<ul>
<li>是所有核打满，还是某几个核；</li>
<li>Load average很高但是 CPU iowait也高，可能是IO问题不是CPU算力问题；</li>
<li>top里能直接看到哪个进程%CPU排第一；</li>
</ul>
<p>或者用<code>ps</code>：</p>
<figure class="highlight bash"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ps -eo pid,ppid,user,%cpu,%mem,cmd --<span class="built_in">sort</span>=%cpu | <span class="built_in">head</span></span><br></pre></td></tr></table></div></figure>

<p>进而确定：<strong>PID + 可执行命令，确定是哪个服务。</strong> 在 K8s可以反查这个PID属于哪个容器（kubectl exec进Pod后用Top）</p>
<p><strong>第三步，线程级分析</strong></p>
<p>CPU 100%不是进程整体每一行代码一起跑，而是 <strong>某几个线程</strong> 在疯狂干活。</p>
<p>通过命令：</p>
<figure class="highlight bash"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">top -H -p &lt;pid&gt; 	<span class="comment"># 查看该进程中各线程 CPU 占用</span></span><br></pre></td></tr></table></div></figure>

<p>将占用最高的线程ID转成十六进制，如果是Go，就用 <code>go tool pprof</code> 查调用栈。进而明确 <strong>是那段代码在吞CPU。</strong></p>
<p><strong>第四步，代码层面</strong></p>
<p>结合业务指标，判断问题，常见业务指标有：</p>
<ul>
<li>QPS &#x2F; 请求数</li>
<li>RT &#x2F; 响应时间</li>
<li>错误率 (5xx，超时)</li>
<li>GC次数 &#x2F; 耗时</li>
<li>下游依赖（DB&#x2F;Redis&#x2F;外部接口）的QPS&amp;RT</li>
</ul>
<p>常见场景：</p>
<ul>
<li>QPS高+RT变慢+CPU高<ul>
<li>典型容量&#x2F;性能问题（流量变多、代码&#x2F;O(N2)&#x2F;SQL慢</li>
</ul>
</li>
<li>QPS正常甚至偏低+RT正常+CPU高<ul>
<li>死循环、自旋锁、空轮询，或者是某些后台业务疯掉</li>
</ul>
</li>
<li>错误率飙升+CPU高<ul>
<li>可能是某种异常疯狂重试，降级逻辑写挂了等</li>
</ul>
</li>
<li>system CPU高+context switch多<ul>
<li>线程数太多、频繁加锁&#x2F;解锁、频繁syscalls</li>
</ul>
</li>
</ul>

        <h2 id="深挖">
          <a href="#深挖" class="heading-link"><i class="fas fa-link"></i></a><a href="#深挖" class="headerlink" title="深挖"></a>深挖</h2>
      <p><strong>场景A：业务量确实变大 -&gt; 性能&#x2F;容量问题</strong></p>
<p><strong>场景B：业务平稳但CPU飙高 -&gt; 疑似死循环&#x2F;自锁&#x2F;bug</strong></p>
<p><strong>场景C：system CPU很高 -&gt; 系统调用&#x2F;内核开销大</strong></p>

        <h1 id="找到-10-亿个数中的-Top-K">
          <a href="#找到-10-亿个数中的-Top-K" class="heading-link"><i class="fas fa-link"></i></a><a href="#找到-10-亿个数中的-Top-K" class="headerlink" title="找到 10 亿个数中的 Top K"></a>找到 10 亿个数中的 Top K</h1>
      <p>假设你现在有 10 亿个整数，请你找出其中最大的 K 个数。如果数据量太大内存放不下，又该如何查找。</p>
<ul>
<li>能否从算法的角度理清思路</li>
<li>能否从工程角度考虑落地实现</li>
<li>能否说清楚时间复杂度和空间复杂度</li>
</ul>

        <h2 id="数据能放到内存中去">
          <a href="#数据能放到内存中去" class="heading-link"><i class="fas fa-link"></i></a><a href="#数据能放到内存中去" class="headerlink" title="数据能放到内存中去"></a>数据能放到内存中去</h2>
      <p><strong>维护一个大小为 K 的最小堆（Min-Heap）</strong></p>
<p>只维护当前最大的 K 个数：</p>
<ul>
<li>堆顶永远是这 K 个数里最小的</li>
<li>新数比堆顶大，加进来，并弹出最小值</li>
<li>新数比堆顶小，直接跳过</li>
</ul>
<p>**时间复杂度 O(NlogK)**，空间复杂度为 O(K)</p>

        <h2 id="10-亿数据放不下内存（真正的海量数据场景）">
          <a href="#10-亿数据放不下内存（真正的海量数据场景）" class="heading-link"><i class="fas fa-link"></i></a><a href="#10-亿数据放不下内存（真正的海量数据场景）" class="headerlink" title="10 亿数据放不下内存（真正的海量数据场景）"></a>10 亿数据放不下内存（真正的海量数据场景）</h2>
      <p>你不能一次读进内存，所以需要使用 <strong>外部排序</strong> 思路。</p>
<p><strong>先分片 + 部分排序 + 再归并（外部排序）</strong>,时间复杂度为 O(N logK)</p>
<p>chunksize &#x3D; UsableMemory * 0.7</p>
<ul>
<li>1.分片：把文件大小切成若干块，每块能放内存</li>
<li>对每个块做：<ul>
<li>排序；</li>
<li>去除每个块中的最大的K个；</li>
</ul>
</li>
<li>把所有的块（K*N个数）再用最小堆取一次 Top K，最终得到全局最大的 K 个。</li>
</ul>

        <h1 id="发生内存泄漏时如何定位">
          <a href="#发生内存泄漏时如何定位" class="heading-link"><i class="fas fa-link"></i></a><a href="#发生内存泄漏时如何定位" class="headerlink" title="发生内存泄漏时如何定位"></a>发生内存泄漏时如何定位</h1>
      <p>针对Go的项目，如果线上发生了内存泄漏，你会如何排查。</p>
<div class="table-container"><table>
<thead>
<tr>
<th>泄漏原因</th>
<th>pprof 特征</th>
<th>代码特征</th>
</tr>
</thead>
<tbody><tr>
<td><strong>goroutine 泄漏</strong></td>
<td>goroutine 数量持续上升</td>
<td>业务忘记退出、阻塞</td>
</tr>
<tr>
<td><strong>slice &#x2F; map 无限增长</strong></td>
<td>inuse_space 指向具体包和行号</td>
<td>append 不断插入；map cache 无淘汰</td>
</tr>
<tr>
<td><strong>未关闭 chan</strong></td>
<td>goroutine 堆积在 send&#x2F;recv</td>
<td>通常在管道关闭&#x2F;退出逻辑缺失</td>
</tr>
<tr>
<td><strong>Ticker 未 Stop()</strong></td>
<td>goroutine 堆积</td>
<td>新建 ticker 忘记 stop</td>
</tr>
<tr>
<td><strong>HTTP client 未关闭 Body</strong></td>
<td>pprof 显示大量 net&#x2F;http 内存</td>
<td>resp.Body 未 Close 导致泄漏连接</td>
</tr>
<tr>
<td><strong>缓存未清理（LFU&#x2F;LRU 不完善）</strong></td>
<td>map 占用内存大</td>
<td>自定义 cache 没有 eviction</td>
</tr>
<tr>
<td><strong>Cgo 导致内存泄漏</strong></td>
<td>Go 侧正常但 RSS 持续上涨</td>
<td>C 分配的内存未 free</td>
</tr>
</tbody></table></div>
</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2025/11/17/%E4%BA%91%E5%8E%9F%E7%94%9F%E5%9C%BA%E6%99%AF/">云原生场景</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2025-11-17</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2025-11-24</span></span></div></header><div class="post-body"><div class="post-excerpt">
        <h1 id="Kubernetes">
          <a href="#Kubernetes" class="heading-link"><i class="fas fa-link"></i></a><a href="#Kubernetes" class="headerlink" title="Kubernetes"></a>Kubernetes</h1>
      
        <h2 id="Pod启动后一直CrashLoopBackOff，该怎么排查">
          <a href="#Pod启动后一直CrashLoopBackOff，该怎么排查" class="heading-link"><i class="fas fa-link"></i></a><a href="#Pod启动后一直CrashLoopBackOff，该怎么排查" class="headerlink" title="Pod启动后一直CrashLoopBackOff，该怎么排查"></a>Pod启动后一直CrashLoopBackOff，该怎么排查</h2>
      <p><strong>1.查看 Pod 事件和状态</strong></p>
<p>先确认 Pod 为什么在重启。</p>
<figure class="highlight bash"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl describe pod &lt;pod-name&gt; -n &lt;namespace&gt;</span><br></pre></td></tr></table></div></figure>

<p>主要看以下几个参数：</p>
<ul>
<li><strong>Last State：Terminated</strong> （退出码、原因)</li>
<li><strong>Events</strong> （如无法挂载卷、拉取镜像失败、探针失败等）</li>
<li><strong>OOMKilled</strong> （内存不够）</li>
<li><strong>Error 或 Completed</strong> （非0退出）</li>
</ul>
<p><strong>2.查看 Pod 内部日志</strong></p>
<p>如果容器启动后马上退出，有可能日志里已经告诉你原因：</p>
<figure class="highlight bash"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl logs &lt;pod-name&gt; -n &lt;namespace&gt;</span><br></pre></td></tr></table></div></figure>

<p>Pod 有多个容器时：</p>
<figure class="highlight bash"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl logs &lt;pod-name&gt; -c &lt;container-name&gt; -n &lt;namespace&gt;</span><br></pre></td></tr></table></div></figure>

<p>如果是不断重启的容器，查看前一个容器的日志：</p>
<figure class="highlight bash"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl logs &lt;pod-name&gt; --previous</span><br></pre></td></tr></table></div></figure>

<p><strong>常见 CrashLoopBackOff 场景与排查方式</strong></p>
<ul>
<li><strong>应用本身异常退出</strong></li>
</ul>
<p>​	症状：<code>ExitCode: 1</code> 或 <code>2</code><br>​    解决：根据日志修复应用错误。</p>
<p>​    比如：配置文件错误、数据库连接失败、程序抛出异常、找不到启动命令。</p>
<ul>
<li><strong>健康检查探针失败（Liveness&#x2F;Readiness&#x2F;Startup）</strong></li>
</ul>
<p>​	查看 describe 中是否有：</p>
<figure class="highlight nginx"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">Liveness</span> probe failed</span><br><span class="line">Readiness probe failed</span><br></pre></td></tr></table></div></figure>

<p>排查方向：探针是否正确、探针请求路径是否存在、应用启动时间太长(initialDelaySeconds、timeoutSeconds)等</p>
<ul>
<li><strong>OOMKilled（内存不足）</strong></li>
</ul>
<p>查看 describe 是否有：</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">State: Terminated</span><br><span class="line">Reason: OOMKilled</span><br></pre></td></tr></table></div></figure>

<p>解决办法：增大 Pod 内存Limits；如果设置了limit而没设置request，可能造成调度问题也会 OOM.</p>
<ul>
<li><p><strong>权限或文件系统问题</strong></p>
<p>事件中可能看到：无法挂载卷、权限不足。可以检查：</p>
<figure class="highlight bash"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl describe pod &lt;pod&gt;</span><br><span class="line"><span class="comment"># 或者查看挂载情况</span></span><br><span class="line">kubectl <span class="built_in">exec</span> -it &lt;pod&gt; --<span class="built_in">ls</span> -l &lt;path&gt;</span><br></pre></td></tr></table></div></figure>


</li>
<li><p><strong>容器 Entrypointt &#x2F; CMD 配置错误</strong></p>
</li>
</ul>
<p>常见错误：入口脚本找不到、镜像打包不完整、Dockerfile没有正确CMD&#x2F;ENTTRYPOINT。</p>
<p>查看：</p>
<figure class="highlight bash"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl describe pod &lt;pod&gt; | grep Command -A 2</span><br></pre></td></tr></table></div></figure>

<ul>
<li>环境变量缺失或配置错误</li>
<li>镜像异常（例如程序 Crash 或基础镜像损坏）</li>
</ul>
<p><strong>进一步调试</strong> ： 让 Pod 启动时先不执行程序，只执行 sleep：</p>
<p>给Deployment 加一个临时 command:</p>
<figure class="highlight yaml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">command:</span> [<span class="string">&quot;sh&quot;</span>, <span class="string">&quot;-c&quot;</span>, <span class="string">&quot;sleep 3000&quot;</span>]</span><br></pre></td></tr></table></div></figure>




        <h1 id="如何排查-Service-无法访问的问题">
          <a href="#如何排查-Service-无法访问的问题" class="heading-link"><i class="fas fa-link"></i></a><a href="#如何排查-Service-无法访问的问题" class="headerlink" title="如何排查 Service 无法访问的问题"></a>如何排查 Service 无法访问的问题</h1>
      <p><strong>1.Service 是否存在、端口是否正确</strong></p>
<figure class="highlight bash"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl get svc -n &lt;ns&gt;</span><br><span class="line">kubectl describe svc &lt;svc-name&gt; -n &lt;ns&gt;</span><br></pre></td></tr></table></div></figure>

<p>检查：</p>
<ul>
<li>Service 类型（ClusterIP &#x2F; Nodeport &#x2F; LoadBalancer）</li>
<li>Port &#x2F; targetPort 是否对应 Pod 监听端口</li>
<li>Selector 是否正确匹配 Pod Label</li>
<li>ClusterIP 是否正常分配</li>
</ul>
<p><strong>2.检查Endpoints &#x2F; EndpoinSlice（是否成功绑定 Pod）</strong></p>
<figure class="highlight bash"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl get endpoints &lt;svc-name&gt; -n &lt;ns&gt;</span><br><span class="line">kubectl describe endpoints &lt;svc-name&gt; -n &lt;ns&gt;</span><br><span class="line">kubectl get endpoinsslice -n &lt;ns&gt; | grep &lt;svc-name&gt;</span><br></pre></td></tr></table></div></figure>

<p>如果看到：</p>
<figure class="highlight makefile"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">Endpoints: &lt;none&gt;</span></span><br></pre></td></tr></table></div></figure>

<p>说明Service找不到Pod，原因可能是：</p>
<ul>
<li>Pod的label与Service的selector不一致；</li>
<li>Pod NotReady: Readiness probe failed</li>
<li>Pod不属于同一namespace</li>
</ul>
<p><strong>3.确认Pod是否实际监听 targetPort</strong></p>
<p>进入Pod内部检查：</p>
<figure class="highlight bash"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl <span class="built_in">exec</span> -it &lt;pod&gt; -n &lt;ns&gt; -- netstat -ntlp</span><br><span class="line"><span class="comment"># 或者</span></span><br><span class="line">ss -lntp</span><br></pre></td></tr></table></div></figure>




        <h1 id="Pod-OOMKilled-的原因？如何定位？">
          <a href="#Pod-OOMKilled-的原因？如何定位？" class="heading-link"><i class="fas fa-link"></i></a><a href="#Pod-OOMKilled-的原因？如何定位？" class="headerlink" title="Pod OOMKilled 的原因？如何定位？"></a>Pod OOMKilled 的原因？如何定位？</h1>
      <p>当容器内存占用超过Pod的memory limi时，Kubernetes会触发OOM，直接杀死容器进程，状态显示：</p>
<figure class="highlight makefile"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">State: Terminated</span></span><br><span class="line"><span class="section">Reason: OOMKilled</span></span><br></pre></td></tr></table></div></figure>

<p><strong>1.应用内存使用超过了limit</strong></p>
<ul>
<li>堆太大</li>
<li>go程序内存泄漏</li>
<li>redis&#x2F;mongodb缓存占满</li>
<li>请求量突然增大，内存峰值高</li>
</ul>
<p><strong>2. memory request 与 limit 配置不合理</strong></p>
<p>比如：</p>
<figure class="highlight yaml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">resources:</span></span><br><span class="line">  <span class="attr">requests:</span></span><br><span class="line">    <span class="attr">memory:</span> <span class="string">512Mi</span></span><br><span class="line">  <span class="attr">limits:</span></span><br><span class="line">    <span class="attr">memory:</span> <span class="string">512Mi</span></span><br></pre></td></tr></table></div></figure>

<p>应用稍微抖动就超过 limit → 立刻 OOM。</p>
<p><strong>3.节点本身内存不足（Node OOM）</strong></p>
<p>即使容器没超过 limit，但节点内存不足，可能触发：</p>
<ol>
<li>cgroup OOM</li>
<li>kubelet EvictPod</li>
</ol>

        <h2 id="定位思路">
          <a href="#定位思路" class="heading-link"><i class="fas fa-link"></i></a><a href="#定位思路" class="headerlink" title="定位思路"></a>定位思路</h2>
      <p><strong>1.查看Pod状态（确定是否是OOMKilled）</strong></p>
<figure class="highlight bash"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl describe pod &lt;pod&gt; -n &lt;ns&gt;</span><br></pre></td></tr></table></div></figure>

<p>检查：</p>
<figure class="highlight yaml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">Last State:</span></span><br><span class="line">  <span class="attr">Reason:</span>       <span class="string">OOMKilled</span></span><br><span class="line">  <span class="attr">Exit Code:</span>    <span class="number">137</span></span><br></pre></td></tr></table></div></figure>

<p>Exit Code 137 &#x3D; 128 + 9，容器被系统杀死。</p>
<p><strong>2.确认容器的memory limit</strong></p>
<figure class="highlight bash"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pod &lt;pod&gt; -o jsonpath=<span class="string">&#x27;&#123;.spec.container[0].resources&#125;&#x27;</span></span><br></pre></td></tr></table></div></figure>

<p>重点看,如果limits低，一定会OOM：</p>
<ul>
<li>limits.memory</li>
<li>requests.memory</li>
</ul>
<p><strong>3.查看容器真实内存使用量</strong></p>
<p>使用 metric-server 或 Prometheus:</p>
<figure class="highlight bash"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl top pod &lt;pod&gt; -n &lt;ns&gt;</span><br></pre></td></tr></table></div></figure>

<p>如果看到：</p>
<figure class="highlight scss"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">MEMORY</span>(%) <span class="number">150%</span> 或 MEM &gt; limit</span><br></pre></td></tr></table></div></figure>

<p>则OOM。</p>
<p><strong>4.查看内存历史曲线</strong></p>
<p>通过k8s dashboard查看：</p>
<ul>
<li>内存是否慢慢上涨→内存泄漏</li>
<li>峰值是否在某个时刻暴涨→大流量&#x2F;大对象</li>
</ul>
<p><strong>5.检查节点内存是否不足（Node OOM）</strong></p>
<figure class="highlight bash"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl describe node &lt;node-name&gt;</span><br></pre></td></tr></table></div></figure>

<p>查看是否有：</p>
<figure class="highlight sql"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">System</span> OOM encounterer</span><br><span class="line">Evition Threshould crossed</span><br></pre></td></tr></table></div></figure>

<p>或者</p>
<figure class="highlight nginx"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">The</span> node was low <span class="literal">on</span> resource: memory.</span><br></pre></td></tr></table></div></figure>

<p>进而确定是否是node OOM，而不是应用的 OOM。</p>
<p><strong>6.查看语言运行时参数</strong></p>
<p>对于Go语言：</p>
<ul>
<li>检查 goroutine 是否泄漏；</li>
<li>map 无限制增长；</li>
<li>slice 扩容次数多等；</li>
</ul>

        <h1 id="Ingress-和-Service-的区别？什么时候用-Ingress？">
          <a href="#Ingress-和-Service-的区别？什么时候用-Ingress？" class="heading-link"><i class="fas fa-link"></i></a><a href="#Ingress-和-Service-的区别？什么时候用-Ingress？" class="headerlink" title="Ingress 和 Service 的区别？什么时候用 Ingress？"></a>Ingress 和 Service 的区别？什么时候用 Ingress？</h1>
      <p><strong>Service &#x3D; 暴露服务的网络抽象，Service 负责把流量送到 Pod。</strong> </p>
<p><strong>Ingress &#x3D; 对外 HTTP 入口 + 反向代理 + 路由， Ingress 负责把互联网的 HTTP 流量路由进来。</strong></p>
<p>两者通常 <strong>一起使用</strong>。Ingress 后端必须是 Service。</p>
<div class="table-container"><table>
<thead>
<tr>
<th align="center">项目</th>
<th align="center">Service</th>
<th align="center">Ingress</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><strong>作用</strong></td>
<td align="center">让集群内或外部访问 Pod</td>
<td align="center">提供 HTTP&#x2F;HTTPS 七层路由，管理多个服务的外部访问</td>
</tr>
<tr>
<td align="center"><strong>工作层级</strong></td>
<td align="center">四层（L4，TCP&#x2F;UDP）</td>
<td align="center">七层（L7，HTTP&#x2F;HTTPS）</td>
</tr>
<tr>
<td align="center"><strong>暴露方式</strong></td>
<td align="center">ClusterIP &#x2F; NodePort &#x2F; LoadBalancer</td>
<td align="center">通过域名和路径路由入站流量</td>
</tr>
<tr>
<td align="center"><strong>是否提供域名&#x2F;路径路由</strong></td>
<td align="center">❌ 不支持</td>
<td align="center">✔ 支持</td>
</tr>
<tr>
<td align="center"><strong>是否需要 Ingress Controller</strong></td>
<td align="center">❌ 不需要</td>
<td align="center">✔ 必须有（如 nginx &#x2F; traefik &#x2F; istio）</td>
</tr>
<tr>
<td align="center"><strong>典型用途</strong></td>
<td align="center">服务发现、Pod 访问入口</td>
<td align="center">对外统一入口、域名管理、SSL、反向代理</td>
</tr>
</tbody></table></div>
<p><strong>Ingress 的意义（解决了什么问题）</strong></p>
<p>你有 10 个服务，希望用域名访问，比如：</p>
<ul>
<li>api.example.com</li>
<li>web.example.com</li>
<li>admin.example.com</li>
</ul>
<p>如果不用 Ingress，你得为每个 Service 创建一个 <strong>LoadBalancer</strong>：</p>
<ul>
<li>10 个 LB → 费用高</li>
<li>每个 LB 有一个公网 IP → 不方便管理</li>
<li>无法统一做 HTTPS、WAF、路由</li>
</ul>
<p>使用 Ingress 后：</p>
<p><strong>只需要 1 个 LoadBalancer</strong><br>通过 Ingress Controller 做路由：</p>
<figure class="highlight text"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">api.example.com   -&gt; service api-svc</span><br><span class="line">web.example.com   -&gt; service web-svc</span><br><span class="line">/admin            -&gt; service admin-svc</span><br></pre></td></tr></table></div></figure>

<p>甚至可以基于路径：</p>
<ul>
<li>&#x2F;api → api 服务</li>
<li>&#x2F;web → 前端服务</li>
</ul>
<p>还能加：</p>
<ul>
<li>HTTPS 证书</li>
<li>限流</li>
<li>重试</li>
<li>反向代理</li>
<li>统一入口日志</li>
</ul>
<p><strong>什么时候用Ingress</strong></p>
<p>当你需要对外 HTTP &#x2F; HTTPS 访问时→用 Ingress</p>
<ul>
<li>需要域名访问（xxx.com）</li>
<li>需要路径路由（&#x2F;api、&#x2F;app）</li>
<li>需要 HTTPS（自带 TLS 管理）</li>
<li>需要统一入口负载均衡</li>
<li>不想创建多个外部 LoadBalancer</li>
<li>需要 nginx 级别的功能：重写、限流、请求头修改等</li>
</ul>
<p><strong>什么时候不用Ingress</strong></p>
<ul>
<li>服务仅集群内部访问 → 用 ClusterIP 即可</li>
<li>用 gRPC&#x2F;TCP&#x2F;UDP 等非 HTTP 协议 → 用 Service（L4）</li>
<li>你用的是 Service Mesh（如 istio Gateway） → Gateway 替代 Ingress</li>
</ul>
</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2025/11/09/Transformer%E5%85%A5%E9%97%A8/">Transformer入门</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2025-11-09</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2025-11-10</span></span></div></header><div class="post-body"><div class="post-excerpt">
        <h1 id="Transformer入门">
          <a href="#Transformer入门" class="heading-link"><i class="fas fa-link"></i></a><a href="#Transformer入门" class="headerlink" title="Transformer入门"></a>Transformer入门</h1>
      
        <h2 id="神经网络">
          <a href="#神经网络" class="heading-link"><i class="fas fa-link"></i></a><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2>
      
        <h3 id="循环神经网络（RNN）">
          <a href="#循环神经网络（RNN）" class="heading-link"><i class="fas fa-link"></i></a><a href="#循环神经网络（RNN）" class="headerlink" title="循环神经网络（RNN）"></a>循环神经网络（RNN）</h3>
      <p><strong>缺点：</strong></p>
<ul>
<li>无法进行并行计算，只能串行就算，长语句输入，计算效率低；</li>
<li>梯度消失，梯度爆炸，链式信息损失，难以实现长期记忆；</li>
</ul>

        <h3 id="卷积神经网络（CNN）">
          <a href="#卷积神经网络（CNN）" class="heading-link"><i class="fas fa-link"></i></a><a href="#卷积神经网络（CNN）" class="headerlink" title="卷积神经网络（CNN）"></a>卷积神经网络（CNN）</h3>
      <p><strong>优点：</strong></p>
<ul>
<li>可以进行并行计算，计算效率高；</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>一个filter感受长度有限，如果输入长度很长，需要叠很多层来感受到完整句子，如果不满足这些层数，就无法感受完整的句子；</li>
</ul>

        <h2 id="Transformer特点">
          <a href="#Transformer特点" class="heading-link"><i class="fas fa-link"></i></a><a href="#Transformer特点" class="headerlink" title="Transformer特点"></a>Transformer特点</h2>
      
        <h3 id="Inputs">
          <a href="#Inputs" class="heading-link"><i class="fas fa-link"></i></a><a href="#Inputs" class="headerlink" title="Inputs"></a>Inputs</h3>
      <p><strong>为什么会出现Transformer，诉求：</strong></p>
<ul>
<li>高计算效率，可以做并行计算；</li>
<li>需要长期记忆；</li>
</ul>
<p><strong>相对于RNN，Transformer特点：</strong></p>
<ul>
<li>高效率计算，可以做并行计算；</li>
<li>固有的全局视野，能够捕获长距离依赖；</li>
</ul>
<p><strong>相对于CNN，Transformer特点：</strong></p>
<ul>
<li>无局限的感受长度；</li>
<li>能够更加灵活地处理位置信息；</li>
</ul>

        <h3 id="Tokenize-分词">
          <a href="#Tokenize-分词" class="heading-link"><i class="fas fa-link"></i></a><a href="#Tokenize-分词" class="headerlink" title="Tokenize(分词)"></a>Tokenize(分词)</h3>
      <p>单词级别的分词；</p>
<p>字符级别的分词；</p>
<p>字节级的分词；</p>
<p>中文分词；</p>

        <h3 id="词嵌入（Embedding）">
          <a href="#词嵌入（Embedding）" class="heading-link"><i class="fas fa-link"></i></a><a href="#词嵌入（Embedding）" class="headerlink" title="词嵌入（Embedding）"></a>词嵌入（Embedding）</h3>
      <p>将词的向量转为特征向量；</p>
<p>词嵌入矩阵维度：需要嵌入的一句句子中词汇的总数（<strong>sequence</strong>）*嵌入向量的维度（<strong>d_model</strong>）；</p>
<p>Embedding是缺乏词的位置信息的；</p>

        <h3 id="位置编码（Positional-Encoding）">
          <a href="#位置编码（Positional-Encoding）" class="heading-link"><i class="fas fa-link"></i></a><a href="#位置编码（Positional-Encoding）" class="headerlink" title="位置编码（Positional Encoding）"></a>位置编码（Positional Encoding）</h3>
      <p>偶数位置</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// pos是元素在序列中的位置</span><br><span class="line">// i是编码向量中的维度索引</span><br><span class="line">// d_model是嵌入式向量的维度，及模型的维度</span><br><span class="line">PE(pos, 2i) = sin(pos/10000^(2i/d_model))  // 表示位置pos在编码向量中的第2i个维度的值；</span><br></pre></td></tr></table></div></figure>

<p>奇数位置</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">PE(pos, 2i+1) = cos(pos/10000^(2i/d_model))	// 表示位置pos在编码向量中的第2i+1个维度的值；</span><br></pre></td></tr></table></div></figure>

<p><strong>这样设计编码的好处：</strong></p>
<ul>
<li><strong>唯一性：</strong> 每个位置的编码是唯一的，这确保了模型能够区分序列中的不同位置；</li>
<li><strong>周期性：</strong> 能够根据相位捕捉位置关系；</li>
<li><strong>正交性：</strong> 偶数位置和奇数位置的编码是正交的，增加了编码的区分度和信息丰富度；</li>
</ul>

        <h3 id="编码器模块（Encoder-Block）">
          <a href="#编码器模块（Encoder-Block）" class="heading-link"><i class="fas fa-link"></i></a><a href="#编码器模块（Encoder-Block）" class="headerlink" title="编码器模块（Encoder Block）"></a>编码器模块（Encoder Block）</h3>
      <img src="/2025/11/09/Transformer%E5%85%A5%E9%97%A8/transformer.png" class title="transformer">


        <h3 id="自注意力-Self-attention">
          <a href="#自注意力-Self-attention" class="heading-link"><i class="fas fa-link"></i></a><a href="#自注意力-Self-attention" class="headerlink" title="自注意力 (Self-attention)"></a>自注意力 (Self-attention)</h3>
      
        <h3 id="多头注意力层-Multi-Head-Attention">
          <a href="#多头注意力层-Multi-Head-Attention" class="heading-link"><i class="fas fa-link"></i></a><a href="#多头注意力层-Multi-Head-Attention" class="headerlink" title="多头注意力层(Multi-Head Attention)"></a>多头注意力层(Multi-Head Attention)</h3>
      <p>每个头独立地关注输入的不同表示子空间，从而能够捕捉不同方面的信息，并综合这些信息来更全面地理解文本。</p>

        <h3 id="残差-amp-层归一-Add-amp-Layer-Normalization">
          <a href="#残差-amp-层归一-Add-amp-Layer-Normalization" class="heading-link"><i class="fas fa-link"></i></a><a href="#残差-amp-层归一-Add-amp-Layer-Normalization" class="headerlink" title="残差&amp;层归一(Add &amp; Layer Normalization)"></a>残差&amp;层归一(Add &amp; Layer Normalization)</h3>
      <p><strong>残差的目的</strong></p>
<p>虽然每次循环都在充分地挖掘特征，但是希望能把以前循环地挖掘结果一并记住。</p>
<p><strong>归一的目的</strong></p>
<p>有助于稳定神经网络的学习过程，减少训练时间，并有时还可以提高模型的最终性能。</p>

        <h3 id="前馈神经网络（Feed-Forward）">
          <a href="#前馈神经网络（Feed-Forward）" class="heading-link"><i class="fas fa-link"></i></a><a href="#前馈神经网络（Feed-Forward）" class="headerlink" title="前馈神经网络（Feed Forward）"></a>前馈神经网络（Feed Forward）</h3>
      <p><strong>目的</strong></p>
<ul>
<li>增加网络结构的非线性，原来只是个加权求和的过程；</li>
<li>增加参数量，神经元带来大量可训练的参数，提升模型复杂度和深度，使模型可以处理更复杂的任务。</li>
</ul>

        <h3 id="解码器模块-Decoder-Block">
          <a href="#解码器模块-Decoder-Block" class="heading-link"><i class="fas fa-link"></i></a><a href="#解码器模块-Decoder-Block" class="headerlink" title="解码器模块(Decoder Block)"></a>解码器模块(Decoder Block)</h3>
      <p><strong>Decoder block整体用于：</strong></p>
<ul>
<li>集成上下文信息；</li>
<li>自回归生成输出；</li>
<li>保证输出时序性；</li>
</ul>

        <h3 id="遮蔽多头注意力-Masked-Multi-Head-Attention">
          <a href="#遮蔽多头注意力-Masked-Multi-Head-Attention" class="heading-link"><i class="fas fa-link"></i></a><a href="#遮蔽多头注意力-Masked-Multi-Head-Attention" class="headerlink" title="遮蔽多头注意力 Masked Multi-Head Attention"></a>遮蔽多头注意力 Masked Multi-Head Attention</h3>
      <p><strong>原理：</strong> 通过对注意力分数矩阵应用一个遮蔽（一个上三角矩阵，其中未来位置的元素被设置为非常大的负数），在计算softmax前有效地将这些位置地注意力分数降低到接近0。这意味着生成序列的每一步，模型只能注意到当前位置的词或标记。</p>
<p><strong>目的：</strong> 遮蔽多头注意力机制确保了在解码器生成当前输出时，不会受到未来输出的影响。</p>

        <h3 id="交叉注意力-Cross-Attention">
          <a href="#交叉注意力-Cross-Attention" class="heading-link"><i class="fas fa-link"></i></a><a href="#交叉注意力-Cross-Attention" class="headerlink" title="交叉注意力 Cross Attention"></a>交叉注意力 Cross Attention</h3>
      <p><strong>原理：</strong> 解码器使用当前的状态作为查询，与编码器的输出（键和值）进行交互，通过注意力机制确定编码器输出中的哪些部分是重要的，并据此生成下一个输出元素。</p>
<p><strong>目的：</strong> 融合两个不同序列或信息源的特征。</p>

        <h3 id="线性层-amp-Softmax层-Linear-amp-Softmax">
          <a href="#线性层-amp-Softmax层-Linear-amp-Softmax" class="heading-link"><i class="fas fa-link"></i></a><a href="#线性层-amp-Softmax层-Linear-amp-Softmax" class="headerlink" title="线性层&amp;Softmax层 (Linear &amp; Softmax)"></a>线性层&amp;Softmax层 (Linear &amp; Softmax)</h3>
      <p><strong>原理：</strong> decoder输出为[100, 512]，这个512是抽象的特征，无法指导我具体下面该生成哪个概率最大的词，所以我要向办法把[100, 512]的矩阵映射到[100, 1000]，其中每个维度的值代表了相应单词作为序列下一单词的未归一化分数，再利用Softmax做下归一，就能得到10000个词的概率。</p>

        <h3 id="Transformer推理过程">
          <a href="#Transformer推理过程" class="heading-link"><i class="fas fa-link"></i></a><a href="#Transformer推理过程" class="headerlink" title="Transformer推理过程"></a>Transformer推理过程</h3>
      <p><strong>贪心搜索(Greedy)：</strong> 每次去概率最大的词作为推理输出；</p>
<p><strong>宽度有限搜索(Beam Search)：</strong> 保留了若干个最有可能的候选序列，最后输出概率最大的序列作为推理输出；</p>
</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2025/10/19/k8s%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/">k8s集群环境搭建.md</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2025-10-19</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2025-10-19</span></span></div></header><div class="post-body"><div class="post-excerpt">
        <h1 id="Kubernetes集群">
          <a href="#Kubernetes集群" class="heading-link"><i class="fas fa-link"></i></a><a href="#Kubernetes集群" class="headerlink" title="Kubernetes集群"></a>Kubernetes集群</h1>
      
        <h2 id="环境准备">
          <a href="#环境准备" class="heading-link"><i class="fas fa-link"></i></a><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h2>
      <div class="table-container"><table>
<thead>
<tr>
<th align="center">节点</th>
<th align="center">ip</th>
</tr>
</thead>
<tbody><tr>
<td align="center">k8s-node1</td>
<td align="center">192.168.157.21</td>
</tr>
<tr>
<td align="center">k8s-node2</td>
<td align="center">192.168.157.22</td>
</tr>
<tr>
<td align="center">k8s-node3</td>
<td align="center">192.168.157.23</td>
</tr>
</tbody></table></div>
</div></div></article></section><nav class="paginator"><div class="paginator-inner"><span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/7/">7</a><a class="extend next" rel="next" href="/page/2/"><i class="fas fa-angle-right"></i></a></div></nav></div></div><div class="sidebar-wrap" id="sidebar-wrap"><aside class="sidebar" id="sidebar"><section class="sidebar-toc hide"></section><!-- ov = overview--><section class="sidebar-ov"><div class="sidebar-ov-author"><div class="sidebar-ov-author__avatar"><img class="sidebar-ov-author__avatar_img" src="/images/icons/2.jpg" alt="avatar"></div><p class="sidebar-ov-author__text">今天要比昨天进步一点点</p></div><div class="sidebar-ov-social"><a class="sidebar-ov-social-item" href="https://github.com/IAMLLT796" target="_blank" rel="noopener" data-popover="Github" data-popover-pos="up"><span class="sidebar-ov-social-item__icon"><i class="fab fa-github"></i></span></a><a class="sidebar-ov-social-item" href="lt.liu796@gmail.com" target="_blank" rel="noopener" data-popover="Gmail" data-popover-pos="up"><span class="sidebar-ov-social-item__icon">邮</span></a><a class="sidebar-ov-social-item" href="https://space.bilibili.com/274733853?spm_id_from=333.1007.0.0" target="_blank" rel="noopener" data-popover="bilibili" data-popover-pos="up"><span class="sidebar-ov-social-item__icon">Bili</span></a><a class="sidebar-ov-social-item" href="https://www.zhihu.com/people/kui-hua-cheng-hai-xiang-yang-er-kai-76" target="_blank" rel="noopener" data-popover="知乎" data-popover-pos="up"><span class="sidebar-ov-social-item__icon">知</span></a></div><div class="sidebar-ov-state"><a class="sidebar-ov-state-item sidebar-ov-state-item--posts" href="/archives/"><div class="sidebar-ov-state-item__count">66</div><div class="sidebar-ov-state-item__name">归档</div></a><a class="sidebar-ov-state-item sidebar-ov-state-item--categories" href="/categories/"><div class="sidebar-ov-state-item__count">14</div><div class="sidebar-ov-state-item__name">分类</div></a><a class="sidebar-ov-state-item sidebar-ov-state-item--tags" href="/tags/"><div class="sidebar-ov-state-item__count">73</div><div class="sidebar-ov-state-item__name">标签</div></a></div><div class="sidebar-ov-cc"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" target="_blank" rel="noopener" data-popover="知识共享许可协议" data-popover-pos="up"><img src="/images/cc-by-nc-sa.svg"></a></div></section></aside></div><div class="clearfix"></div></div></main><footer class="footer" id="footer"><div class="footer-inner"><div><span>冀ICP备19023374</span></div><div><span>由 <a href="http://hexo.io/" title="Hexo" target="_blank" rel="noopener">Hexo</a> 大力支持</span><span> v6.3.0</span><span class="footer__devider">|</span><span>Theme - <a href="https://github.com/liuyib/hexo-theme-stun/" title="Stun" target="_blank" rel="noopener">Stun</a></span><span> v2.8.0</span></div><div class="busuanzi"><span class="busuanzi-siteuv"><span class="busuanzi-siteuv__icon"><i class="fas fa-user"></i></span><span class="busuanzi-siteuv__info">Visitors</span><span class="busuanzi-siteuv__value" id="busuanzi_value_site_uv"></span></span><span class="busuanzi-sitepv"><span class="busuanzi-siteuv__icon"><i class="fas fa-eye"></i></span><span class="busuanzi-siteuv__info">Views</span><span class="busuanzi-siteuv__value" id="busuanzi_value_site_pv"></span></span></div><div>葵海-做一个有趣的人</div></div></footer><div class="loading-bar" id="loading-bar"><div class="loading-bar__progress"></div></div><div class="back2top" id="back2top"><span class="back2top__icon"><i class="fas fa-rocket"></i></span></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@v3.4.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.ui.min.js"></script><script src="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax@latest/pjax.min.js"></script><script>window.addEventListener('DOMContentLoaded', function () {
  var pjax = new Pjax({"selectors":["head title","#main",".pjax-reload",".header-banner"],"history":true,"scrollTo":false,"scrollRestoration":false,"cacheBust":false,"debug":false,"currentUrlFullReload":false,"timeout":0});
  // 加载进度条的计时器
  var loadingTimer = null;

  // 重置页面 Y 方向上的滚动偏移量
  document.addEventListener('pjax:send', function () {
    $('.header-nav-menu').removeClass('show');
    if (CONFIG.pjax && CONFIG.pjax.avoidBanner) {
      $('html').velocity('scroll', {
        duration: 500,
        offset: $('#header').height(),
        easing: 'easeInOutCubic'
      });
    }

    var loadingBarWidth = 20;
    var MAX_LOADING_WIDTH = 95;

    $('.loading-bar').addClass('loading');
    $('.loading-bar__progress').css('width', loadingBarWidth + '%');
    clearInterval(loadingTimer);
    loadingTimer = setInterval(function () {
      loadingBarWidth += 3;
      if (loadingBarWidth > MAX_LOADING_WIDTH) {
        loadingBarWidth = MAX_LOADING_WIDTH;
      }
      $('.loading-bar__progress').css('width', loadingBarWidth + '%');
    }, 500);
  }, false);

  window.addEventListener('pjax:complete', function () {
    clearInterval(loadingTimer);
    $('.loading-bar__progress').css('width', '100%');
    $('.loading-bar').removeClass('loading');
    setTimeout(function () {
      $('.loading-bar__progress').css('width', '0');
    }, 400);
    $('link[rel=prefetch], script[data-pjax-rm]').each(function () {
      $(this).remove();
    });
    $('script[data-pjax], #pjax-reload script').each(function () {
      $(this).parent().append($(this).remove());
    });

    if (Stun.utils.pjaxReloadBoot) {
      Stun.utils.pjaxReloadBoot();
    }
    if (Stun.utils.pjaxReloadScroll) {
      Stun.utils.pjaxReloadScroll();
    }
    if (Stun.utils.pjaxReloadSidebar) {
      Stun.utils.pjaxReloadSidebar();
    }
    if (false) {
      if (Stun.utils.pjaxReloadHeader) {
        Stun.utils.pjaxReloadHeader();
      }
      if (Stun.utils.pjaxReloadScrollIcon) {
        Stun.utils.pjaxReloadScrollIcon();
      }
      if (Stun.utils.pjaxReloadLocalSearch) {
        Stun.utils.pjaxReloadLocalSearch();
      }
    }
  }, false);
}, false);</script><div id="pjax-reload"><script src="https://cdn.jsdelivr.net/npm/quicklink@1.0.1/dist/quicklink.umd.js"></script><script>function initQuicklink() {
  quicklink({
    timeout: '10000',
    priority: true,
    ignores: [uri => uri.includes('#'), uri => uri === 'https://iamllt796.github.io/', /\/api\/?/,uri => uri.includes('.xml'),uri => uri.includes('.zip'),(uri, el) => el.hasAttribute('nofollow'),(uri, el) => el.hasAttribute('noprefetch')]
  });
}

if (true || false) {
  initQuicklink();
} else {
  window.addEventListener('DOMContentLoaded', initQuicklink, false);
}</script><script src="https://cdn.jsdelivr.net/gh/sukkaw/busuanzi@latest/bsz.pure.mini.js" async></script></div><script src="/js/utils.js?v=2.8.0"></script><script src="/js/stun-boot.js?v=2.8.0"></script><script src="/js/scroll.js?v=2.8.0"></script><script src="/js/header.js?v=2.8.0"></script><script src="/js/sidebar.js?v=2.8.0"></script></body></html>